{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"storm_white \u96f7\u767d These are my notes. Final note.","title":"storm_white \u96f7\u767d"},{"location":"index.html#storm_white","text":"These are my notes. Final note.","title":"storm_white \u96f7\u767d"},{"location":"math/3601/ch1.html","text":"Chapter 1: Preliminaries Errors Absolute error: \\(|x - \\tilde{x}|\\) Relative error: \\(\\frac{|x - \\tilde{x}|}{|x|}\\) Important theorems Rolle's Theorem Intermediate Value Theorem Mean-Value Theorem Taylor's Theorem \\(R_n\\) : Truncation error estimate 2D Case Triangle inequality \\(||a|-|b|| \\leq |a-b|\\) AM GM inequality \\(\\frac{x+y}{2} \\geq \\sqrt{xy}\\) Condition number \\[ CN = \\frac{\\frac{f(x) - f(x_0)}{f(x_0)}}{\\frac{x-x_0}{x_0}} \\approx \\frac{x_0 f'(x_0)}{f(x_0)} \\] Order of Convergence \\[ \\lim_{n\\to\\infty} \\frac{|x_{n+1}-x^*|}{|x_n-x^*|^p} = \\beta \\] Use L'Hopital rule? Big O \\(O(h^2)\\)","title":"Chapter 1: Preliminaries"},{"location":"math/3601/ch1.html#chapter-1-preliminaries","text":"","title":"Chapter 1: Preliminaries"},{"location":"math/3601/ch1.html#errors","text":"Absolute error: \\(|x - \\tilde{x}|\\) Relative error: \\(\\frac{|x - \\tilde{x}|}{|x|}\\)","title":"Errors"},{"location":"math/3601/ch1.html#important-theorems","text":"Rolle's Theorem Intermediate Value Theorem Mean-Value Theorem Taylor's Theorem \\(R_n\\) : Truncation error estimate 2D Case Triangle inequality \\(||a|-|b|| \\leq |a-b|\\) AM GM inequality \\(\\frac{x+y}{2} \\geq \\sqrt{xy}\\)","title":"Important theorems"},{"location":"math/3601/ch1.html#condition-number","text":"\\[ CN = \\frac{\\frac{f(x) - f(x_0)}{f(x_0)}}{\\frac{x-x_0}{x_0}} \\approx \\frac{x_0 f'(x_0)}{f(x_0)} \\]","title":"Condition number"},{"location":"math/3601/ch1.html#order-of-convergence","text":"\\[ \\lim_{n\\to\\infty} \\frac{|x_{n+1}-x^*|}{|x_n-x^*|^p} = \\beta \\] Use L'Hopital rule?","title":"Order of Convergence"},{"location":"math/3601/ch1.html#big-o","text":"\\(O(h^2)\\)","title":"Big O"},{"location":"math/3601/ch2.html","text":"Chapter 2: Solutions of Nonlinear Equations Bisection method Based on the intermediate value theorem . If \\(f\\) is continuous on \\([a, b]\\) , and if \\(f(a) \\cdot f(b) < 0\\) (sign flip), then \\(f\\) must have a zero in \\((a, b)\\) . Main loop Set \\(c = (a+b)/2\\) and test whether \\(f(a)f(c) < 0\\) . If this is true, then \\(f\\) has a zero in \\((a, c)\\) (the signs of \\(f(a)\\) and \\(f(c)\\) are different). So we rename \\(c\\) as \\(b\\) and start again with the new interval in \\([a, b]\\) . Else if \\(f(a)f(c) > 0\\) , then \\(f(c)f(b) < 0\\) (the sign of \\(f(a)\\) and \\(f(c)\\) are the same). In this case, rename \\(c\\) as \\(a\\) . Repeat until the length of the interval or \\(f(c)\\) is small enough . Example Find root of \\(e^x = sin x\\) in the interval \\([-4, -3]\\) , with termination scalar \\(\\epsilon\\) . Then let \\(f(x) = e^x - sin x\\) . Finding zero of \\(f(x)\\) is equivalent to finding root in original equation. \\(f(-4)f(-3) < 0\\) Then continue iterating until \\(f(c) < \\epsilon\\) Therorem (Convergence of Bisection Method) The limits \\(\\lim_{n\\to\\infty} a_n\\) and \\(\\lim_{n\\to\\infty} b_n\\) exist. Proof \\(a_0 \\leq a_1 \\leq \\cdots \\leq b_0\\) The sequence \\(a_n\\) is nondecreasing and bounded above , so \\(\\lim_{n\\to\\infty} a_n\\) exists. Similarly, \\(\\lim_{n\\to\\infty} b_n\\) exists. If \\(c_n = (a_n + b_n) / 2\\) and \\(\\lim_{n\\to\\infty} c_n = r\\) , then \\(|r - c_n| \\leq 2^{-(n+1)}(b_0 - a_0)\\) Proof \\(b_1 - a_1 = \\frac{1}{2}(b_0 - a_0)\\) \\(b_2 - a_2 = \\frac{1}{2}(b_1 - a_1) = \\frac{1}{4}(b_0 - a_0)\\) \\(\\cdots\\) \\(b_n - a_n = \\frac{1}{2^n}(b_0 - a_0) = 2^{-n}(b_0 - a_0)\\) Thus, \\(\\lim_{n\\to\\infty}b_n - \\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}2^{-n}(b_0-a_0) = 0\\) Therefore \\(\\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}b_n \\equiv r\\) Since \\(f\\) is continuous, \\(\\lim_{n\\to\\infty}f(a_n) = f(\\lim_{n\\to\\infty}f(a_n)) = r\\) and \\(\\lim_{n\\to\\infty}f(b_n) = f(\\lim_{n\\to\\infty}f(b_n)) = r\\) \\(0 \\geq f(a_n)f(b_n) \\implies 0 \\geq [f(r)]^2 \\implies f(r) = 0\\) \\(|r - c_n| \\leq \\frac{1}{2}(b_n - a_n) = 2^{-(n+1)}(b_0 - a_0)\\) Analysis For an error \\(\\epsilon > 0\\) , if \\(\\frac{1}{2}(b_n - a_n) = 2^{-(n+1)}(b_0 - a_0) \\le \\epsilon\\) , we need to choose \\(n > \\frac{\\ln[(b_0 - a_0)/\\epsilon]}{\\ln 2} - 1\\) Derivation \\(2^{-(n+1)}(b_0 - a_0) \\leq \\epsilon\\) \\(-(n+1)\\ln 2 + \\ln(b_0 - a_0) \\leq \\ln \\epsilon\\) \\(-\\ln 2 + \\ln(b_0 - a_0) - \\ln \\epsilon \\leq n \\ln 2\\) \\(n > \\frac{\\ln[(b_0 - a_0)/\\epsilon]}{\\ln 2} - 1\\) Linear convergence: \\(|r - c_n| \\leq \\frac{1}{2}(b_n - a_n), |r - c_{n+1}| \\leq \\frac{1}{2}(\\frac{1}{2}(b_n - a_n)) \\implies |r - c_{n+1}| \\approx \\frac{1}{2}|r - c_{n}|\\) . Reliable but slow . Guaranteed to work if \\(f(x)\\) is continuous. Newton-Raphson's Method \\[ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} \\] Derivation By Taylor's theorem, \\(0 = f(r) = f(x + h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(\\delta)\\) Treat \\(\\frac{h^2}{2}f''(\\delta)\\) as small and ignore the \\(O(h^2)\\) term. Then \\(f(x) + hf'(x) = 0 \\implies h = -\\frac{f(x)}{f'(x)}\\) Linearize the function \\(f(x)\\) (tangent line) to approximate the curve of \\(f(x)\\) and find the zero of the tangent line. Faster than bisection method, but might diverge. Only have local convergence . Example Q: Use Newton's method to find \\(1/b > 0\\) without using division. Solution: Use \\(f(x) = b-1/x\\) . Then \\(1/b\\) is the root of \\(f(x) = 0\\) . \\(f'(x) = \\frac{1}{x^2}\\) \\(x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} = x_k - \\frac{b - (1/x_k)}{1/x^2} = x_k(2-bx_k)\\) Extension to matrix inverse : \\(X_{k+1} = X_k (2I - AX_k)\\) Error analysis Quadratic convergence Assumptions : \\(f''(x)\\) continuous, \\(f'(r) \\neq 0\\) Danger If the root has multiplicity greater than one, the convergence rate is linear . Theorem Let \\(f''\\) be continuous and \\(r\\) satisfy \\(f(r) = 0, f'(r) \\neq 0\\) . Then ther is a neighborhood of \\(r\\) and a constant \\(C\\) such that if \\(x_0\\) is in this neighborhood, the successive points generated become steadily closer to \\(r\\) and satisfy \\[ |x_{n+1} - r| \\leq C |x_n - r|^2 \\] Proof (Taylor's theorem) \\(e_n = x_n - r\\) Assume: \\(f''(x)\\) continuous, \\(f'(r) \\neq 0\\) \\(e_{n+1} = x_{n+1} - r = x_n - \\frac{f(x_n)}{f'(x_n)} - r = \\frac{e_n \\cdot f'(x_n) - f(x_n)}{f'(x_n)}\\) By Taylor's theorem, \\(0 = f(r) = f(x_n - e_n) = f(x_n) - e_n f'(x_n) + \\frac{1}{2} e_n^2 f''(\\delta)\\) \\(e_n f'(x_n) - f(x_n) = \\frac{1}{2} e_n^2 f''(\\delta)\\) Substituting, \\(e_{n+1} = \\frac{\\frac{1}{2} e_n^2 f''(\\delta)}{f'(x_n)}\\) As \\(n\\to\\infty\\) , \\(\\delta\\to r\\) and \\(x_n\\to r\\) . So \\(\\frac{e_{n+1}}{e_n^2} = \\frac{\\frac{1}{2} f''(r)}{f'(r)} = C\\) Secant method Motivation: Calculationg \\(f'(x)\\) is difficult. Why not use an approximation of \\(f'(x)\\) instead? Approximate \\(f'(x_n)\\) as \\(\\frac{f(x_n)-f(x_{n-1})}{f(x_n)-f(x_{n-1})}\\) So the iterative scheme becomes \\[ x_{n+1} = x_n - \\frac{f(x_n)-f(x_{n-1})}{f(x_n)-f(x_{n-1})} f(x_n) \\] Slower than Newton's method (proof not required) Practical considerations for Newton's method Difficulty in calculating \\(f'(x)\\) Failure of the method to converge to the root Overshoot and diverge away from the root Stationary point \\(f'(x) = 0\\) Poor initial estimate leading to non-convergence Slow convergence for repeated roots If the root has multiplicity greater than one, the convergence rate is linear. Divergent loop System of nonlinear equations \\[ \\begin{cases} f_1(x_1, x_2) = 0 \\\\ f_2(x_1, x_2) = 0 \\end{cases} \\] By Taylor's Theorem, \\[ f(x_1^{(k)} + h_1^{(k)}, x_2^{(k)} + h_2^{(k)}) = f(x_1^{(k)}, x_2^{(k)}) + h_1^{(k)} \\frac{\\partial f}{\\partial x_1}(x_1^{(k)}, x_2^{(k)}) + h_2^{(k)} \\frac{\\partial f}{\\partial x_2}(x_1^{(k)}, x_2^{(k)}) + O(h^2) = 0 \\] Jacobian matrix: \\[ J(X^{(k)}) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}(X^{(k)}) & \\frac{\\partial f_1}{\\partial x_2}(X^{(k)})\\\\ \\frac{\\partial f_2}{\\partial x_1}(X^{(k)}) & \\frac{\\partial f_2}{\\partial x_2}(X^{(k)}) \\end{bmatrix} \\] \\[ Jh = -F(X^{(k)}) \\] \\[ X^{(k+1)} = X^{(k)} - (J(X^{(k)}))^{-1}F(X^{(k)}) \\] Multi-variable Newton's method still enjoys the property of quadratic convergence of the starting point is near the exact solution. However, a significant weakness is the requirement that, in each iteration: a Jacobian matrix (involving \\(n^2\\) partial derivatives) has to be evaluated an \\(n \\times n\\) linear system involving this matrix must be solved. Rescaled simple iteration: Reduce cost of approximating \\(F'(X^{(k)})^{(-1)}\\) Simplest way: Not to recompute it, just use an initial approximation \\(F'(X^{(0)})^{(-1)}\\) throughout the iteration Difficult to guarantee that this condition will be met... Convergence is generally linear Or take one Newton step followed by several additional iteration steps without updating the Jacobian matrix. Generalized secant method (Broyden method) (not required) Begin with a rough guess of the Jacobian, and use successive evaluations of \\(F\\) and its gradient to evaluate the guess of the Jacobian \\(J\\) . Each iteration is far less costly, but more iterations are needed. Fixed point iteration \\[ x_{n+1} = g(x_n) \\] Finding a zero/root for \\(f(x)\\) \\(\\implies\\) Finding a fixed point for \\(g(x) = x - f(x)\\) Contractive Mapping Theorem Contractive if there exists \\(\\lambda \\in (0, 1)\\) s.t. \\(|F(x) - F(y)| \\leq \\lambda |x - y|\\) Suppose \\(F: C \\to C\\) where \\(C\\) is a closed set, is a contractive mapping. Then \\(F\\) has a unique fixed point. This fixed point is the limit of every sequence obtained by \\(x_{n+1} = F(x_n)\\) with any starting point \\(x_0 \\in C\\) Proof of convergence of \\(x_n\\) Write \\(x_n = x_0 + (x_1 - x_0) + (x_2 - x_1) + \\cdots (x_n - x_{n-1})\\) Need to show that \\(\\sum^{\\infty}_{n=1}(x_n - x_{n-1})\\) converges. Suffices to show that \\(\\sum^{\\infty}_{n=1}|x_n - x_{n-1}|\\) converges. Since \\(F\\) is contractive, we have \\(|x_n - x_{n-1}| = |F(x_{n-1}) - F(x_{n-2})| \\leq \\lambda |x_{n-1} - x_{n-2}|\\) This argument can be repeated: \\(|x_n - x_{n-1}| \\leq \\lambda |x_{n-1} - x_{n-2}| \\leq \\lambda^2|x_{n-2} - x_{n-3}| \\leq \\cdots \\leq \\lambda^{n-1}|x_1 - x_0|\\) Then we have \\(\\sum^{\\infty}_{n=1}|x_n - x_{n-1}| \\leq \\sum^{\\infty}_{n=1}\\lambda^{n-1}|x_1-x_0| = |x_1-x_0|\\sum^{\\infty}_{n=1}\\lambda^{n-1}=\\frac{1}{1-\\lambda}|x_1-x_0|\\) So the sum is bounded, hence it converges, and thus the sequence \\(x_n\\) converges for any inital point \\(x_0\\) . Proof of uniqueness of fixed point Let \\(x\\) and \\(y\\) both be fixed points of \\(F\\) . Then \\(|x - y| = |F(x) - F(y)| \\leq \\lambda |x - y|\\) Since \\(\\lambda < 1\\) , this forces \\(|x - y| = 0\\) . That is, \\(x = y\\) . Example Q: Prove that the sequence \\(x_0 = -15, x_{n+1} = 3 - \\frac{1}{2}|x_n|\\) is convergent. A: \\(|F(x) - F(y)| = |3 - \\frac{1}{2}|x| - 3 + \\frac{1}{2}|y|| = \\frac{1}{2}||y|-|x|| \\overset{\\mathrm{Triangle~inequality}}{\\leq} \\frac{1}{2}|y-x|\\) Useful convergence criteria If \\(F \\in C[a, b]\\) such that \\(a \\leq F(x) \\leq b\\) for all \\(x \\in [a, b]\\) , then \\(F\\) has a fixed point in \\([a, b]\\) . ( \\(F\\) maps a closed set onto itself) In addition, if \\(F'(x)\\) exists in \\((a, b)\\) and there exists a positive constant \\(M < 1\\) such that \\(|F'(x)| \\leq M < 1\\) for all \\(x \\in (a, b)\\) , the fixed point is unique. ( \\(F\\) is contractive) Smaller slope will lead to faster convergence! Proof Existence of fixed point: Intermediate value theorem on \\(F(a) - a > 0\\) and \\(F(b) - b < 0\\) . Unique fixed point: Mean value theorem on two fixed points, must exist slope \\(1\\) between them, but this contradicts the slope bound condition. Error analysis Order of convergence is \\(m\\) , where \\(F^{(k)}(x^*) = 0\\) for \\(1 \\leq k \\leq m-1\\) but \\(F^{(k)}(x^*) \\neq 0\\) Proof \\(e_{n+1} = x_{n+1} - x^* = F(x_n) - F(x*) = F'(x^*)(x_n - x^*) + \\cdots + \\frac{F^{(m-1)}(x^*)}{(m-1)!}(x_n - x^*)^{m-1} + \\frac{F^{(m)}(\\delta_n)}{m!}(x_n - x^*)^{m} = \\frac{F^{(m)}(\\delta_n)}{m!}e_n^m\\) So \\(\\lim_{n\\to\\infty}\\frac{|e_{n+1}|}{|e_n|^m} = \\frac{1}{m!}|F^{(m)}(x^*)|\\)","title":"Chapter 2: Solutions of Nonlinear Equations"},{"location":"math/3601/ch2.html#chapter-2-solutions-of-nonlinear-equations","text":"","title":"Chapter 2: Solutions of Nonlinear Equations"},{"location":"math/3601/ch2.html#bisection-method","text":"Based on the intermediate value theorem . If \\(f\\) is continuous on \\([a, b]\\) , and if \\(f(a) \\cdot f(b) < 0\\) (sign flip), then \\(f\\) must have a zero in \\((a, b)\\) .","title":"Bisection method"},{"location":"math/3601/ch2.html#main-loop","text":"Set \\(c = (a+b)/2\\) and test whether \\(f(a)f(c) < 0\\) . If this is true, then \\(f\\) has a zero in \\((a, c)\\) (the signs of \\(f(a)\\) and \\(f(c)\\) are different). So we rename \\(c\\) as \\(b\\) and start again with the new interval in \\([a, b]\\) . Else if \\(f(a)f(c) > 0\\) , then \\(f(c)f(b) < 0\\) (the sign of \\(f(a)\\) and \\(f(c)\\) are the same). In this case, rename \\(c\\) as \\(a\\) . Repeat until the length of the interval or \\(f(c)\\) is small enough . Example Find root of \\(e^x = sin x\\) in the interval \\([-4, -3]\\) , with termination scalar \\(\\epsilon\\) . Then let \\(f(x) = e^x - sin x\\) . Finding zero of \\(f(x)\\) is equivalent to finding root in original equation. \\(f(-4)f(-3) < 0\\) Then continue iterating until \\(f(c) < \\epsilon\\) Therorem (Convergence of Bisection Method) The limits \\(\\lim_{n\\to\\infty} a_n\\) and \\(\\lim_{n\\to\\infty} b_n\\) exist. Proof \\(a_0 \\leq a_1 \\leq \\cdots \\leq b_0\\) The sequence \\(a_n\\) is nondecreasing and bounded above , so \\(\\lim_{n\\to\\infty} a_n\\) exists. Similarly, \\(\\lim_{n\\to\\infty} b_n\\) exists. If \\(c_n = (a_n + b_n) / 2\\) and \\(\\lim_{n\\to\\infty} c_n = r\\) , then \\(|r - c_n| \\leq 2^{-(n+1)}(b_0 - a_0)\\) Proof \\(b_1 - a_1 = \\frac{1}{2}(b_0 - a_0)\\) \\(b_2 - a_2 = \\frac{1}{2}(b_1 - a_1) = \\frac{1}{4}(b_0 - a_0)\\) \\(\\cdots\\) \\(b_n - a_n = \\frac{1}{2^n}(b_0 - a_0) = 2^{-n}(b_0 - a_0)\\) Thus, \\(\\lim_{n\\to\\infty}b_n - \\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}2^{-n}(b_0-a_0) = 0\\) Therefore \\(\\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}b_n \\equiv r\\) Since \\(f\\) is continuous, \\(\\lim_{n\\to\\infty}f(a_n) = f(\\lim_{n\\to\\infty}f(a_n)) = r\\) and \\(\\lim_{n\\to\\infty}f(b_n) = f(\\lim_{n\\to\\infty}f(b_n)) = r\\) \\(0 \\geq f(a_n)f(b_n) \\implies 0 \\geq [f(r)]^2 \\implies f(r) = 0\\) \\(|r - c_n| \\leq \\frac{1}{2}(b_n - a_n) = 2^{-(n+1)}(b_0 - a_0)\\)","title":"Main loop"},{"location":"math/3601/ch2.html#analysis","text":"For an error \\(\\epsilon > 0\\) , if \\(\\frac{1}{2}(b_n - a_n) = 2^{-(n+1)}(b_0 - a_0) \\le \\epsilon\\) , we need to choose \\(n > \\frac{\\ln[(b_0 - a_0)/\\epsilon]}{\\ln 2} - 1\\) Derivation \\(2^{-(n+1)}(b_0 - a_0) \\leq \\epsilon\\) \\(-(n+1)\\ln 2 + \\ln(b_0 - a_0) \\leq \\ln \\epsilon\\) \\(-\\ln 2 + \\ln(b_0 - a_0) - \\ln \\epsilon \\leq n \\ln 2\\) \\(n > \\frac{\\ln[(b_0 - a_0)/\\epsilon]}{\\ln 2} - 1\\) Linear convergence: \\(|r - c_n| \\leq \\frac{1}{2}(b_n - a_n), |r - c_{n+1}| \\leq \\frac{1}{2}(\\frac{1}{2}(b_n - a_n)) \\implies |r - c_{n+1}| \\approx \\frac{1}{2}|r - c_{n}|\\) . Reliable but slow . Guaranteed to work if \\(f(x)\\) is continuous.","title":"Analysis"},{"location":"math/3601/ch2.html#newton-raphsons-method","text":"\\[ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} \\] Derivation By Taylor's theorem, \\(0 = f(r) = f(x + h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(\\delta)\\) Treat \\(\\frac{h^2}{2}f''(\\delta)\\) as small and ignore the \\(O(h^2)\\) term. Then \\(f(x) + hf'(x) = 0 \\implies h = -\\frac{f(x)}{f'(x)}\\) Linearize the function \\(f(x)\\) (tangent line) to approximate the curve of \\(f(x)\\) and find the zero of the tangent line. Faster than bisection method, but might diverge. Only have local convergence . Example Q: Use Newton's method to find \\(1/b > 0\\) without using division. Solution: Use \\(f(x) = b-1/x\\) . Then \\(1/b\\) is the root of \\(f(x) = 0\\) . \\(f'(x) = \\frac{1}{x^2}\\) \\(x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} = x_k - \\frac{b - (1/x_k)}{1/x^2} = x_k(2-bx_k)\\) Extension to matrix inverse : \\(X_{k+1} = X_k (2I - AX_k)\\)","title":"Newton-Raphson's Method"},{"location":"math/3601/ch2.html#error-analysis","text":"Quadratic convergence Assumptions : \\(f''(x)\\) continuous, \\(f'(r) \\neq 0\\) Danger If the root has multiplicity greater than one, the convergence rate is linear . Theorem Let \\(f''\\) be continuous and \\(r\\) satisfy \\(f(r) = 0, f'(r) \\neq 0\\) . Then ther is a neighborhood of \\(r\\) and a constant \\(C\\) such that if \\(x_0\\) is in this neighborhood, the successive points generated become steadily closer to \\(r\\) and satisfy \\[ |x_{n+1} - r| \\leq C |x_n - r|^2 \\] Proof (Taylor's theorem) \\(e_n = x_n - r\\) Assume: \\(f''(x)\\) continuous, \\(f'(r) \\neq 0\\) \\(e_{n+1} = x_{n+1} - r = x_n - \\frac{f(x_n)}{f'(x_n)} - r = \\frac{e_n \\cdot f'(x_n) - f(x_n)}{f'(x_n)}\\) By Taylor's theorem, \\(0 = f(r) = f(x_n - e_n) = f(x_n) - e_n f'(x_n) + \\frac{1}{2} e_n^2 f''(\\delta)\\) \\(e_n f'(x_n) - f(x_n) = \\frac{1}{2} e_n^2 f''(\\delta)\\) Substituting, \\(e_{n+1} = \\frac{\\frac{1}{2} e_n^2 f''(\\delta)}{f'(x_n)}\\) As \\(n\\to\\infty\\) , \\(\\delta\\to r\\) and \\(x_n\\to r\\) . So \\(\\frac{e_{n+1}}{e_n^2} = \\frac{\\frac{1}{2} f''(r)}{f'(r)} = C\\)","title":"Error analysis"},{"location":"math/3601/ch2.html#secant-method","text":"Motivation: Calculationg \\(f'(x)\\) is difficult. Why not use an approximation of \\(f'(x)\\) instead? Approximate \\(f'(x_n)\\) as \\(\\frac{f(x_n)-f(x_{n-1})}{f(x_n)-f(x_{n-1})}\\) So the iterative scheme becomes \\[ x_{n+1} = x_n - \\frac{f(x_n)-f(x_{n-1})}{f(x_n)-f(x_{n-1})} f(x_n) \\] Slower than Newton's method (proof not required) Practical considerations for Newton's method Difficulty in calculating \\(f'(x)\\) Failure of the method to converge to the root Overshoot and diverge away from the root Stationary point \\(f'(x) = 0\\) Poor initial estimate leading to non-convergence Slow convergence for repeated roots If the root has multiplicity greater than one, the convergence rate is linear. Divergent loop","title":"Secant method"},{"location":"math/3601/ch2.html#system-of-nonlinear-equations","text":"\\[ \\begin{cases} f_1(x_1, x_2) = 0 \\\\ f_2(x_1, x_2) = 0 \\end{cases} \\] By Taylor's Theorem, \\[ f(x_1^{(k)} + h_1^{(k)}, x_2^{(k)} + h_2^{(k)}) = f(x_1^{(k)}, x_2^{(k)}) + h_1^{(k)} \\frac{\\partial f}{\\partial x_1}(x_1^{(k)}, x_2^{(k)}) + h_2^{(k)} \\frac{\\partial f}{\\partial x_2}(x_1^{(k)}, x_2^{(k)}) + O(h^2) = 0 \\] Jacobian matrix: \\[ J(X^{(k)}) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}(X^{(k)}) & \\frac{\\partial f_1}{\\partial x_2}(X^{(k)})\\\\ \\frac{\\partial f_2}{\\partial x_1}(X^{(k)}) & \\frac{\\partial f_2}{\\partial x_2}(X^{(k)}) \\end{bmatrix} \\] \\[ Jh = -F(X^{(k)}) \\] \\[ X^{(k+1)} = X^{(k)} - (J(X^{(k)}))^{-1}F(X^{(k)}) \\] Multi-variable Newton's method still enjoys the property of quadratic convergence of the starting point is near the exact solution. However, a significant weakness is the requirement that, in each iteration: a Jacobian matrix (involving \\(n^2\\) partial derivatives) has to be evaluated an \\(n \\times n\\) linear system involving this matrix must be solved.","title":"System of nonlinear equations"},{"location":"math/3601/ch2.html#rescaled-simple-iteration-reduce-cost-of-approximating-fxk-1","text":"Simplest way: Not to recompute it, just use an initial approximation \\(F'(X^{(0)})^{(-1)}\\) throughout the iteration Difficult to guarantee that this condition will be met... Convergence is generally linear Or take one Newton step followed by several additional iteration steps without updating the Jacobian matrix. Generalized secant method (Broyden method) (not required) Begin with a rough guess of the Jacobian, and use successive evaluations of \\(F\\) and its gradient to evaluate the guess of the Jacobian \\(J\\) . Each iteration is far less costly, but more iterations are needed.","title":"Rescaled simple iteration: Reduce cost of approximating \\(F'(X^{(k)})^{(-1)}\\)"},{"location":"math/3601/ch2.html#fixed-point-iteration","text":"\\[ x_{n+1} = g(x_n) \\] Finding a zero/root for \\(f(x)\\) \\(\\implies\\) Finding a fixed point for \\(g(x) = x - f(x)\\) Contractive Mapping Theorem Contractive if there exists \\(\\lambda \\in (0, 1)\\) s.t. \\(|F(x) - F(y)| \\leq \\lambda |x - y|\\) Suppose \\(F: C \\to C\\) where \\(C\\) is a closed set, is a contractive mapping. Then \\(F\\) has a unique fixed point. This fixed point is the limit of every sequence obtained by \\(x_{n+1} = F(x_n)\\) with any starting point \\(x_0 \\in C\\) Proof of convergence of \\(x_n\\) Write \\(x_n = x_0 + (x_1 - x_0) + (x_2 - x_1) + \\cdots (x_n - x_{n-1})\\) Need to show that \\(\\sum^{\\infty}_{n=1}(x_n - x_{n-1})\\) converges. Suffices to show that \\(\\sum^{\\infty}_{n=1}|x_n - x_{n-1}|\\) converges. Since \\(F\\) is contractive, we have \\(|x_n - x_{n-1}| = |F(x_{n-1}) - F(x_{n-2})| \\leq \\lambda |x_{n-1} - x_{n-2}|\\) This argument can be repeated: \\(|x_n - x_{n-1}| \\leq \\lambda |x_{n-1} - x_{n-2}| \\leq \\lambda^2|x_{n-2} - x_{n-3}| \\leq \\cdots \\leq \\lambda^{n-1}|x_1 - x_0|\\) Then we have \\(\\sum^{\\infty}_{n=1}|x_n - x_{n-1}| \\leq \\sum^{\\infty}_{n=1}\\lambda^{n-1}|x_1-x_0| = |x_1-x_0|\\sum^{\\infty}_{n=1}\\lambda^{n-1}=\\frac{1}{1-\\lambda}|x_1-x_0|\\) So the sum is bounded, hence it converges, and thus the sequence \\(x_n\\) converges for any inital point \\(x_0\\) . Proof of uniqueness of fixed point Let \\(x\\) and \\(y\\) both be fixed points of \\(F\\) . Then \\(|x - y| = |F(x) - F(y)| \\leq \\lambda |x - y|\\) Since \\(\\lambda < 1\\) , this forces \\(|x - y| = 0\\) . That is, \\(x = y\\) . Example Q: Prove that the sequence \\(x_0 = -15, x_{n+1} = 3 - \\frac{1}{2}|x_n|\\) is convergent. A: \\(|F(x) - F(y)| = |3 - \\frac{1}{2}|x| - 3 + \\frac{1}{2}|y|| = \\frac{1}{2}||y|-|x|| \\overset{\\mathrm{Triangle~inequality}}{\\leq} \\frac{1}{2}|y-x|\\) Useful convergence criteria If \\(F \\in C[a, b]\\) such that \\(a \\leq F(x) \\leq b\\) for all \\(x \\in [a, b]\\) , then \\(F\\) has a fixed point in \\([a, b]\\) . ( \\(F\\) maps a closed set onto itself) In addition, if \\(F'(x)\\) exists in \\((a, b)\\) and there exists a positive constant \\(M < 1\\) such that \\(|F'(x)| \\leq M < 1\\) for all \\(x \\in (a, b)\\) , the fixed point is unique. ( \\(F\\) is contractive) Smaller slope will lead to faster convergence! Proof Existence of fixed point: Intermediate value theorem on \\(F(a) - a > 0\\) and \\(F(b) - b < 0\\) . Unique fixed point: Mean value theorem on two fixed points, must exist slope \\(1\\) between them, but this contradicts the slope bound condition.","title":"Fixed point iteration"},{"location":"math/3601/ch2.html#error-analysis_1","text":"Order of convergence is \\(m\\) , where \\(F^{(k)}(x^*) = 0\\) for \\(1 \\leq k \\leq m-1\\) but \\(F^{(k)}(x^*) \\neq 0\\) Proof \\(e_{n+1} = x_{n+1} - x^* = F(x_n) - F(x*) = F'(x^*)(x_n - x^*) + \\cdots + \\frac{F^{(m-1)}(x^*)}{(m-1)!}(x_n - x^*)^{m-1} + \\frac{F^{(m)}(\\delta_n)}{m!}(x_n - x^*)^{m} = \\frac{F^{(m)}(\\delta_n)}{m!}e_n^m\\) So \\(\\lim_{n\\to\\infty}\\frac{|e_{n+1}|}{|e_n|^m} = \\frac{1}{m!}|F^{(m)}(x^*)|\\)","title":"Error analysis"},{"location":"math/3601/ch3.html","text":"Chapter 3: Numerical Linear Algebra \\[ Ax = b \\] Easy to solve for lower triangular and upper triangular matrix: Lower triangular matrix \\(Lx = b\\) : Forward substitution Upper triangular matrix \\(Ux = b\\) : Backward substitution Computational complexity: \\(O(n^2)\\) Reduce general matrix to triangular matrix via Gaussian Elimination , complexity \\(O(n^3)\\) . Gaussian Elimination with Partial Pivoting In Gaussian elimination, a row interchange was needed when one of the pivot elements \\(a^{(k)}_{kk}\\) is \\(0\\) . When \\(a^{(k)}_{kk}\\) is nonzero but very small, pivoting using this row may cause round-off error due to large multipliers. Partial pivoting strategy Select an element in the same column that is below the diagonal and has the largest absolue value, and use that row as the pivot. Swap rows if neccessary. LU Factorization Decompose \\(A = LU\\) ( \\(O(\\frac{2}{3}n^3)\\) , smaller constant) Solve \\(Ly = b\\) ( \\(O(n^2)\\) ) Solve \\(Ux = y\\) ( \\(O(n^2)\\) ) Finding the decomposition: For each cell in \\(A\\) , just solve the multiplication equation. (Re: Doolittle algorithm) Iterative methods Norm \\(L_2\\) norm: \\(||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}\\) \\(L_1\\) norm: \\(||x||_1 = |x_1| + |x_2| + \\cdots + |x_n|\\) \\(L_\\infty\\) norm: \\(||x||_\\infty = \\max{(|x_1|, |x_2|, \\cdots, |x_n|)}\\) Matrix norm induced by a given vector norm: \\(||A|| = \\sup_{x \\neq 0} \\frac{||Ax||}{||x||} \\implies ||Ax|| \\leq ||A||||x||\\) Special cases refer to slides 1-norm is max col sum of abs elements inf-norm is max row sum of abs elements 2-norm is sqrt of largest eigenvalue of \\(A^TA\\) Bound on relative error: \\(\\frac{||\\delta x||}{||x||} \\leq ||A||||A^{-1}||\\frac{||\\delta b||}{||b||}\\) (smaller is better) Condition number: \\(||A||||A^{-1}||\\) (smaller is better) Derivation Jacobi method \\[ \\begin{align*} A = D + R\\\\ R = \\begin{bmatrix} 0 & a_{12} & a_{13} \\\\ a_{21} & 0 & a_{23} \\\\ a_{31} & a_{32} & 0 \\\\ \\end{bmatrix}\\\\ D = \\begin{bmatrix} a_{11} & 0 & 0 \\\\ 0 & a_{22} & 0 \\\\ 0 & 0 & a_{33} \\\\ \\end{bmatrix}\\\\ x^{(k+1)} = D^{-1}(b - Rx^{(k)}) = D^{-1}b - D^{-1}Rx^{(k)}\\\\ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j \\neq i}{a_{ij}x_j^{(k)}}\\right) \\end{align*} \\] Computation of \\(x_i^{(k+1)}\\) requires each element in \\(x^{(k)}\\) except itself. The iterative scheme is easy to be parallelized. General convergence condition The standard convergence condition (for any iterative method) is when the norm of the iteration matrix (i.e. \\(||D^{-1}R||\\) ) is less than \\(1\\) . Proof \\(e^{(k+1)} = X^{(k+1)} - X^* = D^{-1}b - D^{-1}Rx^{(k)}-X^*\\) \\((D+R)X^* = b \\implies X^* = D^{-1}b - D^{-1}RX^*\\) \\(e^{(k+1)} = X^{(k+1)} - X^* = D^{-1}R\\left(X^* - X^{(k)}\\right) = D^{-1}R \\cdot e^{(k)} \\implies e^{(k+1)} = (D^{-1}R)^{k+1} \\cdot e^{(0)}\\) \\(||D^{-1}R|| < 1 \\implies \\lim_{k \\rightarrow \\infty} e^{k+1} = 0\\) Strictly diagonally dominant case If the matrix \\(A\\) is strictly diagonally dominant (i.e. \\(|a_{ii}| > \\sum_{j \\neq i}|a_{ij}|)\\) (for all rows the absolute value of the diagonal element in a row is strictly greater than than the sum of absolute value of the rest of the elements in that row.), the iterative method is guaraneed to converge. The Jacobi method sometimes converges even if these conditions are not satisfied. Gauss-Seidel method \\[ \\begin{align*} A = L_* + U\\\\ x^{(k+1)} = L_*^{-1}(b - Ux^{(k)})\\\\ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j < i}{a_{ij}x_j^{(k+1)}} - \\sum_{j > i}{a_{ij}x_j^{(k)}}\\right) \\end{align*} \\] \\(L*\\) includes diagonal, \\(U\\) does not. The computation of \\(x_i^{(k+1)}\\) uses only the elements of \\(x^{(k+1)}\\) that have already been computed , and only the elements of \\(x^{(k)}\\) that have not yet advanced to iteration \\(k+1\\) . Cannot be done in parallel, the values at each iteration are dependent on the order of the original equations. Convergence speed faster than Jacobi method. General convergence condition The procedure is known to converge if \\(A\\) is symmetric positive-de\u001cnite, or \\(A\\) is strictly diagonally dominant. The Gauss-Seidel method sometimes converges even if these conditions are not satisfied. SOR method (not required) Gradient descent Assumptions: Symmetric positive definite Solving \\(Ax = b\\) is equivalent to minimizing \\(f(x) = \\frac{1}{2}x^TAx-b^Tx+c\\) Algorithm Compute the search direction \\(d_k = -\\nabla f(x_{k-1}) = b - Ax_{k-1}\\) Define \\(x_k = x_{k-1} + \\alpha_kd_k\\) , where \\(\\alpha_k\\) is chosen such that \\(f(x_k) < f(x_{k-1})\\) Convergence check. Stop or repeat. Deciding on \\(\\alpha_k\\) is minimizing \\(f(x_{k-1} + \\alpha_kd_k)\\) as a function of a single scalar variable \\(\\alpha_k\\) . After some derivation of differentiating scalar function \\(g(\\alpha) = f(x_{k-1} + \\alpha d_k)\\) , we obtain \\(\\frac{dg(\\alpha)}{d\\alpha} = \\alpha d_k^TAd_k + d_k^T(A x_{k-1}-b)\\) Solving \\(\\frac{dg(\\alpha)}{d\\alpha} = 0\\) , obtain \\(\\alpha = \\frac{d_k^T(b-Ax_{k-1})}{d_k^TAd_k}\\) For gradient descent, choose \\(d_k = b - Ax_{k-1}\\) , so \\(\\alpha_k = \\frac{||d_k||_2^2}{d_k^TAd_k}\\) Algorithm Compute the search direction \\(d_k = -\\nabla f(x_{k-1}) = b - Ax_{k-1}\\) Define \\(x_k = x_{k-1} + \\alpha_kd_k\\) , where \\(\\alpha_k = \\frac{||d_k||_2^2}{d_k^TAd_k}\\) Convergence check. Stop or repeat. Some notes: Successive search directions \\(d_{k+1}\\) and \\(d_{k}\\) are orthogonal \\(R_k = 1-\\frac{\\sigma_{min}}{\\sigma_{max}}\\) (smaller is better) Smallest and greatest eigenvalue Smaller is better (clustered, like circle) Large: Eigenvalue scattered, long and thin landscape, zig-zagging on energy landscape","title":"Chapter 3: Numerical Linear Algebra"},{"location":"math/3601/ch3.html#chapter-3-numerical-linear-algebra","text":"\\[ Ax = b \\] Easy to solve for lower triangular and upper triangular matrix: Lower triangular matrix \\(Lx = b\\) : Forward substitution Upper triangular matrix \\(Ux = b\\) : Backward substitution Computational complexity: \\(O(n^2)\\) Reduce general matrix to triangular matrix via Gaussian Elimination , complexity \\(O(n^3)\\) .","title":"Chapter 3: Numerical Linear Algebra"},{"location":"math/3601/ch3.html#gaussian-elimination-with-partial-pivoting","text":"In Gaussian elimination, a row interchange was needed when one of the pivot elements \\(a^{(k)}_{kk}\\) is \\(0\\) . When \\(a^{(k)}_{kk}\\) is nonzero but very small, pivoting using this row may cause round-off error due to large multipliers. Partial pivoting strategy Select an element in the same column that is below the diagonal and has the largest absolue value, and use that row as the pivot. Swap rows if neccessary.","title":"Gaussian Elimination with Partial Pivoting"},{"location":"math/3601/ch3.html#lu-factorization","text":"Decompose \\(A = LU\\) ( \\(O(\\frac{2}{3}n^3)\\) , smaller constant) Solve \\(Ly = b\\) ( \\(O(n^2)\\) ) Solve \\(Ux = y\\) ( \\(O(n^2)\\) ) Finding the decomposition: For each cell in \\(A\\) , just solve the multiplication equation. (Re: Doolittle algorithm)","title":"LU Factorization"},{"location":"math/3601/ch3.html#iterative-methods","text":"","title":"Iterative methods"},{"location":"math/3601/ch3.html#norm","text":"\\(L_2\\) norm: \\(||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}\\) \\(L_1\\) norm: \\(||x||_1 = |x_1| + |x_2| + \\cdots + |x_n|\\) \\(L_\\infty\\) norm: \\(||x||_\\infty = \\max{(|x_1|, |x_2|, \\cdots, |x_n|)}\\) Matrix norm induced by a given vector norm: \\(||A|| = \\sup_{x \\neq 0} \\frac{||Ax||}{||x||} \\implies ||Ax|| \\leq ||A||||x||\\) Special cases refer to slides 1-norm is max col sum of abs elements inf-norm is max row sum of abs elements 2-norm is sqrt of largest eigenvalue of \\(A^TA\\) Bound on relative error: \\(\\frac{||\\delta x||}{||x||} \\leq ||A||||A^{-1}||\\frac{||\\delta b||}{||b||}\\) (smaller is better) Condition number: \\(||A||||A^{-1}||\\) (smaller is better) Derivation","title":"Norm"},{"location":"math/3601/ch3.html#jacobi-method","text":"\\[ \\begin{align*} A = D + R\\\\ R = \\begin{bmatrix} 0 & a_{12} & a_{13} \\\\ a_{21} & 0 & a_{23} \\\\ a_{31} & a_{32} & 0 \\\\ \\end{bmatrix}\\\\ D = \\begin{bmatrix} a_{11} & 0 & 0 \\\\ 0 & a_{22} & 0 \\\\ 0 & 0 & a_{33} \\\\ \\end{bmatrix}\\\\ x^{(k+1)} = D^{-1}(b - Rx^{(k)}) = D^{-1}b - D^{-1}Rx^{(k)}\\\\ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j \\neq i}{a_{ij}x_j^{(k)}}\\right) \\end{align*} \\] Computation of \\(x_i^{(k+1)}\\) requires each element in \\(x^{(k)}\\) except itself. The iterative scheme is easy to be parallelized. General convergence condition The standard convergence condition (for any iterative method) is when the norm of the iteration matrix (i.e. \\(||D^{-1}R||\\) ) is less than \\(1\\) . Proof \\(e^{(k+1)} = X^{(k+1)} - X^* = D^{-1}b - D^{-1}Rx^{(k)}-X^*\\) \\((D+R)X^* = b \\implies X^* = D^{-1}b - D^{-1}RX^*\\) \\(e^{(k+1)} = X^{(k+1)} - X^* = D^{-1}R\\left(X^* - X^{(k)}\\right) = D^{-1}R \\cdot e^{(k)} \\implies e^{(k+1)} = (D^{-1}R)^{k+1} \\cdot e^{(0)}\\) \\(||D^{-1}R|| < 1 \\implies \\lim_{k \\rightarrow \\infty} e^{k+1} = 0\\) Strictly diagonally dominant case If the matrix \\(A\\) is strictly diagonally dominant (i.e. \\(|a_{ii}| > \\sum_{j \\neq i}|a_{ij}|)\\) (for all rows the absolute value of the diagonal element in a row is strictly greater than than the sum of absolute value of the rest of the elements in that row.), the iterative method is guaraneed to converge. The Jacobi method sometimes converges even if these conditions are not satisfied.","title":"Jacobi method"},{"location":"math/3601/ch3.html#gauss-seidel-method","text":"\\[ \\begin{align*} A = L_* + U\\\\ x^{(k+1)} = L_*^{-1}(b - Ux^{(k)})\\\\ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left(b_i - \\sum_{j < i}{a_{ij}x_j^{(k+1)}} - \\sum_{j > i}{a_{ij}x_j^{(k)}}\\right) \\end{align*} \\] \\(L*\\) includes diagonal, \\(U\\) does not. The computation of \\(x_i^{(k+1)}\\) uses only the elements of \\(x^{(k+1)}\\) that have already been computed , and only the elements of \\(x^{(k)}\\) that have not yet advanced to iteration \\(k+1\\) . Cannot be done in parallel, the values at each iteration are dependent on the order of the original equations. Convergence speed faster than Jacobi method. General convergence condition The procedure is known to converge if \\(A\\) is symmetric positive-de\u001cnite, or \\(A\\) is strictly diagonally dominant. The Gauss-Seidel method sometimes converges even if these conditions are not satisfied.","title":"Gauss-Seidel method"},{"location":"math/3601/ch3.html#sor-method-not-required","text":"","title":"SOR method (not required)"},{"location":"math/3601/ch3.html#gradient-descent","text":"Assumptions: Symmetric positive definite Solving \\(Ax = b\\) is equivalent to minimizing \\(f(x) = \\frac{1}{2}x^TAx-b^Tx+c\\) Algorithm Compute the search direction \\(d_k = -\\nabla f(x_{k-1}) = b - Ax_{k-1}\\) Define \\(x_k = x_{k-1} + \\alpha_kd_k\\) , where \\(\\alpha_k\\) is chosen such that \\(f(x_k) < f(x_{k-1})\\) Convergence check. Stop or repeat. Deciding on \\(\\alpha_k\\) is minimizing \\(f(x_{k-1} + \\alpha_kd_k)\\) as a function of a single scalar variable \\(\\alpha_k\\) . After some derivation of differentiating scalar function \\(g(\\alpha) = f(x_{k-1} + \\alpha d_k)\\) , we obtain \\(\\frac{dg(\\alpha)}{d\\alpha} = \\alpha d_k^TAd_k + d_k^T(A x_{k-1}-b)\\) Solving \\(\\frac{dg(\\alpha)}{d\\alpha} = 0\\) , obtain \\(\\alpha = \\frac{d_k^T(b-Ax_{k-1})}{d_k^TAd_k}\\) For gradient descent, choose \\(d_k = b - Ax_{k-1}\\) , so \\(\\alpha_k = \\frac{||d_k||_2^2}{d_k^TAd_k}\\) Algorithm Compute the search direction \\(d_k = -\\nabla f(x_{k-1}) = b - Ax_{k-1}\\) Define \\(x_k = x_{k-1} + \\alpha_kd_k\\) , where \\(\\alpha_k = \\frac{||d_k||_2^2}{d_k^TAd_k}\\) Convergence check. Stop or repeat. Some notes: Successive search directions \\(d_{k+1}\\) and \\(d_{k}\\) are orthogonal \\(R_k = 1-\\frac{\\sigma_{min}}{\\sigma_{max}}\\) (smaller is better) Smallest and greatest eigenvalue Smaller is better (clustered, like circle) Large: Eigenvalue scattered, long and thin landscape, zig-zagging on energy landscape","title":"Gradient descent"},{"location":"math/3601/ch4.html","text":"Chapter 4: Approximating functions Function interpolation Motivating question Given a set of points \\((x_i, y_i)\\) for \\(i = 0, 1, \\cdots, n\\) , where the \\(x_i\\) are distinct values of the independent variable and the \\(y_i\\) are the corresponding value of some function \\(f\\) at \\(x_i\\) , that is \\(y_i = f(x_i)\\) . Determine a function \\(g\\) that satisfies \\(g(x_i) = f(x_i)\\) and predicts the value of \\(f\\) at some value \\(x\\) not listed among \\(x_i\\) Goal Interpolation: The function \\(g\\) is determined by requiring the error to be zero at each of the \\(x_i\\) , i.e. \\(g(x_i) = y_i\\) . Linear interpolation: \\(g(x; a_0, \\cdots, a_n) = a_0g_0(x) + a_1g_1(x) + \\cdots + a_ng_n(x)\\) Typical linear interpolation methods include: Polynomial interpolation Spline (piecewise polynomial) Trignometric interpolation Polynomial interpolation Goal: Seek a polynomial \\(p\\) of the lowest degree for which \\(p(x_i) = y_i, (0 \\leq i \\leq n)\\) Polynomials are smooth functions Easy to store, manipulate, take derivative/antiderivative However, polynomials are rigid and may create problems at boundaries (Re: Runge's phenomenon) Theorem (degree of polynomial interpolation) If \\(x_0, x_1, \\cdots, x_n\\) are distinct real numbers, then for arbitrary values \\(y_0, y_1, \\cdots, y_n\\) , there is a unique polynomial \\(p_n\\) of degree at most \\(n\\) such that \\(p_n(x_i) = y_i\\) . Proof (uniqueness) Suppose there were two such polynomails \\(p_n\\) and \\(q_n\\) . Then \\(r_n(x) = (p_n - q_n)(x_i) = 0, 0 \\leq i \\leq n\\) . Since the degree of \\(r_n\\) can be at most \\(n\\) , this polynomial can have at most \\(n\\) zeros if it is not the \\(0\\) polynomial. Since the \\(x_i\\) are distinct, \\(r_n\\) has \\(n+1\\) zeros; it must therefore be \\(0\\) . Hence \\(p_n \\equiv q_n\\) . Proof (existence) The proof for existence actually let us know some ways to generate the polynomial. So we leave it to the next part. Theorem (Polynomial interpolation error) To each \\(x\\) there corresponds a point \\(\\eta_x \\in (a, b)\\) such that: \\[ f(x) - p(x) = \\frac{1}{(n+1)!} f^{(n+1)}(\\eta_x) \\prod^n_{i=0}(x-x_i) \\] Proof \\(x\\) is fixed. Let \\(w(t) = \\prod^n_{i=0} (t-x_i)\\) , \\(\\phi(t) \\equiv f(t) - p(t) + \\lambda w(t)\\) , where \\(\\phi(x) = 0\\) and \\(\\lambda = \\frac{f(x)-p(x)}{w(x)}\\) \\(\\phi \\in C^{n+1}[a, b]\\) , and \\(\\phi\\) vanishes at the \\(n+2\\) points \\(x, x_0, x_1, \\cdots, x_n\\) . By Mean-Value Theorem, \\(\\phi'\\) has at least \\(n+1\\) distinct zeros in \\([a, b]\\) (one zero in \\(\\phi'\\) between each original zero in \\(phi\\) ) Similarly, \\(\\phi''\\) has at least \\(n\\) distinct zeros in (a, b). Eventually, \\(\\phi^{(n+1)}\\) has at least one zero, say \\(\\eta_x\\) in \\((a, b)\\) . Now \\(w^{(n+1)}(t)\\) = \\frac{d {n+1}}{dt }\\prod^n_{i=1}(t-x_i) = (n+1)!$ (polynomial in \\(t\\) ) \\(\\phi^{(n+1)} = f^{(n+1)} - p^{(n+1)} - \\lambda w^{(n+1)} = f^{(n+1)} - (n+1)!\\lambda\\) ( \\(p\\) has degree at most \\(n\\) ) Lastly, \\(0 = \\phi^{(n+1)}(\\eta_x) = f^{(n+1)}(\\eta_x) - (n+1)!\\frac{f(x)-p(x)}{w(x)}\\) Wolfram alpha command interpolate [(5,1),(-7,-23),(-6,-54), (0, -954)] Lagrange interpolation \\[ \\begin{align*} p_n(x) = y_0 \\ell_0(x) + y_1 \\ell_1(x) + \\cdots + y_n \\ell_n(x) \\\\ \\ell_i(x) = \\prod^n_{j=0, j \\neq i} \\frac{x-x_j}{x_i-x_j} \\end{align*} \\] where \\(\\ell_i\\) are polynomials that depend on \\(x_0, \\cdots x_n\\) but not on \\(y_0 \\cdots y_n\\) . Note that \\(y_i = p_n(x_i) = y_0 \\ell_0(x_i) + y_1 \\ell_1(x_i) + \\cdots + y_n \\ell_n(x_i)\\) . So \\(\\ell_i(x_j) = 1\\) if \\(i = j\\) and \\(\\ell_i(x_j) = 0\\) if \\(i \\neq j\\) Example \\((1, 1), (2, 8), (3, 27)\\) The interpolating polynomial is: \\(1 \\cdot \\frac{x-2}{1-2} \\cdot \\frac{x-3}{1-3} + 8 \\cdot \\frac{x-1}{2-1} \\cdot \\frac{x-3}{2-3} + 27 \\cdot \\frac{x-1}{3-1} \\cdot \\frac{x-2}{3-2} = 6x^2 - 11x + 6\\) interpolate [(1, 1) (2, 8) (3, 27)] Note \\(\\sum^n_{i=0}\\ell_i(x) = 1\\) Try interpolating \\(P(x) = f(x) = 1\\) While elegant and theoretically important, the Lagrange interpolation formula is not suitable for actual computations, because: the evaluation of the Lagrange interpolation requires more floating point operations than necessary (Re: Horner's method) difficult to obtain derivative the inlcusion of additional data points require the reevalutaion of all Lagrange polynomials Runge's phenomenon : If the points are equally spaced, then there is large oscillation above and below the true function. Newton interpolation \\[ \\begin{align*} p_0(x_0) = c_0 = y_0 \\\\ p_k(x) = p_{k-1}(x) + c_k(x-x_0)(x-x_1)\\cdots(x-x_{k-1})\\\\ c_k = \\frac{y_k - p_{k-1}(x)}{(x_k-x_0)(x_k-x_1)\\cdots(x_k-x_{k-1})} \\end{align*} \\] Divided differences notation \\[ \\begin{align*} c_0 = p_n(x_0) = y_0 = [y_0]\\\\ c_1 = \\frac{y_1 - y_0}{x_1 - x_0} = \\frac{[y_1] - [y_0]}{x_1 - x_0} = [y_0, y_1]\\\\ c_k = \\frac{[y_1, \\cdots, y_k] - [y_0, \\cdots, y_{k-1}]}{x_k - x_0} = [y_0, \\cdots, y_k]\\\\ p_n(x) = [y_0] + \\sum^n_{k=1}[y_0, y_1, \\cdots, y_k](x-x_0) \\cdots (x-x_{k-1}) \\end{align*} \\] Example \\((0, 1), (1, 3), (3, 2)\\) \\(k=0\\) \\(k=1\\) \\(k=2\\) \\(x_0 = 0\\) \\([y_0] = 1\\) * \\(x_1 = 1\\) \\([y_1] = 3\\) \\([y_0, y_1] = 2\\) * \\(x_2 = 3\\) \\([y_2] = 2\\) \\([y_1, y_2] = -1/2\\) \\([y_0, y_1, y_2] = -5/6\\) * \\(p_n = [y_0] + [y_0, y_1](x-x_0) + [y_0, y_1, y_2](x-x_0)(x-x_1)\\) \\(p_n = 1 + 2(x-x_0) + -\\frac{5}{6}(x-x_0)(x-x_1) = -\\frac{5}{6}x^2 + \\frac{17}{6}x + 1\\) Hermite interpolation (skipped) Spline interpolation A spline function consists of polynomial pieces on subintervals joined together with certain continuity conditions. Formally suppose that \\(n+1\\) points \\(t_0, t_1, \\cdots, c_n\\) have been specified and satisfy \\(t_0 < t1 < \\cdots < t_n\\) . These points are called knots . Suppose also that an integer \\(k \\geq 0\\) has been perscribed. A spline function of degree \\(k\\) having knots \\(t_0, \\cdots, t_n\\) is a function \\(S\\) such that: On each interval \\([t_{i-1}, t_i]\\) , \\(S\\) is a polynomial of degree \\(\\leq k\\) \\(S\\) has a continuous \\((k-1)\\) st derivative on \\([t_0, t_n]\\) i.e. Piecewise polynomial of degree at most \\(k\\) having continuous derivatives of all orders up to \\(k-1\\) . Spline of degree 0: Piecewise constant, step funtion (right continuous), intervals do not intersect Spline of degree 1: Piecewise linear, continuous Checking spline: Left hand and right hand limit of value and (deg-1) derivatives are continuous Cubic spline Derivation \\(z_i = S''(t_i)\\) \\(S_i^{''}(x) = A_ix+B_i\\) \\(A_it_i + B_i = z_i, A_it_{i+1} + B_i = z_{i+1}\\) \\(h_i = t_{i+1} - t_i\\) \\(A_i = \\frac{z_{i+1}-z_i}{h_i}, B_i = z_i - \\frac{z_{i+1} - z_i}{h_i} t_i\\) \\(S_i^{''}(x) = A_ix+B_i = \\frac{z_{i+1}}{h_i}(x-t_i) + \\frac{z_i}{h_i}(t_{i+1}-x)\\) , straight line between \\(z_i\\) and \\(z_{i+1}\\) \\(S_i^{'}(x) = -\\frac{1}{2}\\frac{z_i}{h_i}(t_{i+1}-x)^2 + \\frac{1}{2}\\frac{z_{i+1}}{h_i}(x-t_i)^2+C_i\\) (integration) \\(S_i(x) = \\frac{z_i}{6h_i}(t_{i+1}-x)^3 + \\frac{z_{i+1}}{6h_i}(x-t_i)^3 + C_i(x-t_i) + D_i\\) Interpolation conditions \\(S_i(t_i) = y_i\\) and \\(S_i(t_{i+1}) = y_{i+1}\\) , find \\(C_i\\) and \\(D_i\\) : \\(D_i = y_i - \\frac{z_ih_i^2}{6}, C_i = \\frac{y_{i+1}-y_i}{h_i} + \\frac{h_i}{6}(z_i-z_{i+1})\\) Determining \\(z_1, z_2, \\cdots, z_{n-1}\\) : Use continuity conditions for \\(S'\\) at the interior knots \\(t_i\\) : \\(S_{i-1}^{'}(t_i) = S_{i}^{'}(t_i)\\) TODO Theorem (Optimality of Natural Cubic Spline) Let \\(f^{''}\\) be continuous on \\([a, b]\\) and let \\(a = t_0 < t_1 < \\cdots < t_n = b\\) . If \\(S\\) is the natural cubic spline interpolating \\(f\\) at the knots \\(t_i\\) , then \\(\\int^b_a{[S^{''}(x)]^2} dx \\leq \\int^b_a{[f^{''}(x)]^2} dx\\) Proof Let \\(g \\equiv f-S\\) . \\((f^{''})^2 = (S^{''} + g^{''})^2 = (S^{''})^2 + (g^{''})^2 + 2S^{''}g^{''}\\) \\(\\int^a_b{(f^{''})^2}dx = \\int^a_b{(S^{''})^2}dx + \\int^a_b{(g^{''})^2}dx + \\int^a_b{2S^{''}g^{''}}dx\\) Note that \\(\\int^a_b{(g^{''})^2}dx \\geq 0\\) . So to prove the inequality, we can choose to show that \\(\\int^a_b{2S^{''}g^{''}}dx \\geq 0\\) . Idea: Integration by parts \\(\\int^a_b{2S^{''}g^{''}}dx = \\sum^n_{i=1}{\\int^{t_{i}}_{t_{i-1}}{2S^{''}g^{''}}dx} = \\sum^n_{i=1}{S^{''}g^{'}(t_i) - {S^{''}g^{'}(t_{i-1}) - \\int^{t_{i}}_{t_{i-1}}{2S^{'''}g^{'}}dx}\\) TODO Function approximation Least squares \\[ \\begin{align*} X\\beta = Y \\\\ \\hat{\\beta} = (X^TX)^{-1}X^Ty \\end{align*} \\] Function approximation over interval Example Let \\(f(x) = e^x\\) , let \\(p(x) = \\alpha_0 + \\alpha_1x\\) . Approximate \\(f(x)\\) over \\([-1, 1]\\) . So choose \\(\\alpha_0, \\alpha_1\\) to minimize \\(g(\\alpha_0, \\alpha_1) = \\int_{-1}^1{[e^x - \\alpha_0 - \\alpha_1]^2 dx}\\) \\(0 = \\frac{\\partial g}{\\partial \\alpha_0} = 2 \\int_{-1}^1{[e^x - \\alpha_0 - \\alpha_1x] (-1)} dx\\) \\(0 = \\frac{\\partial g}{\\partial \\alpha_0} = 2 \\int_{-1}^1{[e^x - \\alpha_0 - \\alpha_1x] (-x)} dx\\) Solving gives \\(2\\alpha_0 = e-e^{-1}\\) , \\(\\frac{2}{3}\\alpha_1 = 2e^{-1}\\) . Gram-Schmidt process Something related to orthogonal functions... E.g. Orthogonal function: Fourier series TODO","title":"Chapter 4: Approximating functions"},{"location":"math/3601/ch4.html#chapter-4-approximating-functions","text":"","title":"Chapter 4: Approximating functions"},{"location":"math/3601/ch4.html#function-interpolation","text":"Motivating question Given a set of points \\((x_i, y_i)\\) for \\(i = 0, 1, \\cdots, n\\) , where the \\(x_i\\) are distinct values of the independent variable and the \\(y_i\\) are the corresponding value of some function \\(f\\) at \\(x_i\\) , that is \\(y_i = f(x_i)\\) . Determine a function \\(g\\) that satisfies \\(g(x_i) = f(x_i)\\) and predicts the value of \\(f\\) at some value \\(x\\) not listed among \\(x_i\\) Goal Interpolation: The function \\(g\\) is determined by requiring the error to be zero at each of the \\(x_i\\) , i.e. \\(g(x_i) = y_i\\) . Linear interpolation: \\(g(x; a_0, \\cdots, a_n) = a_0g_0(x) + a_1g_1(x) + \\cdots + a_ng_n(x)\\) Typical linear interpolation methods include: Polynomial interpolation Spline (piecewise polynomial) Trignometric interpolation","title":"Function interpolation"},{"location":"math/3601/ch4.html#polynomial-interpolation","text":"Goal: Seek a polynomial \\(p\\) of the lowest degree for which \\(p(x_i) = y_i, (0 \\leq i \\leq n)\\) Polynomials are smooth functions Easy to store, manipulate, take derivative/antiderivative However, polynomials are rigid and may create problems at boundaries (Re: Runge's phenomenon) Theorem (degree of polynomial interpolation) If \\(x_0, x_1, \\cdots, x_n\\) are distinct real numbers, then for arbitrary values \\(y_0, y_1, \\cdots, y_n\\) , there is a unique polynomial \\(p_n\\) of degree at most \\(n\\) such that \\(p_n(x_i) = y_i\\) . Proof (uniqueness) Suppose there were two such polynomails \\(p_n\\) and \\(q_n\\) . Then \\(r_n(x) = (p_n - q_n)(x_i) = 0, 0 \\leq i \\leq n\\) . Since the degree of \\(r_n\\) can be at most \\(n\\) , this polynomial can have at most \\(n\\) zeros if it is not the \\(0\\) polynomial. Since the \\(x_i\\) are distinct, \\(r_n\\) has \\(n+1\\) zeros; it must therefore be \\(0\\) . Hence \\(p_n \\equiv q_n\\) . Proof (existence) The proof for existence actually let us know some ways to generate the polynomial. So we leave it to the next part. Theorem (Polynomial interpolation error) To each \\(x\\) there corresponds a point \\(\\eta_x \\in (a, b)\\) such that: \\[ f(x) - p(x) = \\frac{1}{(n+1)!} f^{(n+1)}(\\eta_x) \\prod^n_{i=0}(x-x_i) \\] Proof \\(x\\) is fixed. Let \\(w(t) = \\prod^n_{i=0} (t-x_i)\\) , \\(\\phi(t) \\equiv f(t) - p(t) + \\lambda w(t)\\) , where \\(\\phi(x) = 0\\) and \\(\\lambda = \\frac{f(x)-p(x)}{w(x)}\\) \\(\\phi \\in C^{n+1}[a, b]\\) , and \\(\\phi\\) vanishes at the \\(n+2\\) points \\(x, x_0, x_1, \\cdots, x_n\\) . By Mean-Value Theorem, \\(\\phi'\\) has at least \\(n+1\\) distinct zeros in \\([a, b]\\) (one zero in \\(\\phi'\\) between each original zero in \\(phi\\) ) Similarly, \\(\\phi''\\) has at least \\(n\\) distinct zeros in (a, b). Eventually, \\(\\phi^{(n+1)}\\) has at least one zero, say \\(\\eta_x\\) in \\((a, b)\\) . Now \\(w^{(n+1)}(t)\\) = \\frac{d {n+1}}{dt }\\prod^n_{i=1}(t-x_i) = (n+1)!$ (polynomial in \\(t\\) ) \\(\\phi^{(n+1)} = f^{(n+1)} - p^{(n+1)} - \\lambda w^{(n+1)} = f^{(n+1)} - (n+1)!\\lambda\\) ( \\(p\\) has degree at most \\(n\\) ) Lastly, \\(0 = \\phi^{(n+1)}(\\eta_x) = f^{(n+1)}(\\eta_x) - (n+1)!\\frac{f(x)-p(x)}{w(x)}\\) Wolfram alpha command interpolate [(5,1),(-7,-23),(-6,-54), (0, -954)]","title":"Polynomial interpolation"},{"location":"math/3601/ch4.html#lagrange-interpolation","text":"\\[ \\begin{align*} p_n(x) = y_0 \\ell_0(x) + y_1 \\ell_1(x) + \\cdots + y_n \\ell_n(x) \\\\ \\ell_i(x) = \\prod^n_{j=0, j \\neq i} \\frac{x-x_j}{x_i-x_j} \\end{align*} \\] where \\(\\ell_i\\) are polynomials that depend on \\(x_0, \\cdots x_n\\) but not on \\(y_0 \\cdots y_n\\) . Note that \\(y_i = p_n(x_i) = y_0 \\ell_0(x_i) + y_1 \\ell_1(x_i) + \\cdots + y_n \\ell_n(x_i)\\) . So \\(\\ell_i(x_j) = 1\\) if \\(i = j\\) and \\(\\ell_i(x_j) = 0\\) if \\(i \\neq j\\) Example \\((1, 1), (2, 8), (3, 27)\\) The interpolating polynomial is: \\(1 \\cdot \\frac{x-2}{1-2} \\cdot \\frac{x-3}{1-3} + 8 \\cdot \\frac{x-1}{2-1} \\cdot \\frac{x-3}{2-3} + 27 \\cdot \\frac{x-1}{3-1} \\cdot \\frac{x-2}{3-2} = 6x^2 - 11x + 6\\) interpolate [(1, 1) (2, 8) (3, 27)] Note \\(\\sum^n_{i=0}\\ell_i(x) = 1\\) Try interpolating \\(P(x) = f(x) = 1\\) While elegant and theoretically important, the Lagrange interpolation formula is not suitable for actual computations, because: the evaluation of the Lagrange interpolation requires more floating point operations than necessary (Re: Horner's method) difficult to obtain derivative the inlcusion of additional data points require the reevalutaion of all Lagrange polynomials Runge's phenomenon : If the points are equally spaced, then there is large oscillation above and below the true function.","title":"Lagrange interpolation"},{"location":"math/3601/ch4.html#newton-interpolation","text":"\\[ \\begin{align*} p_0(x_0) = c_0 = y_0 \\\\ p_k(x) = p_{k-1}(x) + c_k(x-x_0)(x-x_1)\\cdots(x-x_{k-1})\\\\ c_k = \\frac{y_k - p_{k-1}(x)}{(x_k-x_0)(x_k-x_1)\\cdots(x_k-x_{k-1})} \\end{align*} \\] Divided differences notation \\[ \\begin{align*} c_0 = p_n(x_0) = y_0 = [y_0]\\\\ c_1 = \\frac{y_1 - y_0}{x_1 - x_0} = \\frac{[y_1] - [y_0]}{x_1 - x_0} = [y_0, y_1]\\\\ c_k = \\frac{[y_1, \\cdots, y_k] - [y_0, \\cdots, y_{k-1}]}{x_k - x_0} = [y_0, \\cdots, y_k]\\\\ p_n(x) = [y_0] + \\sum^n_{k=1}[y_0, y_1, \\cdots, y_k](x-x_0) \\cdots (x-x_{k-1}) \\end{align*} \\] Example \\((0, 1), (1, 3), (3, 2)\\) \\(k=0\\) \\(k=1\\) \\(k=2\\) \\(x_0 = 0\\) \\([y_0] = 1\\) * \\(x_1 = 1\\) \\([y_1] = 3\\) \\([y_0, y_1] = 2\\) * \\(x_2 = 3\\) \\([y_2] = 2\\) \\([y_1, y_2] = -1/2\\) \\([y_0, y_1, y_2] = -5/6\\) * \\(p_n = [y_0] + [y_0, y_1](x-x_0) + [y_0, y_1, y_2](x-x_0)(x-x_1)\\) \\(p_n = 1 + 2(x-x_0) + -\\frac{5}{6}(x-x_0)(x-x_1) = -\\frac{5}{6}x^2 + \\frac{17}{6}x + 1\\)","title":"Newton interpolation"},{"location":"math/3601/ch4.html#hermite-interpolation","text":"(skipped)","title":"Hermite interpolation"},{"location":"math/3601/ch4.html#spline-interpolation","text":"A spline function consists of polynomial pieces on subintervals joined together with certain continuity conditions. Formally suppose that \\(n+1\\) points \\(t_0, t_1, \\cdots, c_n\\) have been specified and satisfy \\(t_0 < t1 < \\cdots < t_n\\) . These points are called knots . Suppose also that an integer \\(k \\geq 0\\) has been perscribed. A spline function of degree \\(k\\) having knots \\(t_0, \\cdots, t_n\\) is a function \\(S\\) such that: On each interval \\([t_{i-1}, t_i]\\) , \\(S\\) is a polynomial of degree \\(\\leq k\\) \\(S\\) has a continuous \\((k-1)\\) st derivative on \\([t_0, t_n]\\) i.e. Piecewise polynomial of degree at most \\(k\\) having continuous derivatives of all orders up to \\(k-1\\) . Spline of degree 0: Piecewise constant, step funtion (right continuous), intervals do not intersect Spline of degree 1: Piecewise linear, continuous Checking spline: Left hand and right hand limit of value and (deg-1) derivatives are continuous","title":"Spline interpolation"},{"location":"math/3601/ch4.html#cubic-spline","text":"Derivation \\(z_i = S''(t_i)\\) \\(S_i^{''}(x) = A_ix+B_i\\) \\(A_it_i + B_i = z_i, A_it_{i+1} + B_i = z_{i+1}\\) \\(h_i = t_{i+1} - t_i\\) \\(A_i = \\frac{z_{i+1}-z_i}{h_i}, B_i = z_i - \\frac{z_{i+1} - z_i}{h_i} t_i\\) \\(S_i^{''}(x) = A_ix+B_i = \\frac{z_{i+1}}{h_i}(x-t_i) + \\frac{z_i}{h_i}(t_{i+1}-x)\\) , straight line between \\(z_i\\) and \\(z_{i+1}\\) \\(S_i^{'}(x) = -\\frac{1}{2}\\frac{z_i}{h_i}(t_{i+1}-x)^2 + \\frac{1}{2}\\frac{z_{i+1}}{h_i}(x-t_i)^2+C_i\\) (integration) \\(S_i(x) = \\frac{z_i}{6h_i}(t_{i+1}-x)^3 + \\frac{z_{i+1}}{6h_i}(x-t_i)^3 + C_i(x-t_i) + D_i\\) Interpolation conditions \\(S_i(t_i) = y_i\\) and \\(S_i(t_{i+1}) = y_{i+1}\\) , find \\(C_i\\) and \\(D_i\\) : \\(D_i = y_i - \\frac{z_ih_i^2}{6}, C_i = \\frac{y_{i+1}-y_i}{h_i} + \\frac{h_i}{6}(z_i-z_{i+1})\\) Determining \\(z_1, z_2, \\cdots, z_{n-1}\\) : Use continuity conditions for \\(S'\\) at the interior knots \\(t_i\\) : \\(S_{i-1}^{'}(t_i) = S_{i}^{'}(t_i)\\) TODO Theorem (Optimality of Natural Cubic Spline) Let \\(f^{''}\\) be continuous on \\([a, b]\\) and let \\(a = t_0 < t_1 < \\cdots < t_n = b\\) . If \\(S\\) is the natural cubic spline interpolating \\(f\\) at the knots \\(t_i\\) , then \\(\\int^b_a{[S^{''}(x)]^2} dx \\leq \\int^b_a{[f^{''}(x)]^2} dx\\) Proof Let \\(g \\equiv f-S\\) . \\((f^{''})^2 = (S^{''} + g^{''})^2 = (S^{''})^2 + (g^{''})^2 + 2S^{''}g^{''}\\) \\(\\int^a_b{(f^{''})^2}dx = \\int^a_b{(S^{''})^2}dx + \\int^a_b{(g^{''})^2}dx + \\int^a_b{2S^{''}g^{''}}dx\\) Note that \\(\\int^a_b{(g^{''})^2}dx \\geq 0\\) . So to prove the inequality, we can choose to show that \\(\\int^a_b{2S^{''}g^{''}}dx \\geq 0\\) . Idea: Integration by parts \\(\\int^a_b{2S^{''}g^{''}}dx = \\sum^n_{i=1}{\\int^{t_{i}}_{t_{i-1}}{2S^{''}g^{''}}dx} = \\sum^n_{i=1}{S^{''}g^{'}(t_i) - {S^{''}g^{'}(t_{i-1}) - \\int^{t_{i}}_{t_{i-1}}{2S^{'''}g^{'}}dx}\\) TODO","title":"Cubic spline"},{"location":"math/3601/ch4.html#function-approximation","text":"","title":"Function approximation"},{"location":"math/3601/ch4.html#least-squares","text":"\\[ \\begin{align*} X\\beta = Y \\\\ \\hat{\\beta} = (X^TX)^{-1}X^Ty \\end{align*} \\]","title":"Least squares"},{"location":"math/3601/ch4.html#function-approximation-over-interval","text":"Example Let \\(f(x) = e^x\\) , let \\(p(x) = \\alpha_0 + \\alpha_1x\\) . Approximate \\(f(x)\\) over \\([-1, 1]\\) . So choose \\(\\alpha_0, \\alpha_1\\) to minimize \\(g(\\alpha_0, \\alpha_1) = \\int_{-1}^1{[e^x - \\alpha_0 - \\alpha_1]^2 dx}\\) \\(0 = \\frac{\\partial g}{\\partial \\alpha_0} = 2 \\int_{-1}^1{[e^x - \\alpha_0 - \\alpha_1x] (-1)} dx\\) \\(0 = \\frac{\\partial g}{\\partial \\alpha_0} = 2 \\int_{-1}^1{[e^x - \\alpha_0 - \\alpha_1x] (-x)} dx\\) Solving gives \\(2\\alpha_0 = e-e^{-1}\\) , \\(\\frac{2}{3}\\alpha_1 = 2e^{-1}\\) .","title":"Function approximation over interval"},{"location":"math/3601/ch4.html#gram-schmidt-process","text":"Something related to orthogonal functions... E.g. Orthogonal function: Fourier series TODO","title":"Gram-Schmidt process"},{"location":"math/3601/ch5.html","text":"Chapter 5: Numerical Integration Quote The differential calculus is a science ; the integral calculus is an art . \\[ I(f) \\approx Q(f) := \\sum^n_{j=0} A_j f(x_j) \\] Q: How to determine weights \\(A_j\\) ? How to measure the error? Integration by interpolation polynomials \\(p(x) = \\sum^n_{i=0} f(x_i)l_i(x)\\) \\(I(f) \\approx \\sum^n_{i=0}f(x_i) \\int^b_a l_i(x) dx\\) When the \\(x_i\\) 's are equally spaced, then this formula is called the Newton-Cotes formula of order \\(n\\) . Theorem The order of accuracy is \\(n\\) if it is exact for all \\(f \\in \\Pi_n\\) but there will be a nonzero error term for some degree \\((n+1)\\) polynomial. The formula is exact for all \\(f \\in \\Pi_n\\) if and only if it is derived from the integration of the polynomial (with minimal degree) interpolating \\(f(x_0), f(x_1), \\cdots, f(x_n)\\) . \\(A_i = \\int^b_a \\ell_i(x) dx\\) Trapezoidal Rule \\[ I(f) \\approx \\frac{b-a}{2}(f(a) + f(b)) \\] Composite trapezoidal rule: \\[ \\begin{align*} \\frac{h}{2}\\left(f(a)+f(b)+2\\sum^{n-1}_{i=1}f(x_i)\\right)\\\\ x_i = a+ih, h=(b-a)/n \\end{align*} \\] Exact for linear function. Order of accuracy is one. Error analysis of trapezoidal rule If \\(f \\in C^2\\) , then the error of the trapezoidal's rule is \\(-\\frac{1}{12}f''(\\xi)(b-a)^3\\) , where \\(\\xi \\in (a, b)\\) Error analysis of composite trapezoidal rule If \\(f \\in C^2\\) , then the error is \\(-\\frac{h^3}{12}\\sum^{n-1}_{i=0}f''(c_i) = -\\frac{h^2}{12}(b-a)f''(c)\\) , where \\(c_i \\in (x_i, x_{i+1})\\) and \\(c \\in [a, b]\\) . Simpson's rule Quadratic Newton-Cotes formula. \\[ \\int^b_a f(t) dt = \\frac{b-a}{6}(f(a) + 4f((a+b)/2)+f(b)) \\] Composite Simpson's rule: \\[ \\frac{b-a}{3n}(f(x_0) + 4f(x_1) + f(x_2) + f(x_2) + 4f(x_3) + f(x_4) + f(x_4) + \\cdots + 4f(x_{n-1}) + f(x_n)) \\] Error analysis of Simpson's rule if \\(f \\in C^4\\) , then the error of the Simpson's rule is \\(-\\frac{1}{90}f^{(4)}(\\xi)h^5\\) , where \\(h=(b-a)/2\\) and \\(\\xi \\in (a, b)\\) Exact for cubic function (some magic here??). Order of accuracy is three. The order of accuracy is not continuous! Re: Even nodal point, Odd nodal point Linearity of integration and interpolation For Simpson's rule, we can verify the cubic exact by putting \\(f(x) = x^3\\) . By the linearity of integration and the interpolation , the formula is exact for \\(p \\in \\Pi^3\\) . It is not exact when \\(f(x) = x^4\\) Method of undetermined coefficients Solve ( \\(x_j, a, b, n\\) given) \\[ \\int^b_a f(x) dx = \\sum^n_{j=0} A_j f(x_j) \\] Take \\(f(x) = 1, f(x) = x, f(x) = x^2, \\cdots, x_n\\) . Then solve the system of linear equations. It is equivalent to taking a basis in the linear space of functions, so the functions can be \"a linear combination of nonlinear functions\" (e.g. \\(f(x) = ae^x + b \\cos(\\pi x/2)\\) ) Gaussian Quadrature Motivation: Choose both the weights \\(A_i\\) and nodes \\(x_i\\) freely, then we can achieve exact for \\(f \\in \\Pi_{2n+1}\\) Find the q-orthogonal polynomial and the roots Solve the weigths Example Gaussian quadrature Gram-Schmidt Process Proof Definition: w-orthogonal","title":"Chapter 5: Numerical Integration"},{"location":"math/3601/ch5.html#chapter-5-numerical-integration","text":"Quote The differential calculus is a science ; the integral calculus is an art . \\[ I(f) \\approx Q(f) := \\sum^n_{j=0} A_j f(x_j) \\] Q: How to determine weights \\(A_j\\) ? How to measure the error?","title":"Chapter 5: Numerical Integration"},{"location":"math/3601/ch5.html#integration-by-interpolation-polynomials","text":"\\(p(x) = \\sum^n_{i=0} f(x_i)l_i(x)\\) \\(I(f) \\approx \\sum^n_{i=0}f(x_i) \\int^b_a l_i(x) dx\\) When the \\(x_i\\) 's are equally spaced, then this formula is called the Newton-Cotes formula of order \\(n\\) . Theorem The order of accuracy is \\(n\\) if it is exact for all \\(f \\in \\Pi_n\\) but there will be a nonzero error term for some degree \\((n+1)\\) polynomial. The formula is exact for all \\(f \\in \\Pi_n\\) if and only if it is derived from the integration of the polynomial (with minimal degree) interpolating \\(f(x_0), f(x_1), \\cdots, f(x_n)\\) . \\(A_i = \\int^b_a \\ell_i(x) dx\\)","title":"Integration by interpolation polynomials"},{"location":"math/3601/ch5.html#trapezoidal-rule","text":"\\[ I(f) \\approx \\frac{b-a}{2}(f(a) + f(b)) \\] Composite trapezoidal rule: \\[ \\begin{align*} \\frac{h}{2}\\left(f(a)+f(b)+2\\sum^{n-1}_{i=1}f(x_i)\\right)\\\\ x_i = a+ih, h=(b-a)/n \\end{align*} \\] Exact for linear function. Order of accuracy is one. Error analysis of trapezoidal rule If \\(f \\in C^2\\) , then the error of the trapezoidal's rule is \\(-\\frac{1}{12}f''(\\xi)(b-a)^3\\) , where \\(\\xi \\in (a, b)\\) Error analysis of composite trapezoidal rule If \\(f \\in C^2\\) , then the error is \\(-\\frac{h^3}{12}\\sum^{n-1}_{i=0}f''(c_i) = -\\frac{h^2}{12}(b-a)f''(c)\\) , where \\(c_i \\in (x_i, x_{i+1})\\) and \\(c \\in [a, b]\\) .","title":"Trapezoidal Rule"},{"location":"math/3601/ch5.html#simpsons-rule","text":"Quadratic Newton-Cotes formula. \\[ \\int^b_a f(t) dt = \\frac{b-a}{6}(f(a) + 4f((a+b)/2)+f(b)) \\] Composite Simpson's rule: \\[ \\frac{b-a}{3n}(f(x_0) + 4f(x_1) + f(x_2) + f(x_2) + 4f(x_3) + f(x_4) + f(x_4) + \\cdots + 4f(x_{n-1}) + f(x_n)) \\] Error analysis of Simpson's rule if \\(f \\in C^4\\) , then the error of the Simpson's rule is \\(-\\frac{1}{90}f^{(4)}(\\xi)h^5\\) , where \\(h=(b-a)/2\\) and \\(\\xi \\in (a, b)\\) Exact for cubic function (some magic here??). Order of accuracy is three. The order of accuracy is not continuous! Re: Even nodal point, Odd nodal point Linearity of integration and interpolation For Simpson's rule, we can verify the cubic exact by putting \\(f(x) = x^3\\) . By the linearity of integration and the interpolation , the formula is exact for \\(p \\in \\Pi^3\\) . It is not exact when \\(f(x) = x^4\\)","title":"Simpson's rule"},{"location":"math/3601/ch5.html#method-of-undetermined-coefficients","text":"Solve ( \\(x_j, a, b, n\\) given) \\[ \\int^b_a f(x) dx = \\sum^n_{j=0} A_j f(x_j) \\] Take \\(f(x) = 1, f(x) = x, f(x) = x^2, \\cdots, x_n\\) . Then solve the system of linear equations. It is equivalent to taking a basis in the linear space of functions, so the functions can be \"a linear combination of nonlinear functions\" (e.g. \\(f(x) = ae^x + b \\cos(\\pi x/2)\\) )","title":"Method of undetermined coefficients"},{"location":"math/3601/ch5.html#gaussian-quadrature","text":"Motivation: Choose both the weights \\(A_i\\) and nodes \\(x_i\\) freely, then we can achieve exact for \\(f \\in \\Pi_{2n+1}\\) Find the q-orthogonal polynomial and the roots Solve the weigths Example Gaussian quadrature Gram-Schmidt Process Proof Definition: w-orthogonal","title":"Gaussian Quadrature"},{"location":"math/3601/ch6.html","text":"Chapter 6: Numerical Differentiation and ODE Numerical Differentiation Differentiate interpolating polynomial Interpolate the function then differentiate the function Error analysis \\[ \\begin{align*} f'(x_i) - p'(x_i) = \\frac{1}{(n+1)!}f^{(n+1)}(\\xi_x)\\prod^n_{j=0, j \\neq i}(x_i-x_j)\\\\ \\xi \\in (a, b) \\end{align*} \\] \\[ \\begin{align*} f'(x) - p'(x) = \\frac{1}{n!}f^{(n+1)}(\\eta)\\prod^{n-1}_{i=0}(x-\\xi_i)\\\\ \\xi_i \\in (x_i, x_{i+1}), \\eta \\text{ in interval containing } \\{ \\xi_0, \\cdots, \\xi_{n-1}, x \\} \\end{align*} \\] Raw image ptsd Finite difference schemes \\(\\boxed{f'(x) \\approx \\frac{f(x+h)-f(x)}{h}}\\) is order 1 accuracy, because \\(f'(x) = \\frac{f(x+h)-f(x)}{h} - \\frac{h}{2}f''(\\xi)\\) \\(\\frac{h}{2}f''(\\xi)\\) can give us a bound on the error. Subtractive cancellation If we keep on decreasing \\(h\\) , we will not neccessarily get better accuracy due to subtractive cancellation . The number of significant digits drops as difference of very close numbers are being calculated. Roundoff error prevents us from getting better approximations by decreasing \\(h\\) . \\(\\boxed{f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}}\\) is order 2 accuracy Proof Richardson Extrapolation Let \\(\\varphi(h) = \\frac{1}{2h}(f(x+h)-f(x-h))\\) Richardson Extrapolation 4 th order approximation: \\(\\boxed{\\frac{4}{3}\\varphi(h/2)-\\frac{1}{3}\\varphi(h)}\\) Proof IVP (Initial Value Problem) Given that \\(\\boxed{\\varphi'(t) = x' = f(t, x), x(t_0) = x_0}\\) , obtain a solution \\(\\boxed{x = \\varphi(t)}\\) By numerical solutions, we mean a sequence of approximated values of \\(x\\) at a sequence of values of \\(t\\) . Usually, denote \\(h = t_i - t_{i-1}\\) Euler's method \\[ \\varphi(t) \\approx x_t = x_0 + f(t_0, x_0)(t - t_0) \\] Example Global truncation error is \\(O(h)\\) . Because the local truncation error in each step is \\(\\frac{1}{2}f''(\\xi_i)h^2\\) , and we take \\((t_i-t_0)/h\\) steps. Can achieve higher order, but it's complicated to derive formula for higher derivatives. Taylor Series method Expand the taylor series of \\(\\varphi(t = t_0 + h)\\) , then express the derivatives in terms of lower derivatives. Example Runge-Kutta method (2 nd order) \\[ \\begin{align*} K_1 &= f(t_i, x_i)\\\\ \\bar{x}_{i+1} &= x_i + hf(t_i, x_i)\\\\ K_2 &= f(t_i + h, x_i + hK_1)\\\\ x_{i+1} &= x_i + h(K_1/2 + K_2/2) \\end{align*} \\] Example Derivation (trapezoidal rule) Local truncation error: \\(O(h^3)\\) Proof In general, can choose \\[ \\begin{align*} K_1 &= f(t_i, x_i)\\\\ \\bar{x}_{i+1} &= x_i + bhf(t_i, x_i)\\\\ K_2 &= f(t_i + ah, x_i + bhK_1)\\\\ x_{i+1} &= x_i + h(c_1K_1 + c_2K_2)\\\\ c_1 + c_2 = 1, & ac_2 = 1/2 = bc_2 \\end{align*} \\] Derivation (Taylor series) BVP (Boundary Value Problem) Given that \\(\\boxed{\\varphi''(t) = x'' = f(t, x, x'), x(0) = \\alpha, x_{n+1} = \\beta}\\) , obtain a solution \\(\\boxed{x = \\varphi(t)}\\) Approximate \\(\\boxed{f'(x) \\approx \\frac{1}{2h} (f(x+h)-f(x-h))}\\) \\(\\boxed{f''(x) \\approx \\frac{1}{h^2} (f(x+h)-2f(x)+f(x-h))}\\) , order 2 Discretized BVP Linear case: \\(\\boxed{x'' = f(t, x, x') = u(t) + v(t)x + w(t)x'}\\) Discretized BVP (Linear case Coefficient matrix is a tridiagonal matrix. It is strictly diagonally dominant (proof omitted) so inverse exists. It can be solved in \\(O(n)\\) . Convergence analysis Basically, both Euler method and BVP converge when \\(h \\rightarrow 0\\) and \\(n \\rightarrow 0\\) . But we don't have to prove it. Convergene of Euler method Convergene of finite difference BVP","title":"Chapter 6: Numerical Differentiation and ODE"},{"location":"math/3601/ch6.html#chapter-6-numerical-differentiation-and-ode","text":"","title":"Chapter 6: Numerical Differentiation and ODE"},{"location":"math/3601/ch6.html#numerical-differentiation","text":"","title":"Numerical Differentiation"},{"location":"math/3601/ch6.html#differentiate-interpolating-polynomial","text":"Interpolate the function then differentiate the function Error analysis \\[ \\begin{align*} f'(x_i) - p'(x_i) = \\frac{1}{(n+1)!}f^{(n+1)}(\\xi_x)\\prod^n_{j=0, j \\neq i}(x_i-x_j)\\\\ \\xi \\in (a, b) \\end{align*} \\] \\[ \\begin{align*} f'(x) - p'(x) = \\frac{1}{n!}f^{(n+1)}(\\eta)\\prod^{n-1}_{i=0}(x-\\xi_i)\\\\ \\xi_i \\in (x_i, x_{i+1}), \\eta \\text{ in interval containing } \\{ \\xi_0, \\cdots, \\xi_{n-1}, x \\} \\end{align*} \\] Raw image ptsd","title":"Differentiate interpolating polynomial"},{"location":"math/3601/ch6.html#finite-difference-schemes","text":"\\(\\boxed{f'(x) \\approx \\frac{f(x+h)-f(x)}{h}}\\) is order 1 accuracy, because \\(f'(x) = \\frac{f(x+h)-f(x)}{h} - \\frac{h}{2}f''(\\xi)\\) \\(\\frac{h}{2}f''(\\xi)\\) can give us a bound on the error. Subtractive cancellation If we keep on decreasing \\(h\\) , we will not neccessarily get better accuracy due to subtractive cancellation . The number of significant digits drops as difference of very close numbers are being calculated. Roundoff error prevents us from getting better approximations by decreasing \\(h\\) . \\(\\boxed{f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h}}\\) is order 2 accuracy Proof","title":"Finite difference schemes"},{"location":"math/3601/ch6.html#richardson-extrapolation","text":"Let \\(\\varphi(h) = \\frac{1}{2h}(f(x+h)-f(x-h))\\) Richardson Extrapolation 4 th order approximation: \\(\\boxed{\\frac{4}{3}\\varphi(h/2)-\\frac{1}{3}\\varphi(h)}\\) Proof","title":"Richardson Extrapolation"},{"location":"math/3601/ch6.html#ivp-initial-value-problem","text":"Given that \\(\\boxed{\\varphi'(t) = x' = f(t, x), x(t_0) = x_0}\\) , obtain a solution \\(\\boxed{x = \\varphi(t)}\\) By numerical solutions, we mean a sequence of approximated values of \\(x\\) at a sequence of values of \\(t\\) . Usually, denote \\(h = t_i - t_{i-1}\\)","title":"IVP (Initial Value Problem)"},{"location":"math/3601/ch6.html#eulers-method","text":"\\[ \\varphi(t) \\approx x_t = x_0 + f(t_0, x_0)(t - t_0) \\] Example Global truncation error is \\(O(h)\\) . Because the local truncation error in each step is \\(\\frac{1}{2}f''(\\xi_i)h^2\\) , and we take \\((t_i-t_0)/h\\) steps. Can achieve higher order, but it's complicated to derive formula for higher derivatives.","title":"Euler's method"},{"location":"math/3601/ch6.html#taylor-series-method","text":"Expand the taylor series of \\(\\varphi(t = t_0 + h)\\) , then express the derivatives in terms of lower derivatives. Example","title":"Taylor Series method"},{"location":"math/3601/ch6.html#runge-kutta-method-2nd-order","text":"\\[ \\begin{align*} K_1 &= f(t_i, x_i)\\\\ \\bar{x}_{i+1} &= x_i + hf(t_i, x_i)\\\\ K_2 &= f(t_i + h, x_i + hK_1)\\\\ x_{i+1} &= x_i + h(K_1/2 + K_2/2) \\end{align*} \\] Example Derivation (trapezoidal rule) Local truncation error: \\(O(h^3)\\) Proof In general, can choose \\[ \\begin{align*} K_1 &= f(t_i, x_i)\\\\ \\bar{x}_{i+1} &= x_i + bhf(t_i, x_i)\\\\ K_2 &= f(t_i + ah, x_i + bhK_1)\\\\ x_{i+1} &= x_i + h(c_1K_1 + c_2K_2)\\\\ c_1 + c_2 = 1, & ac_2 = 1/2 = bc_2 \\end{align*} \\] Derivation (Taylor series)","title":"Runge-Kutta method (2nd order)"},{"location":"math/3601/ch6.html#bvp-boundary-value-problem","text":"Given that \\(\\boxed{\\varphi''(t) = x'' = f(t, x, x'), x(0) = \\alpha, x_{n+1} = \\beta}\\) , obtain a solution \\(\\boxed{x = \\varphi(t)}\\) Approximate \\(\\boxed{f'(x) \\approx \\frac{1}{2h} (f(x+h)-f(x-h))}\\) \\(\\boxed{f''(x) \\approx \\frac{1}{h^2} (f(x+h)-2f(x)+f(x-h))}\\) , order 2 Discretized BVP Linear case: \\(\\boxed{x'' = f(t, x, x') = u(t) + v(t)x + w(t)x'}\\) Discretized BVP (Linear case Coefficient matrix is a tridiagonal matrix. It is strictly diagonally dominant (proof omitted) so inverse exists. It can be solved in \\(O(n)\\) .","title":"BVP (Boundary Value Problem)"},{"location":"math/3601/ch6.html#convergence-analysis","text":"Basically, both Euler method and BVP converge when \\(h \\rightarrow 0\\) and \\(n \\rightarrow 0\\) . But we don't have to prove it. Convergene of Euler method Convergene of finite difference BVP","title":"Convergence analysis"},{"location":"stat/3602/ch1.html","text":"Chapter 1: Decision Problem: Frequentist Approach General setup Parameter space: \\(\\Theta\\) The true parameter is some unknown \\(\\theta\\in\\Theta\\) Sample space \\(S\\) Collection of all the possible realizations \\(x\\) of a random vector \\(X\\) Statistical model \\(f(\\cdot | \\theta)\\) \"True\" probability function of the random vector \\(X\\) Action space \\(A\\) Loss function \\(L(\\theta, a)\\) Frequentist approach Decision rule: \\(d(x): S \\to A\\) Action \\(A\\) to be taken when \\(X\\) is observed Risk function: \\(R(\\theta, d) = \\mathbb{E}_\\theta[L(\\theta, d(X))] = \\int_S{L(\\theta, d(x)) f(x|\\theta) dx}\\) Characterize the performance of rule \\(d\\) under each possible \\(\\theta\\in\\Theta\\) Goal: Choose one rule \\(d(\\cdot)\\) (by their risk functions) that is the \"best\" (in some sense) Example: Quality control From a batch of 10 batteries, draw 1 at random and test it. Either it is defective or OK. This is the only observation upon which our decision about the whole batch has to rely. Sample space: \\(S = {0, 1}\\) Parameter space: \\(\\theta\\in\\Theta = {0, 1, \\cdots, 10}\\) Action space: \\(a_0\\) sell all, \\(a_1\\) scrap all Loss function: \\(L(\\theta, a_0) = 30\\theta - 150\\) , \\(L(\\theta, a_1) = 10\\) Only 4 possible decision rules: \\(d_{00}, d_{01}, d_{10}, d_{11}\\) Can plot the risk function for each \\(d\\) . x axis is \\(\\theta\\) , y axis is the risk. \\(R(\\theta, d) = \\Pr_\\theta(X = 0 | \\theta)L(\\theta, d_1(0)) + \\Pr_\\theta(X = 1 | \\theta)L(\\theta, d_1(1))\\) Criteria for selecting decision rules Admissible Admissible: Not being strictly domintated by another rule A rule \\(d\\) strictly dominates another rule \\(d^*\\) if \\(R(\\theta, d) \\leq R(\\theta, d^*)\\) for all \\(\\theta\\in\\Theta\\) , and \\(R(\\theta', d) < R(\\theta', d^*)\\) for some \\(\\theta'\\in\\Theta\\) . A rather weak (but sensible) condition on choice of decision rule... An inadmissible rule is obviously stupid! Admissibilty provides the first criterion for discarding obviously stupid rules. Minimax Minimaxity: Have the smallest \"worst-case\" risk among all the \"worst-case\" risks of all rules Formally, rule \\(d\\) is minimax if \\(\\sup{R(\\theta, d'): \\theta\\in\\Theta} \\geq \\sup{R(\\theta, d): \\theta\\in\\Theta}\\) ...a conservative rule? Unbiased Unbiased: Incurs the smallest expected loss if the loss is measured w.r.t to the true \\(\\theta\\) generating \\(X\\) . That is, the loss may be measured accoding to another \\(\\theta'\\) . However, the probabilty distribution of the loss (that governs the observation \\(X\\) ) remains the original. Unbiased rule: The loss is the smallest when the two \\(\\theta\\) are the same, opposed to when the loss \\(\\theta\\) and probabilty \\(\\theta\\) are different. Formally, a rule \\(d\\) is unbiased if \\(\\mathbb{E}_\\theta[L(\\theta', d(X))] \\geq \\mathbb{E}_\\theta[L(\\theta, d(X))] = R(\\theta, d)\\) for all \\(\\theta,\\theta'\\in\\Theta\\) Warning When determining whether a rule is unbiased or not, remember to start from the definition! Example TODO Application: It may be intractable to find the best rule in the class of all rules, but we may be able to find the best rule in the subclass of all unbiased rules. Bayes If \\(R(\\theta, d_1) > R(\\theta, d_2)\\) for some \\(\\theta\\) , but \\(R(\\theta, d_1) < R(\\theta, d_2)\\) for other \\(\\theta\\) , which is better? Sometimes, although we do not know the true \\(\\theta\\) , somehow we may have a subjective opinion on how the true \\(\\theta\\) is like (e.g. based on past experience). We call this the prior knowledge of unknown \\(\\theta\\) . With the prior weight function \\(\\pi(\\theta)\\) , we can reduce the risk function to a scalar. The weight function indicates how much \"belief\" we have in \\(\\theta\\) being the true parameter. Bayes risk: \\(r(\\pi, d) = \\int_\\Theta{R(\\theta, d)\\pi(\\theta)d\\theta}\\) Bayes rule: Rule that has the smallest Bayes risk. Goal: Given \\(\\pi(\\cdot)\\) , seek the Bayes rule (w.r.t. prior \\(\\pi\\) ). To find Bayes rules in practice, we can find \\(d\\) that minimizes \\(\\int_S\\int_\\theta L(\\theta, d(x)) \\pi(\\theta) f(x|\\theta) d\\theta dx\\) instead. (seems that you can just minimize the inner integral?) Derivation TODO Randomized decision rule A random mixture of rules - Choose a decision rule by random! Mainly for analytical purposes... seldom put into practice Risk function: \\(R(\\theta, d^*) = \\sum^s_{i=1} p_i R(\\theta, d_i)\\) Convex combination of the risk functions of the individual rules \\(d_i\\) 's. So it won't exceed the original deterministic curves Bayes risk: \\(r(\\pi, d^*) = \\int R(\\theta, d^*) \\pi(\\theta) d\\theta = \\sum^s_{i=1}p_i\\int R(\\theta, d_i) \\pi(\\theta) d\\theta = \\sum^s_{i=1}p_i r(\\pi, d_i) \\geq \\min (r(\\pi, d_1), \\cdots, r(\\pi, d_s))\\) Thus, the risk of a Bayes rule chosen among \\((d_1, \\cdots d_s)\\) cannot be reduced further by making them into a randomized rule. Minimax: Randomized rule can be more minimax than deterministic minimax rules! Risk set (restrict to special setting: \\(\\Theta = {\\theta_1, \\theta_2}\\) ) 2D plot: x axis \\(R(\\theta_1, d)\\) , y axis \\(R(\\theta_2, d)\\) . Then each decision rule is a point in the plot. Can find the good rules easily! All points in the convex hull -> includes both deterministic and randomized decision rules Risk set must be convex Admissible rules: lower-left boundary of risk set Minimax: Moving the \"right angle\" \\(\\max(x, y) = c\\) until it touches the risk set. In some (not always work) cases, draw a line \\(R(\\theta_1, d) = R(\\theta_2, d)\\) and find the intersection with the lower boundary. Unbiased rule: Nil (not using the diagram alone ). Because the risk set only tells us the risk function, but the definition of unbiased rules requires formulae outside of the risk function. Bayes rule (w.r.t prior \\(\\pi(\\theta_1) = \\pi_1, \\pi(\\theta_2) = \\pi_2\\) ): Push a line \\(\\pi_1 R(\\theta_1, d) + \\pi_2 R(\\theta_2, d) = c\\) . Minimize \\(c\\) such that it is still touching the risk set. Example: Hypothesis testing TODO Admissible rule: Lower boundary. Refer to Neyman-Pearson lemma. Minimax rule: \\(R(0, d_k) = R(1, d_k) \\implies 1-\\Phi(k) = \\Phi(k-1) \\implies \\Phi(-k) = \\Phi(k-1) \\implies k = 0.5\\) Unbiased rule: \\(R(0, d_k) \\leq \\frac{1}{2}, R(1, d_k) \\leq \\frac{1}{2}\\) Randomized rule: Find the point via section formula Bayes rule: Minimize \\(\\psi (1-\\Phi(k)) + (1-\\psi) \\Phi(k-1)\\) . Differentiating w.r.t. \\(k\\) gives \\(-\\psi \\phi(k) + (1-\\psi) \\psi(k-1) = 0\\) , then solve for \\(k\\) . We can find the Bayes prior for a given rule. In order words, what prior needs to be exist for this rule to be a Bayes rule? E.g. \\(k = \\Phi^{-1}(0.95)\\) , solve for \\(\\psi\\) . Then \\(\\psi = 0.75857\\) Size 5% test -> 75% belief on the null hypothesis. We are not making a neutral decision! Equivalent to finding slope \\(= -\\frac{\\psi}{1-\\psi}\\) of risk set at \\(R(0, d) = 0.05\\)","title":"Chapter 1: Decision Problem: Frequentist Approach"},{"location":"stat/3602/ch1.html#chapter-1-decision-problem-frequentist-approach","text":"","title":"Chapter 1: Decision Problem: Frequentist Approach"},{"location":"stat/3602/ch1.html#general-setup","text":"Parameter space: \\(\\Theta\\) The true parameter is some unknown \\(\\theta\\in\\Theta\\) Sample space \\(S\\) Collection of all the possible realizations \\(x\\) of a random vector \\(X\\) Statistical model \\(f(\\cdot | \\theta)\\) \"True\" probability function of the random vector \\(X\\) Action space \\(A\\) Loss function \\(L(\\theta, a)\\)","title":"General setup"},{"location":"stat/3602/ch1.html#frequentist-approach","text":"Decision rule: \\(d(x): S \\to A\\) Action \\(A\\) to be taken when \\(X\\) is observed Risk function: \\(R(\\theta, d) = \\mathbb{E}_\\theta[L(\\theta, d(X))] = \\int_S{L(\\theta, d(x)) f(x|\\theta) dx}\\) Characterize the performance of rule \\(d\\) under each possible \\(\\theta\\in\\Theta\\) Goal: Choose one rule \\(d(\\cdot)\\) (by their risk functions) that is the \"best\" (in some sense) Example: Quality control From a batch of 10 batteries, draw 1 at random and test it. Either it is defective or OK. This is the only observation upon which our decision about the whole batch has to rely. Sample space: \\(S = {0, 1}\\) Parameter space: \\(\\theta\\in\\Theta = {0, 1, \\cdots, 10}\\) Action space: \\(a_0\\) sell all, \\(a_1\\) scrap all Loss function: \\(L(\\theta, a_0) = 30\\theta - 150\\) , \\(L(\\theta, a_1) = 10\\) Only 4 possible decision rules: \\(d_{00}, d_{01}, d_{10}, d_{11}\\) Can plot the risk function for each \\(d\\) . x axis is \\(\\theta\\) , y axis is the risk. \\(R(\\theta, d) = \\Pr_\\theta(X = 0 | \\theta)L(\\theta, d_1(0)) + \\Pr_\\theta(X = 1 | \\theta)L(\\theta, d_1(1))\\)","title":"Frequentist approach"},{"location":"stat/3602/ch1.html#criteria-for-selecting-decision-rules","text":"","title":"Criteria for selecting decision rules"},{"location":"stat/3602/ch1.html#admissible","text":"Admissible: Not being strictly domintated by another rule A rule \\(d\\) strictly dominates another rule \\(d^*\\) if \\(R(\\theta, d) \\leq R(\\theta, d^*)\\) for all \\(\\theta\\in\\Theta\\) , and \\(R(\\theta', d) < R(\\theta', d^*)\\) for some \\(\\theta'\\in\\Theta\\) . A rather weak (but sensible) condition on choice of decision rule... An inadmissible rule is obviously stupid! Admissibilty provides the first criterion for discarding obviously stupid rules.","title":"Admissible"},{"location":"stat/3602/ch1.html#minimax","text":"Minimaxity: Have the smallest \"worst-case\" risk among all the \"worst-case\" risks of all rules Formally, rule \\(d\\) is minimax if \\(\\sup{R(\\theta, d'): \\theta\\in\\Theta} \\geq \\sup{R(\\theta, d): \\theta\\in\\Theta}\\) ...a conservative rule?","title":"Minimax"},{"location":"stat/3602/ch1.html#unbiased","text":"Unbiased: Incurs the smallest expected loss if the loss is measured w.r.t to the true \\(\\theta\\) generating \\(X\\) . That is, the loss may be measured accoding to another \\(\\theta'\\) . However, the probabilty distribution of the loss (that governs the observation \\(X\\) ) remains the original. Unbiased rule: The loss is the smallest when the two \\(\\theta\\) are the same, opposed to when the loss \\(\\theta\\) and probabilty \\(\\theta\\) are different. Formally, a rule \\(d\\) is unbiased if \\(\\mathbb{E}_\\theta[L(\\theta', d(X))] \\geq \\mathbb{E}_\\theta[L(\\theta, d(X))] = R(\\theta, d)\\) for all \\(\\theta,\\theta'\\in\\Theta\\) Warning When determining whether a rule is unbiased or not, remember to start from the definition! Example TODO Application: It may be intractable to find the best rule in the class of all rules, but we may be able to find the best rule in the subclass of all unbiased rules.","title":"Unbiased"},{"location":"stat/3602/ch1.html#bayes","text":"If \\(R(\\theta, d_1) > R(\\theta, d_2)\\) for some \\(\\theta\\) , but \\(R(\\theta, d_1) < R(\\theta, d_2)\\) for other \\(\\theta\\) , which is better? Sometimes, although we do not know the true \\(\\theta\\) , somehow we may have a subjective opinion on how the true \\(\\theta\\) is like (e.g. based on past experience). We call this the prior knowledge of unknown \\(\\theta\\) . With the prior weight function \\(\\pi(\\theta)\\) , we can reduce the risk function to a scalar. The weight function indicates how much \"belief\" we have in \\(\\theta\\) being the true parameter. Bayes risk: \\(r(\\pi, d) = \\int_\\Theta{R(\\theta, d)\\pi(\\theta)d\\theta}\\) Bayes rule: Rule that has the smallest Bayes risk. Goal: Given \\(\\pi(\\cdot)\\) , seek the Bayes rule (w.r.t. prior \\(\\pi\\) ). To find Bayes rules in practice, we can find \\(d\\) that minimizes \\(\\int_S\\int_\\theta L(\\theta, d(x)) \\pi(\\theta) f(x|\\theta) d\\theta dx\\) instead. (seems that you can just minimize the inner integral?) Derivation TODO","title":"Bayes"},{"location":"stat/3602/ch1.html#randomized-decision-rule","text":"A random mixture of rules - Choose a decision rule by random! Mainly for analytical purposes... seldom put into practice Risk function: \\(R(\\theta, d^*) = \\sum^s_{i=1} p_i R(\\theta, d_i)\\) Convex combination of the risk functions of the individual rules \\(d_i\\) 's. So it won't exceed the original deterministic curves Bayes risk: \\(r(\\pi, d^*) = \\int R(\\theta, d^*) \\pi(\\theta) d\\theta = \\sum^s_{i=1}p_i\\int R(\\theta, d_i) \\pi(\\theta) d\\theta = \\sum^s_{i=1}p_i r(\\pi, d_i) \\geq \\min (r(\\pi, d_1), \\cdots, r(\\pi, d_s))\\) Thus, the risk of a Bayes rule chosen among \\((d_1, \\cdots d_s)\\) cannot be reduced further by making them into a randomized rule. Minimax: Randomized rule can be more minimax than deterministic minimax rules!","title":"Randomized decision rule"},{"location":"stat/3602/ch1.html#risk-set","text":"(restrict to special setting: \\(\\Theta = {\\theta_1, \\theta_2}\\) ) 2D plot: x axis \\(R(\\theta_1, d)\\) , y axis \\(R(\\theta_2, d)\\) . Then each decision rule is a point in the plot. Can find the good rules easily! All points in the convex hull -> includes both deterministic and randomized decision rules Risk set must be convex Admissible rules: lower-left boundary of risk set Minimax: Moving the \"right angle\" \\(\\max(x, y) = c\\) until it touches the risk set. In some (not always work) cases, draw a line \\(R(\\theta_1, d) = R(\\theta_2, d)\\) and find the intersection with the lower boundary. Unbiased rule: Nil (not using the diagram alone ). Because the risk set only tells us the risk function, but the definition of unbiased rules requires formulae outside of the risk function. Bayes rule (w.r.t prior \\(\\pi(\\theta_1) = \\pi_1, \\pi(\\theta_2) = \\pi_2\\) ): Push a line \\(\\pi_1 R(\\theta_1, d) + \\pi_2 R(\\theta_2, d) = c\\) . Minimize \\(c\\) such that it is still touching the risk set. Example: Hypothesis testing TODO Admissible rule: Lower boundary. Refer to Neyman-Pearson lemma. Minimax rule: \\(R(0, d_k) = R(1, d_k) \\implies 1-\\Phi(k) = \\Phi(k-1) \\implies \\Phi(-k) = \\Phi(k-1) \\implies k = 0.5\\) Unbiased rule: \\(R(0, d_k) \\leq \\frac{1}{2}, R(1, d_k) \\leq \\frac{1}{2}\\) Randomized rule: Find the point via section formula Bayes rule: Minimize \\(\\psi (1-\\Phi(k)) + (1-\\psi) \\Phi(k-1)\\) . Differentiating w.r.t. \\(k\\) gives \\(-\\psi \\phi(k) + (1-\\psi) \\psi(k-1) = 0\\) , then solve for \\(k\\) . We can find the Bayes prior for a given rule. In order words, what prior needs to be exist for this rule to be a Bayes rule? E.g. \\(k = \\Phi^{-1}(0.95)\\) , solve for \\(\\psi\\) . Then \\(\\psi = 0.75857\\) Size 5% test -> 75% belief on the null hypothesis. We are not making a neutral decision! Equivalent to finding slope \\(= -\\frac{\\psi}{1-\\psi}\\) of risk set at \\(R(0, d) = 0.05\\)","title":"Risk set"},{"location":"stat/3602/ch2.html","text":"Chapter 2: Decision Problem: Bayesian Approach Posterior distribution Posterior distribution Example Bayesian decision Choose \\(a_0\\) if \\(\\frac{\\mathbb{E}[L(\\theta, a_0) | x]}{\\mathbb{E}[L(\\theta, a_1) | x]} < 1\\) Interval estimation Fixed length Fixed coverage probability, minimize length Fixed coverage probability, equal tailed Predictive distribution","title":"Chapter 2: Decision Problem: Bayesian Approach"},{"location":"stat/3602/ch2.html#chapter-2-decision-problem-bayesian-approach","text":"","title":"Chapter 2: Decision Problem: Bayesian Approach"},{"location":"stat/3602/ch2.html#posterior-distribution","text":"Posterior distribution Example","title":"Posterior distribution"},{"location":"stat/3602/ch2.html#bayesian-decision","text":"Choose \\(a_0\\) if \\(\\frac{\\mathbb{E}[L(\\theta, a_0) | x]}{\\mathbb{E}[L(\\theta, a_1) | x]} < 1\\)","title":"Bayesian decision"},{"location":"stat/3602/ch2.html#interval-estimation","text":"Fixed length Fixed coverage probability, minimize length Fixed coverage probability, equal tailed","title":"Interval estimation"},{"location":"stat/3602/ch2.html#predictive-distribution","text":"","title":"Predictive distribution"},{"location":"stat/3602/ch3.html","text":"Chapter 3: Exponential Families Exponential family Theorem: Exponential family \\[ f(x|\\theta) \\propto h(x) \\exp(\\sum^k_{j=1} \\pi_j(\\theta) t_j(x)), x \\in S \\text{ (free of } \\theta \\text{)} \\] \\[ f(x|\\pi) \\propto h(x) \\exp(\\sum^k_{j=1} \\pi_j t_j(x)), x \\in S \\text{ (free of } \\theta \\text{)} \\] \\[ f(x|\\pi) = C(\\pi) h(x) \\exp(\\sum^k_{j=1} \\pi_j t_j(x)) \\text{ (normalizing constant)} \\] Note: Uniform distribution is not exponential family form, because \\(x \\in [0, \\theta]\\) , the sample space depends on \\(\\theta\\) . Example Natural parameter space Natural parameter space: \\(\\Pi = \\{ \\pi = (\\pi_1(\\theta), \\pi_2(\\theta), \\cdots, \\pi_k(\\theta)) : \\theta \\in \\Theta \\} \\subset \\mathbb{R}^k\\) Natural parameter \\(\\pi = (\\pi_1, \\cdots, \\pi_k) \\in \\Pi\\) Natural statistic \\((t_1(X), \\cdots, t_2(X))\\) for \\(X \\in S\\) drawn from \\(f(\\cdot | \\theta)\\) e.g. \\(T = \\sum^n_{i=1}X_i\\) , \\(T = (\\sum^n_{i=1}X_i^2, \\sum^n_{i=1}X_i)\\) Natural parameter space \\(\\Pi\\) can be different from parameter space \\(\\Theta\\) Example ( \\(N(|\\theta|, |\\theta| + \\theta^2)\\) \\(\\text{Poisson}(\\lambda)\\) : \\(\\theta = \\lambda \\in \\Omega = (0, \\infty), \\pi_1(\\lambda) = ln(\\lambda) \\in \\Pi = (-\\infty, \\infty)\\) \\(\\text{N}(\\mu, \\sigma^2)\\) : \\(\\begin{align*}\\theta = (\\mu, \\sigma^2) \\in \\Omega = (-\\infty, \\infty) \\times (0, \\infty),\\\\ (\\pi_1(\\lambda), \\pi_2(\\lambda)) = \\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right) \\in \\Pi = (-\\infty, 0) \\times (-\\infty, \\infty)\\end{align*}\\) \\(\\text{N}(|\\theta|, |\\theta|+\\theta^2)\\) : \\(\\theta \\in \\Omega = (-\\infty, -1] \\cap [1, \\infty)\\) , \\((\\pi_1(\\theta), \\pi_2(\\theta)) = \\left(\\frac{1}{2(|\\theta + \\theta^2)}, \\frac{1}{1+|\\theta|}\\right) \\in \\Pi = \\{\\pi_1 = \\frac{\\pi_2^2}{2(\\pi_2-1)}\\}\\) i.i.d. samples from exponential family also exponential family form \\[ f(x_1, \\cdots, x_n | \\pi) = f(x_1|\\pi) \\cdots f(x_n|\\pi) \\propto h(x_1) \\cdots h(x_n) \\exp\\left(\\sum_j\\pi_j\\sum^n_{i=1}x_j(x_i)\\right) \\] Natural statistic: \\[ \\pi = \\pi_j \\in \\Pi, T = \\sum^n_{i=1}x_j(x_i) \\] Theorem: Distribution of natural statistic \\(T = (t_1(X), \\cdots, t_k(X))\\) has exponential family form with natural parameter also given by \\(\\pi \\in \\Pi\\) Example Conditional distribution of natural statistic Conditional distribution of natural statistic Distribution of \\(T_1\\) conditional on \\(T_2\\) has exponential family form with natural parameter \\(\\pi_1\\) Success Can get rid of \\(\\pi_2\\) Proof \\(\\text{Conditional} = \\frac{\\text{Joint of } T_1, T_2}{\\text{Marginal of } T_2}\\) \\(g(t_1 | t_2, \\pi) = \\frac{g(t_1, t_2 | \\pi)}{g(t_2 | \\pi)} \\propto g(t_1, t_2 | \\pi) \\propto h^* (t_1, t_2) exp(\\pi_1 t_1 + \\pi_2 t_2) \\propto h^{**} exp (\\pi_1 t_1)\\) So we can try to formulate \\(\\pi_1\\) such that it is useful to us. Example","title":"Chapter 3: Exponential Families"},{"location":"stat/3602/ch3.html#chapter-3-exponential-families","text":"","title":"Chapter 3: Exponential Families"},{"location":"stat/3602/ch3.html#exponential-family","text":"Theorem: Exponential family \\[ f(x|\\theta) \\propto h(x) \\exp(\\sum^k_{j=1} \\pi_j(\\theta) t_j(x)), x \\in S \\text{ (free of } \\theta \\text{)} \\] \\[ f(x|\\pi) \\propto h(x) \\exp(\\sum^k_{j=1} \\pi_j t_j(x)), x \\in S \\text{ (free of } \\theta \\text{)} \\] \\[ f(x|\\pi) = C(\\pi) h(x) \\exp(\\sum^k_{j=1} \\pi_j t_j(x)) \\text{ (normalizing constant)} \\] Note: Uniform distribution is not exponential family form, because \\(x \\in [0, \\theta]\\) , the sample space depends on \\(\\theta\\) . Example","title":"Exponential family"},{"location":"stat/3602/ch3.html#natural-parameter-space","text":"Natural parameter space: \\(\\Pi = \\{ \\pi = (\\pi_1(\\theta), \\pi_2(\\theta), \\cdots, \\pi_k(\\theta)) : \\theta \\in \\Theta \\} \\subset \\mathbb{R}^k\\) Natural parameter \\(\\pi = (\\pi_1, \\cdots, \\pi_k) \\in \\Pi\\) Natural statistic \\((t_1(X), \\cdots, t_2(X))\\) for \\(X \\in S\\) drawn from \\(f(\\cdot | \\theta)\\) e.g. \\(T = \\sum^n_{i=1}X_i\\) , \\(T = (\\sum^n_{i=1}X_i^2, \\sum^n_{i=1}X_i)\\) Natural parameter space \\(\\Pi\\) can be different from parameter space \\(\\Theta\\) Example ( \\(N(|\\theta|, |\\theta| + \\theta^2)\\) \\(\\text{Poisson}(\\lambda)\\) : \\(\\theta = \\lambda \\in \\Omega = (0, \\infty), \\pi_1(\\lambda) = ln(\\lambda) \\in \\Pi = (-\\infty, \\infty)\\) \\(\\text{N}(\\mu, \\sigma^2)\\) : \\(\\begin{align*}\\theta = (\\mu, \\sigma^2) \\in \\Omega = (-\\infty, \\infty) \\times (0, \\infty),\\\\ (\\pi_1(\\lambda), \\pi_2(\\lambda)) = \\left(-\\frac{1}{2\\sigma^2}, \\frac{\\mu}{\\sigma^2}\\right) \\in \\Pi = (-\\infty, 0) \\times (-\\infty, \\infty)\\end{align*}\\) \\(\\text{N}(|\\theta|, |\\theta|+\\theta^2)\\) : \\(\\theta \\in \\Omega = (-\\infty, -1] \\cap [1, \\infty)\\) , \\((\\pi_1(\\theta), \\pi_2(\\theta)) = \\left(\\frac{1}{2(|\\theta + \\theta^2)}, \\frac{1}{1+|\\theta|}\\right) \\in \\Pi = \\{\\pi_1 = \\frac{\\pi_2^2}{2(\\pi_2-1)}\\}\\) i.i.d. samples from exponential family also exponential family form \\[ f(x_1, \\cdots, x_n | \\pi) = f(x_1|\\pi) \\cdots f(x_n|\\pi) \\propto h(x_1) \\cdots h(x_n) \\exp\\left(\\sum_j\\pi_j\\sum^n_{i=1}x_j(x_i)\\right) \\] Natural statistic: \\[ \\pi = \\pi_j \\in \\Pi, T = \\sum^n_{i=1}x_j(x_i) \\] Theorem: Distribution of natural statistic \\(T = (t_1(X), \\cdots, t_k(X))\\) has exponential family form with natural parameter also given by \\(\\pi \\in \\Pi\\) Example","title":"Natural parameter space"},{"location":"stat/3602/ch3.html#conditional-distribution-of-natural-statistic","text":"Conditional distribution of natural statistic Distribution of \\(T_1\\) conditional on \\(T_2\\) has exponential family form with natural parameter \\(\\pi_1\\) Success Can get rid of \\(\\pi_2\\) Proof \\(\\text{Conditional} = \\frac{\\text{Joint of } T_1, T_2}{\\text{Marginal of } T_2}\\) \\(g(t_1 | t_2, \\pi) = \\frac{g(t_1, t_2 | \\pi)}{g(t_2 | \\pi)} \\propto g(t_1, t_2 | \\pi) \\propto h^* (t_1, t_2) exp(\\pi_1 t_1 + \\pi_2 t_2) \\propto h^{**} exp (\\pi_1 t_1)\\) So we can try to formulate \\(\\pi_1\\) such that it is useful to us. Example","title":"Conditional distribution of natural statistic"},{"location":"stat/3602/ch4.html","text":"Chapter 4: Sufficiency and Likelihood Fundamental problem in parametric statistical inference: How to infer about \\(\\theta\\) from evidence \\(X\\) , \\(X \\sim f(x|\\theta)\\) ? Can observation \\(X\\) help at all? Statistic : a \"partition\" of sample space and \"carrier\" of information Sufficient statistic Definition: \\(T = T(X)\\) sufficient for \\(\\theta\\) if distribution of \\(X\\) conditional on \\(T\\) does not depend on \\(\\theta\\) i.e. given knowledge of \\(T(X)\\) , no extra information relevant to \\(\\theta\\) can be gained by knowing further details of \\(X\\) info contained in \\(T(X)\\) about \\(\\theta\\) = info contained in \\(X\\) about \\(\\theta\\) Sufficiency principle: Under same experimental setting, if \\(T(X) = T(X')\\) , then inference from \\(X_A\\) \\(\\equiv\\) Inference from \\(X_B\\) . Outcomes in same cell of partition (same \\(T(X)\\) ) share proportional likelihood function \\(l(\\theta)\\) . Sufficient statistic is not unique. Likelihood function \\(l_X(\\theta) \\propto f(X|\\theta)\\) Which \\(\\theta\\) is more likely Proportionality constant to scale the sum to \\(1\\) . \"posterior probability function\" derived from non-informative prioir. but the area of \\(l_X(\\theta)\\) may be \\(\\infty\\) Loglikelihood function \\(S_X(\\theta) = \\ln l_x(\\theta)\\) \\(S_X(\\theta) = \\ln l_X(\\theta) = \\ln \\prod^n_{i=1} l_{X_i}(\\theta) = \\sum^n_{i=1} S_{X_i}(\\theta)\\) sum of independent terms, good for Central Limit Theorem , WLNN , SLNN , ... Theorem Proof (proportional likelihood function) Proof (factorization theorem) Proof (factorization is sufficient) Example Minimal sufficient \\(T\\) is sufficient for \\(\\theta\\) and is function of any sufficient statistic for \\(\\theta\\) \\(\\forall X, X', T(X) = T(X') \\Leftrightarrow l_X(\\theta) \\propto l_{X'}(\\theta)\\) Remark \\(T(X)\\) sufficient for \\(\\theta\\) : iff \\(\\forall X, X', T(X) = T(X') \\Rightarrow l_x(\\theta) \\propto l_{x'}(\\theta)\\) One direction only!! Example (why \\(\\max{X_i}\\) is minimal sufficient for \\(U[0, \\theta]\\) ) Sufficiency of exponential family Theorem: Natural statistic \\(T\\) is sufficient. Theorem: If \\(\\Pi\\) is not contained in any affine hyperplane in \\(\\mathbb{R}^k\\) , then \\(T(X)\\) is minimal sufficient for \\(\\theta\\) (NOT single point in \\(\\mathbb{R}_1\\) , NOT straight line in \\(\\mathbb{R}_2\\) Example: 2D rectangle minimal sufficient Example: 2D straight line not minimal sufficient Example: 2D curve minimal sufficient Theorem: If \\(\\Pi\\) contains open rectangle in \\(\\mathbb{R}^k\\) , then natural statistic \\(T(X)\\) is complete sufficient for \\(\\pi\\) . Open rectangle: Open interval in 1d, Open rectangle in 2d Proof Quote The natural parameter space for \\(\\ln{\\theta}\\) is \\((-\\infty, \\infty)\\) , which contains an open interval. Thus \\(T\\) is a complete sufficient statistic for \\(\\theta\\) . 2D rectangle complete 2D curve not complete Warning If not contain open rectangle, it is possible that it is still complete sufficient Example Warning Complete statistic must be minimal sufficient But minimal sufficient statistic may not be complete Not complete statistic can still be minimal sufficient Likelihood principle If \\(l_A(\\theta) \\propto l_B(\\theta)\\) , then \\(\\text{Inference from A } \\equiv \\text{ Inference from B}\\) Complete sufficient Definition: \\(T(X)\\) sufficient, \\(\\mathbb{E}_\\theta[g(T)] = 0 \\forall \\theta \\Rightarrow \\mathbb{P}(g(T) = 0) = 1 \\forall \\theta\\) Theorem (Lehmann-Scheffe): \\(T(X)\\) complete sufficient for \\(\\theta\\) \\(\\Rightarrow\\) \\(T(X)\\) minimal sufficient for \\(\\theta\\) Proof Example Example Example Remark on proportionality Warning","title":"Chapter 4: Sufficiency and Likelihood"},{"location":"stat/3602/ch4.html#chapter-4-sufficiency-and-likelihood","text":"Fundamental problem in parametric statistical inference: How to infer about \\(\\theta\\) from evidence \\(X\\) , \\(X \\sim f(x|\\theta)\\) ? Can observation \\(X\\) help at all? Statistic : a \"partition\" of sample space and \"carrier\" of information","title":"Chapter 4: Sufficiency and Likelihood"},{"location":"stat/3602/ch4.html#sufficient-statistic","text":"Definition: \\(T = T(X)\\) sufficient for \\(\\theta\\) if distribution of \\(X\\) conditional on \\(T\\) does not depend on \\(\\theta\\) i.e. given knowledge of \\(T(X)\\) , no extra information relevant to \\(\\theta\\) can be gained by knowing further details of \\(X\\) info contained in \\(T(X)\\) about \\(\\theta\\) = info contained in \\(X\\) about \\(\\theta\\) Sufficiency principle: Under same experimental setting, if \\(T(X) = T(X')\\) , then inference from \\(X_A\\) \\(\\equiv\\) Inference from \\(X_B\\) . Outcomes in same cell of partition (same \\(T(X)\\) ) share proportional likelihood function \\(l(\\theta)\\) . Sufficient statistic is not unique.","title":"Sufficient statistic"},{"location":"stat/3602/ch4.html#likelihood-function","text":"\\(l_X(\\theta) \\propto f(X|\\theta)\\) Which \\(\\theta\\) is more likely Proportionality constant to scale the sum to \\(1\\) . \"posterior probability function\" derived from non-informative prioir. but the area of \\(l_X(\\theta)\\) may be \\(\\infty\\)","title":"Likelihood function"},{"location":"stat/3602/ch4.html#loglikelihood-function","text":"\\(S_X(\\theta) = \\ln l_x(\\theta)\\) \\(S_X(\\theta) = \\ln l_X(\\theta) = \\ln \\prod^n_{i=1} l_{X_i}(\\theta) = \\sum^n_{i=1} S_{X_i}(\\theta)\\) sum of independent terms, good for Central Limit Theorem , WLNN , SLNN , ... Theorem Proof (proportional likelihood function) Proof (factorization theorem) Proof (factorization is sufficient) Example","title":"Loglikelihood function"},{"location":"stat/3602/ch4.html#minimal-sufficient","text":"\\(T\\) is sufficient for \\(\\theta\\) and is function of any sufficient statistic for \\(\\theta\\) \\(\\forall X, X', T(X) = T(X') \\Leftrightarrow l_X(\\theta) \\propto l_{X'}(\\theta)\\) Remark \\(T(X)\\) sufficient for \\(\\theta\\) : iff \\(\\forall X, X', T(X) = T(X') \\Rightarrow l_x(\\theta) \\propto l_{x'}(\\theta)\\) One direction only!! Example (why \\(\\max{X_i}\\) is minimal sufficient for \\(U[0, \\theta]\\) )","title":"Minimal sufficient"},{"location":"stat/3602/ch4.html#sufficiency-of-exponential-family","text":"Theorem: Natural statistic \\(T\\) is sufficient. Theorem: If \\(\\Pi\\) is not contained in any affine hyperplane in \\(\\mathbb{R}^k\\) , then \\(T(X)\\) is minimal sufficient for \\(\\theta\\) (NOT single point in \\(\\mathbb{R}_1\\) , NOT straight line in \\(\\mathbb{R}_2\\) Example: 2D rectangle minimal sufficient Example: 2D straight line not minimal sufficient Example: 2D curve minimal sufficient Theorem: If \\(\\Pi\\) contains open rectangle in \\(\\mathbb{R}^k\\) , then natural statistic \\(T(X)\\) is complete sufficient for \\(\\pi\\) . Open rectangle: Open interval in 1d, Open rectangle in 2d Proof Quote The natural parameter space for \\(\\ln{\\theta}\\) is \\((-\\infty, \\infty)\\) , which contains an open interval. Thus \\(T\\) is a complete sufficient statistic for \\(\\theta\\) . 2D rectangle complete 2D curve not complete Warning If not contain open rectangle, it is possible that it is still complete sufficient Example Warning Complete statistic must be minimal sufficient But minimal sufficient statistic may not be complete Not complete statistic can still be minimal sufficient","title":"Sufficiency of exponential family"},{"location":"stat/3602/ch4.html#likelihood-principle","text":"If \\(l_A(\\theta) \\propto l_B(\\theta)\\) , then \\(\\text{Inference from A } \\equiv \\text{ Inference from B}\\)","title":"Likelihood principle"},{"location":"stat/3602/ch4.html#complete-sufficient","text":"Definition: \\(T(X)\\) sufficient, \\(\\mathbb{E}_\\theta[g(T)] = 0 \\forall \\theta \\Rightarrow \\mathbb{P}(g(T) = 0) = 1 \\forall \\theta\\) Theorem (Lehmann-Scheffe): \\(T(X)\\) complete sufficient for \\(\\theta\\) \\(\\Rightarrow\\) \\(T(X)\\) minimal sufficient for \\(\\theta\\) Proof Example Example Example","title":"Complete sufficient"},{"location":"stat/3602/ch4.html#remark-on-proportionality","text":"Warning","title":"Remark on proportionality"},{"location":"stat/3602/ch5.html","text":"Chapter 5: Estimation Rao-Blackwell Theorem Define \\(\\rho^* = \\rho^*(T) = \\mathbb{E}[\\rho(X) | T]\\) \\(\\rho^*\\) is an estimator of \\(\\psi(\\theta)\\) (it is a function of data, not depending on \\(\\theta\\) ) \\(\\mathbb{E}[L(\\theta, \\rho^*)] \\leq \\mathbb{E}[L(\\theta, \\rho)]\\) (MSE) If \\(\\rho\\) unbiased and \\(T\\) complete for \\(\\theta\\) , then \\(\\rho^* = \\rho^*(T)\\) is the only function of \\(T\\) which is unbiased for \\(\\psi(\\theta)\\) \\(\\forall\\) unbiased estimator \\(S\\) of \\(\\psi(\\theta)\\) , \\(\\mathbb{E}[L(\\theta, \\rho^*)] \\leq \\mathbb{E}[L(\\theta, S)] \\forall \\theta\\) (UMVU estimator under squared loss) Important implications Provide constructive way to find \"optimal\" (smallest risk & unbiased) estimator (e.g. UMVU estimator) if complete sufficient statistic \\(T\\) exists. Either: Choose any unbiased estimator \\(\\rho\\) , modify it to \\(\\rho^* = \\mathbb[\\rho|T]\\) Find function \\(g(T)\\) which is unbiased, then just use \\(g(T)\\) They will get the same result from 3.2. Also, Rao-Blackwell highlights an important use of complete sufficient statistic. This alone justifies our need to study the concept of completeness . Exam example: 2017 ... some calculations of MLE and complete sufficient statistic The mle \\(\\hat{\\theta}\\) is a function of \\(X-Y\\) according to (previous part). If it were unbiased for \\(\\theta\\) , then it must be identical to the UMVU estimator by Rao-Blackwell Theorem. That the two estimates differ on the data, which has a positive probability to be observed, implies that \\(\\hat{\\theta}\\) cannot be unbiased for \\(\\theta\\) . Tutorial 9 ... calculate MLE which is biased then multiply it by a constant to make it unbiased ... calculate CRLB ... calculate variance of MLE using elementary techniques The CRLB is not attained by the unbiased estimator. In fact, as \\(T\\) is a complete sufficient statistic, the CRLB cannot be attained by any unbiased estimator. (Re: Only one function of \\(T\\) is unbiased estimator) Example Fisher information Motivation Compare experiments and their likelihood functions. It is better to have a likelihood function that is \"sharper\" at the true value. Sharper means the negative of the second derivative of loglikelihood! Tip Theorem: Information matrix Information inequality, CRLB Theorem CRLB CRLB: \\(\\text{Var}_\\theta(\\hat{\\theta}_U) \\geq \\frac{1}{I(\\theta)}\\) Exam example: 2016 You can put \\(T\\) to be an (unbiased) estimator to check the variance of that estimator. Regularity conditions Sample space \\(S\\) must not depend on \\(\\theta\\) \\(U[0, \\theta]\\) ruled out Parameter space \\(\\theta\\) contains open rectangle Discrete \\(\\theta\\) ruled out \\(f(x|\\theta)\\) twice differentiable w.r.t. \\(\\theta\\) \\(\\text{Binomial}(\\theta, p)\\) ruled out, \\(\\text{Binomial}(n, \\theta)\\) ok What if the regularity assumptions are violated? Maximum likelihood estimator (MLE) Obtained by solving likelihood equations: \\(U(\\theta) = \\frac{\\partial S_X(\\theta)}{\\partial \\theta} = 0\\) MLE is function of minimal sufficient statistic \\(T\\) , but \\(\\hat{\\theta}\\) may not be sufficient. MLE may not be unbiased \\(\\hat{\\theta}\\) mle of \\(\\theta\\) , then \\(\\psi(\\hat{\\theta})\\) mle of \\(\\psi(\\theta)\\) Large sample properties of MLE, Asymptotic distribution Asymptotic distribution of \\(\\hat{\\theta}_n\\) Abstract \\(\\hat{\\theta}_n \\to \\theta_0\\) in probability \\(\\sqrt{n} (\\hat{\\theta}_n - \\theta_0) \\to N(0, \\mathscr{I}(\\theta_0)^{-1})\\) (very important!!!) \\(\\mathscr{I}(\\theta_0) = \\lim_{n\\to\\infty} n^{-1}I(\\theta_0)\\) \\(\\hat{\\theta}_n \\sim N(\\theta_0, I(\\theta_0)^{-1})\\) (inverse of Fisher information matrix) \\(n^{-1/2} U(\\theta_0) \\to N(0, \\mathscr{I}(\\theta_0))\\) Warning Would not help if \\(\\psi(\\hat{\\theta}_n)\\) is changed to \\(\\mathbb{E}[\\psi(\\hat{\\theta}_n) | \\text{sufficient statistic}]\\) , as already \\(\\psi(\\hat{\\theta}_n) = \\mathbb{E}[\\psi(\\hat{\\theta}_n) | \\text{sufficient statistic}]\\) Note In practice, to obtain the asymptotic distribution from data, we can calculate \\(\\hat{\\theta}_n\\) and \\(I(\\hat{\\theta}_n)^{-1}\\) and treat them as \\(\\theta_0\\) and \\(I(\\theta_0)^{-1}\\) respectively.","title":"Chapter 5: Estimation"},{"location":"stat/3602/ch5.html#chapter-5-estimation","text":"","title":"Chapter 5: Estimation"},{"location":"stat/3602/ch5.html#rao-blackwell-theorem","text":"Define \\(\\rho^* = \\rho^*(T) = \\mathbb{E}[\\rho(X) | T]\\) \\(\\rho^*\\) is an estimator of \\(\\psi(\\theta)\\) (it is a function of data, not depending on \\(\\theta\\) ) \\(\\mathbb{E}[L(\\theta, \\rho^*)] \\leq \\mathbb{E}[L(\\theta, \\rho)]\\) (MSE) If \\(\\rho\\) unbiased and \\(T\\) complete for \\(\\theta\\) , then \\(\\rho^* = \\rho^*(T)\\) is the only function of \\(T\\) which is unbiased for \\(\\psi(\\theta)\\) \\(\\forall\\) unbiased estimator \\(S\\) of \\(\\psi(\\theta)\\) , \\(\\mathbb{E}[L(\\theta, \\rho^*)] \\leq \\mathbb{E}[L(\\theta, S)] \\forall \\theta\\) (UMVU estimator under squared loss) Important implications Provide constructive way to find \"optimal\" (smallest risk & unbiased) estimator (e.g. UMVU estimator) if complete sufficient statistic \\(T\\) exists. Either: Choose any unbiased estimator \\(\\rho\\) , modify it to \\(\\rho^* = \\mathbb[\\rho|T]\\) Find function \\(g(T)\\) which is unbiased, then just use \\(g(T)\\) They will get the same result from 3.2. Also, Rao-Blackwell highlights an important use of complete sufficient statistic. This alone justifies our need to study the concept of completeness . Exam example: 2017 ... some calculations of MLE and complete sufficient statistic The mle \\(\\hat{\\theta}\\) is a function of \\(X-Y\\) according to (previous part). If it were unbiased for \\(\\theta\\) , then it must be identical to the UMVU estimator by Rao-Blackwell Theorem. That the two estimates differ on the data, which has a positive probability to be observed, implies that \\(\\hat{\\theta}\\) cannot be unbiased for \\(\\theta\\) . Tutorial 9 ... calculate MLE which is biased then multiply it by a constant to make it unbiased ... calculate CRLB ... calculate variance of MLE using elementary techniques The CRLB is not attained by the unbiased estimator. In fact, as \\(T\\) is a complete sufficient statistic, the CRLB cannot be attained by any unbiased estimator. (Re: Only one function of \\(T\\) is unbiased estimator) Example","title":"Rao-Blackwell Theorem"},{"location":"stat/3602/ch5.html#fisher-information","text":"Motivation Compare experiments and their likelihood functions. It is better to have a likelihood function that is \"sharper\" at the true value. Sharper means the negative of the second derivative of loglikelihood! Tip Theorem: Information matrix","title":"Fisher information"},{"location":"stat/3602/ch5.html#information-inequality-crlb","text":"Theorem CRLB CRLB: \\(\\text{Var}_\\theta(\\hat{\\theta}_U) \\geq \\frac{1}{I(\\theta)}\\) Exam example: 2016 You can put \\(T\\) to be an (unbiased) estimator to check the variance of that estimator. Regularity conditions Sample space \\(S\\) must not depend on \\(\\theta\\) \\(U[0, \\theta]\\) ruled out Parameter space \\(\\theta\\) contains open rectangle Discrete \\(\\theta\\) ruled out \\(f(x|\\theta)\\) twice differentiable w.r.t. \\(\\theta\\) \\(\\text{Binomial}(\\theta, p)\\) ruled out, \\(\\text{Binomial}(n, \\theta)\\) ok What if the regularity assumptions are violated?","title":"Information inequality, CRLB"},{"location":"stat/3602/ch5.html#maximum-likelihood-estimator-mle","text":"Obtained by solving likelihood equations: \\(U(\\theta) = \\frac{\\partial S_X(\\theta)}{\\partial \\theta} = 0\\) MLE is function of minimal sufficient statistic \\(T\\) , but \\(\\hat{\\theta}\\) may not be sufficient. MLE may not be unbiased \\(\\hat{\\theta}\\) mle of \\(\\theta\\) , then \\(\\psi(\\hat{\\theta})\\) mle of \\(\\psi(\\theta)\\)","title":"Maximum likelihood estimator (MLE)"},{"location":"stat/3602/ch5.html#large-sample-properties-of-mle-asymptotic-distribution","text":"Asymptotic distribution of \\(\\hat{\\theta}_n\\) Abstract \\(\\hat{\\theta}_n \\to \\theta_0\\) in probability \\(\\sqrt{n} (\\hat{\\theta}_n - \\theta_0) \\to N(0, \\mathscr{I}(\\theta_0)^{-1})\\) (very important!!!) \\(\\mathscr{I}(\\theta_0) = \\lim_{n\\to\\infty} n^{-1}I(\\theta_0)\\) \\(\\hat{\\theta}_n \\sim N(\\theta_0, I(\\theta_0)^{-1})\\) (inverse of Fisher information matrix) \\(n^{-1/2} U(\\theta_0) \\to N(0, \\mathscr{I}(\\theta_0))\\) Warning Would not help if \\(\\psi(\\hat{\\theta}_n)\\) is changed to \\(\\mathbb{E}[\\psi(\\hat{\\theta}_n) | \\text{sufficient statistic}]\\) , as already \\(\\psi(\\hat{\\theta}_n) = \\mathbb{E}[\\psi(\\hat{\\theta}_n) | \\text{sufficient statistic}]\\) Note In practice, to obtain the asymptotic distribution from data, we can calculate \\(\\hat{\\theta}_n\\) and \\(I(\\hat{\\theta}_n)^{-1}\\) and treat them as \\(\\theta_0\\) and \\(I(\\theta_0)^{-1}\\) respectively.","title":"Large sample properties of MLE, Asymptotic distribution"},{"location":"stat/3602/ch6.html","text":"Chapter 6: Hypothesis testing Some definitions Abstract Test function: \\(\\phi(X) = \\mathbb{P}(\\text{reject }H_0 | X)\\) If we observe \\(X\\) , then reject \\(H_0\\) with probability \\(\\phi(X)\\) Power function: \\(\\omega(\\theta) = \\mathbb{E}_\\theta [\\phi(X)]\\) Re: Type I error, 1 - Type II error Probablity of reject given the true \\(\\theta\\) Size: \\(\\sup_{\\theta \\in \\Theta_0} \\omega(\\theta) = \\alpha\\) Probability of rejecting true \\(H_0\\) Power: \\(\\omega(\\theta)\\) at \\(\\theta \\in \\Theta_1\\) p-value: \\(\\sup_{\\theta \\in \\Theta_0} \\mathbb{P}_\\theta (T(X) \\geq T(x))\\) \\(x\\) is observation, \\(T(X)\\) is random and has a known distribution Smallest size of LR test that rejects \\(H_0\\) (refer to ranking example) Test is unbiased for size \\(\\alpha\\) if \\(\\sup_{\\theta \\in \\Theta_0} \\omega(\\theta) = \\alpha\\) , and \\(\\omega(\\theta) \\geq \\alpha \\forall \\theta \\in \\Theta_1 \\setminus \\Theta_0\\) UMP: Size \\(\\leq \\alpha\\) , and power is the greatest at \\(\\Theta_1\\) for all tests. If it exists, it is unbiased. UMPU: Size \\(\\leq \\alpha\\) , and power is the greatest at \\(\\Theta_1\\) for all unbiased tests Note on critical region and p-value p-value: \\(\\mathbb{P}_{\\theta_0} (T(X) \\geq T(x))\\) Critical region: \\(\\mathbb{P}_{\\theta_0} (T(X) > c) = \\alpha\\) Likelihood ratio test Neyman Pearson Lemma For simple hypothesis, likelihood ratio test is most powerful among all tests with size \\(\\alpha\\) Example UMP test under monotone likelihood ratio (mlr property) See if the likelihood ratio is monotonic respect to some statistic \\(T(X)\\) Need the likelihood ratio to be increasing. So might need to take \\(-T(X)\\) . Theorem Test function \\(T(X) > t_0\\) is LR test, unbiased, UMP among tests of size \\(\\leq \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}_\\theta [\\phi_0(X)]\\) Note for exponential family: The sign of \\(T\\) follows sign of test. Example Conditional test for exponential family Only want to know single parameter and ignoring others Theorem Example: Normal Refer to slides fuck Example: Poisson UMPU test for exponential family Two sided: \\(H_0: \\theta \\in [\\theta_1, \\theta_2]\\) vs \\(H_1: \\theta \\notin [\\theta_1, \\theta_2]\\) Actually can just refer to the conditional case Summary Theorem (UMPU interval two-sided) Theorem (UMPU Single point two sided: \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\) ) Example 1: Interval Example 2: Point Confidence set Invert the hypothesis test: Find the \\(\\theta_0\\) such that \\(H_0\\) will not be rejected. Example: Test \\(\\theta = k\\) vs \\(\\theta > k\\) . Then during hypothesis testing, we will obtain the p-value \\(\\mathbb{P}(T_{random} > t_{obs} | \\theta = k)\\) . So to obtain the confidence bound, we solve \\(\\mathbb{P}(T_{random} > t_{obs} | \\theta = k) \\geq \\alpha\\) on \\(k\\) . Note Example 2018 exam Generalized Likelihood Ratio (GLR) Test Warning As this is a test based on large-sample theory, the size may be greater than \\(\\alpha\\) if \\(n\\) is too small... Find the degree of freedom \\(s\\) for \\(H_1\\) , \\(r\\) for \\(H_0\\) . \\(H_0\\) is more constrained than \\(H_1\\) (e.g. \\(\\mu = \\lambda\\) ) \\(G = 2(\\ln(\\ell_X(\\hat{\\theta}) - \\ln(\\ell_X(\\tilde{\\theta})))) > \\chi^2_{r-s}(\\alpha)\\) Samples are independent, not necessarily identically distributed Samples are independent, not necessarily identically distributed Theorem Example Example Part 1: Find MLE and constrained MLE Part 2: Find \\(G = 2(\\ln(\\ell_X(\\hat{\\theta}) - \\ln(\\ell_X(\\tilde{\\theta})))) > \\chi^2_{r-s}(\\alpha)\\)","title":"Chapter 6: Hypothesis testing"},{"location":"stat/3602/ch6.html#chapter-6-hypothesis-testing","text":"","title":"Chapter 6: Hypothesis testing"},{"location":"stat/3602/ch6.html#some-definitions","text":"Abstract Test function: \\(\\phi(X) = \\mathbb{P}(\\text{reject }H_0 | X)\\) If we observe \\(X\\) , then reject \\(H_0\\) with probability \\(\\phi(X)\\) Power function: \\(\\omega(\\theta) = \\mathbb{E}_\\theta [\\phi(X)]\\) Re: Type I error, 1 - Type II error Probablity of reject given the true \\(\\theta\\) Size: \\(\\sup_{\\theta \\in \\Theta_0} \\omega(\\theta) = \\alpha\\) Probability of rejecting true \\(H_0\\) Power: \\(\\omega(\\theta)\\) at \\(\\theta \\in \\Theta_1\\) p-value: \\(\\sup_{\\theta \\in \\Theta_0} \\mathbb{P}_\\theta (T(X) \\geq T(x))\\) \\(x\\) is observation, \\(T(X)\\) is random and has a known distribution Smallest size of LR test that rejects \\(H_0\\) (refer to ranking example) Test is unbiased for size \\(\\alpha\\) if \\(\\sup_{\\theta \\in \\Theta_0} \\omega(\\theta) = \\alpha\\) , and \\(\\omega(\\theta) \\geq \\alpha \\forall \\theta \\in \\Theta_1 \\setminus \\Theta_0\\) UMP: Size \\(\\leq \\alpha\\) , and power is the greatest at \\(\\Theta_1\\) for all tests. If it exists, it is unbiased. UMPU: Size \\(\\leq \\alpha\\) , and power is the greatest at \\(\\Theta_1\\) for all unbiased tests","title":"Some definitions"},{"location":"stat/3602/ch6.html#note-on-critical-region-and-p-value","text":"p-value: \\(\\mathbb{P}_{\\theta_0} (T(X) \\geq T(x))\\) Critical region: \\(\\mathbb{P}_{\\theta_0} (T(X) > c) = \\alpha\\)","title":"Note on critical region and p-value"},{"location":"stat/3602/ch6.html#likelihood-ratio-test","text":"Neyman Pearson Lemma For simple hypothesis, likelihood ratio test is most powerful among all tests with size \\(\\alpha\\) Example","title":"Likelihood ratio test"},{"location":"stat/3602/ch6.html#ump-test-under-monotone-likelihood-ratio-mlr-property","text":"See if the likelihood ratio is monotonic respect to some statistic \\(T(X)\\) Need the likelihood ratio to be increasing. So might need to take \\(-T(X)\\) . Theorem Test function \\(T(X) > t_0\\) is LR test, unbiased, UMP among tests of size \\(\\leq \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}_\\theta [\\phi_0(X)]\\) Note for exponential family: The sign of \\(T\\) follows sign of test. Example","title":"UMP test under monotone likelihood ratio (mlr property)"},{"location":"stat/3602/ch6.html#conditional-test-for-exponential-family","text":"Only want to know single parameter and ignoring others Theorem Example: Normal Refer to slides fuck Example: Poisson","title":"Conditional test for exponential family"},{"location":"stat/3602/ch6.html#umpu-test-for-exponential-family","text":"Two sided: \\(H_0: \\theta \\in [\\theta_1, \\theta_2]\\) vs \\(H_1: \\theta \\notin [\\theta_1, \\theta_2]\\) Actually can just refer to the conditional case Summary Theorem (UMPU interval two-sided) Theorem (UMPU Single point two sided: \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\neq \\theta_0\\) ) Example 1: Interval Example 2: Point","title":"UMPU test for exponential family"},{"location":"stat/3602/ch6.html#confidence-set","text":"Invert the hypothesis test: Find the \\(\\theta_0\\) such that \\(H_0\\) will not be rejected. Example: Test \\(\\theta = k\\) vs \\(\\theta > k\\) . Then during hypothesis testing, we will obtain the p-value \\(\\mathbb{P}(T_{random} > t_{obs} | \\theta = k)\\) . So to obtain the confidence bound, we solve \\(\\mathbb{P}(T_{random} > t_{obs} | \\theta = k) \\geq \\alpha\\) on \\(k\\) . Note Example 2018 exam","title":"Confidence set"},{"location":"stat/3602/ch6.html#generalized-likelihood-ratio-glr-test","text":"Warning As this is a test based on large-sample theory, the size may be greater than \\(\\alpha\\) if \\(n\\) is too small... Find the degree of freedom \\(s\\) for \\(H_1\\) , \\(r\\) for \\(H_0\\) . \\(H_0\\) is more constrained than \\(H_1\\) (e.g. \\(\\mu = \\lambda\\) ) \\(G = 2(\\ln(\\ell_X(\\hat{\\theta}) - \\ln(\\ell_X(\\tilde{\\theta})))) > \\chi^2_{r-s}(\\alpha)\\) Samples are independent, not necessarily identically distributed Samples are independent, not necessarily identically distributed Theorem Example Example Part 1: Find MLE and constrained MLE Part 2: Find \\(G = 2(\\ln(\\ell_X(\\hat{\\theta}) - \\ln(\\ell_X(\\tilde{\\theta})))) > \\chi^2_{r-s}(\\alpha)\\)","title":"Generalized Likelihood Ratio (GLR) Test"},{"location":"stat/3620/ch3.html","text":"Chapter 3: k-Sample methods When there are more than two treatments: Do an overall comparison to determine whether or not there are differences among the \\(k\\) samples If the differences are found, we perform multiple comparisons to further detect which treatments differ from each other. Test between \\(H_0: F_1(x) = F_2(x) = \\cdots = F_k(x)\\) for all \\(x\\) , and \\(H_1: F_i(x) \\leq F_j(x)\\) or \\(F_i(x) \\geq F_j(x)\\) for at least one pair with strict inequality holding for at least one \\(x\\) . Alternatively, let \\(F_i(x) = F(x - \\mu_i)\\) . Then the hypothesis become \\(H_0: \\mu_1(x) = \\mu_2(x) = \\cdots = \\mu_k(x)\\) for all \\(x\\) , and \\(H_1: \\mu_i\\) are not all equal Parametric method based on Normality Assumption \\(X_{ij} = \\mu_{i} + \\epsilon_{ij}\\) \\(SST = SSB + SSE\\) F-test: \\(F = \\frac{SSB/(k-1)}{SSE/(N-k)} \\sim F(k-1, N-k)\\) under \\(H_0\\) , large \\(F\\) values favor the rejection of \\(H_0\\) . However, if we are unwilling to assume that the population distributions are normal, we may carry out a permutation version of F-test instead. Permutation F-test Compute the F-statistic for the observed data, denoted \\(F_{obs}\\) . For each of the \\(\\frac{N!}{n_1!n_2! \\cdots n_k!}\\) permutations (or a random sample of the permutations), compute the F-statistic Calculate the p-value as the fraction of the F's that are greater than or equal to \\(F_{obs}\\) . Can also perform permutation test based on \\(SSB = \\sum^k_{i=1}n_i(\\bar{X_i} - \\bar{X})^2\\) or \\(SSX = \\sum^k_{i=1} n_i \\bar{X_i}^2\\) as the test statistic. Note Note that for large samples, the permutation distribution may be approximated by the F-distribution. In other words, the ANOVA test may well approximate the permutation F-test. This is easy to conprehend since sample means will be normally distributed for large samples, according to the Central Limit Theorem. Warning In practice, the ANOVA test is relatively robust to departures from the normality assumption but it is sensitive to departures from the constant variance assumption . link Example 3.1 Three preservatives were compared to see their ability to inhibit the growth of bacteria. Samples were treated with one of the preservatives at the same time or left untreated as a control. After 48 hours, bacteria counts were made. The logarithms of the counts (Why?) were computed to meet ANOVA assumptions, recorded as the table: ctrl = c ( 4.302 , 4.017 , 4.049 , 4.176 ) pre1 = c ( 2.021 , 3.190 , 3.250 , 3.276 , 3.292 , 3.267 ) pre2 = c ( 3.397 , 3.552 , 3.630 , 3.578 , 3.612 ) pre3 = c ( 2.699 , 2.929 , 2.785 , 2.176 , 2.845 , 2.913 ) x = c ( ctrl , pre1 , pre2 , pre3 ) grps = rep ( c ( \"ctrl\" , \"pre1\" , \"pre2\" , \"pre3\" ), c ( 4 , 6 , 5 , 6 )) # grps = rep(1:4, c(4,6,5,6)) dat <- data.frame ( x , grps ) aggregate ( x ~ grps , dat , mean ) # compute group means # one-way ANOVA model onewayanova <- ( aov ( x ~ factor ( grps ))) summary ( onewayanova ) str ( summary ( onewayanova )) Fobs <- summary ( onewayanova )[[ 1 ]][ 1 , 4 ] Fobs # Permutation-F test R <- 9999 reps <- numeric ( R ) for ( i in 1 : R ) { permdat <- data.frame ( x = dat $ x , grps = sample ( dat $ grps , size = dim ( dat )[ 1 ], replace = FALSE )) permanova <- aov ( x ~ factor ( grps ), data = permdat ) reps [ i ] <- summary ( permanova )[[ 1 ]][ 1 , 4 ] # extract the ith permuted F statistic } pvalue <- mean ( c ( Fobs , reps ) >= Fobs ) # always an upper-tailed test pvalue hist ( c ( Fobs , reps ), main = \"Null distribution of F statistic\" ) abline ( v = Fobs , lty = 2 , col = \"red\" ) > # one-way ANOVA model > onewayanova <- (aov(x ~ factor(grps))) > summary(onewayanova) Df Sum Sq Mean Sq F value Pr(>F) factor(grps) 3 5.476 1.8254 17.66 1.82e-05 *** Residuals 17 1.757 0.1034 \u2013- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 > > str(summary(onewayanova)) List of 1 $ :Classes \u2018anova\u2019 and \u2019data.frame\u2019: 2 obs. of 5 variables: ..$ Df : num [1:2] 3 17 ..$ Sum Sq : num [1:2] 5.48 1.76 ..$ Mean Sq: num [1:2] 1.825 0.103 ..$ F value: num [1:2] 17.7 NA ..$ Pr(>F) : num [1:2] 1.82e-05 NA - attr(*, \"class\")= chr [1:2] \"summary.aov\" \"listof\" > Fobs <- summary(onewayanova)[[1]][1,4] > Fobs [1] 17.65729 One-way ANOVA has F-statistic = 17:66 with p-value = 0:000018 while the permutation F-test based on R = 10000 random permutations gives the exact p-value = 0:0001 (is this OK?). Close agreement is found between the two p-values. Note that for large samples, the permutation distribution may be approximated by the F-distribution. In other words, the ANOVA test may well approximate the permutation F-test. This is easy to conprehend since sample means will be normally distributed for large samples, according to the Central Limit Theorem. Kruskal-Wallis Test (ANOVA based on ranks) Objective Obtaining a statistic based on ranks , similar to Wilcoxcon test. Approximating permutation test using chi-square, useful for large sample size. Rank the data combining all treatments. Let \\(\\bar{R_i}\\) denote the mean of the ranks of treatment \\(i\\) 's observations, assuming the data have no ties. The Kruskal-Wallis statistic is defined as: \\[ KW = \\frac{12}{N(N+1)}\\sum^k_{i=1}n_i\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2 = \\frac{12}{N(N+1)} \\sum^k_{i=1} n_i (\\bar{R_i})^2 - 3(N+1) \\] where \\(\\frac{N+1}{2}\\) is the overall mean of all ranks and \\(\\sum^k_{i=1}n_i\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2\\) can be viewed as the \\(SSB\\) on ranks. (recall \\(SSB = \\sum^k_{i=1}n_i(\\bar{X_i} - \\bar{X})^2\\) ) \\(\\frac{12}{N(N+1)}\\) is a constant scaling factor to make the \\(KW\\) statistic to follow approximately a chi-square distribution under \\(H_0\\) , i.e. \\(KW \\sim \\chi^2(k-1)\\) . (note that \\(E[\\sum^k_{i=1}n_i\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2] = \\frac{N(N+1)}{12}(k-1)\\) , we want \\(E[KW] = k-1\\) . \\(E[\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2] = Var(\\bar{R_i}) + (E[\\bar{R_i}] - \\frac{N+1}{2})^2 = \\frac{1}{n_i^2}\\frac{n_i(N-n_i)\\sigma^2}{N-1}\\) Can get the result from p.20 Expectation and Variance of Wilcoxon statistic. But here we are using average rank instead of rank sum, so divide by \\({n_i^2}\\) \\(\\sigma = \\frac{(N-1)(N+1)}{12}\\) The \"exact\" p-value of the \\(KW\\) test can be obtained by using the permutation method. Reject when the upper \\(\\alpha\\) % critical value from the chi-square table is less than \\(KW\\) . Note The permutation \\(KW\\) test tends to have smaller critical values than the chi-square critical values. Hence, if we use the chi-square approximation, rejection of the null hypothesis will assure the same result with the exact \\(KW\\) test, and the exact p-value will be smaller. Example 3.2 Continue to use the data from example 3.1, but this time the sample size \\(N = 21\\) is moderately large. \\(KW = \\cdots = 17.14\\) , \\(KW \\sim \\chi^2(3)\\) under \\(H_0\\) , the upper 1% critical value from the chi-square table is \\(11.34\\) . Thus the test is significant at the 1% level and \\(H_0\\) should be rejected. ctrl = c ( 4.302 , 4.017 , 4.049 , 4.176 ) pre1 = c ( 2.021 , 3.190 , 3.250 , 3.276 , 3.292 , 3.267 ) pre2 = c ( 3.397 , 3.552 , 3.630 , 3.578 , 3.612 ) pre3 = c ( 2.699 , 2.929 , 2.785 , 2.176 , 2.845 , 2.913 ) x = c ( ctrl , pre1 , pre2 , pre3 ) grps = rep ( c ( \"ctrl\" , \"pre1\" , \"pre2\" , \"pre3\" ), c ( 4 , 6 , 5 , 6 )) # grps = rep(1:4, c(4,6,5,6)) dat <- data.frame ( x , grps ) aggregate ( x ~ grps , dat , mean ) # compute group means # K-W test kruskal.test ( x ~ grps , data = dat ) # Alternative way of computing the K-W test statistic rank.x = rank ( x ) summary ( aov ( rank.x ~ factor ( grps ))) SSB = summary ( aov ( rank.x ~ factor ( grps )))[[ 1 ]][ 1 , 2 ] SR2 = var ( rank.x ) KW = SSB / SR2 # Exact p-value using permutation test R <- 9999 reps <- numeric ( R ) for ( i in 1 : R ) { permdat <- data.frame ( x = rank.x , grps = sample ( dat $ grps , size = dim ( dat )[ 1 ], replace = FALSE )) permkw <- aov ( x ~ factor ( grps ), data = permdat ) reps [ i ] <- summary ( permkw )[[ 1 ]][ 1 , 2 ] / SR2 } pvalue <- mean ( c ( KW , reps ) >= KW ) # always an upper-tailed test pvalue hist ( c ( KW , reps ), main = \"Null distribution of KW statistic\" ) abline ( v = KW , lty = 2 , col = \"red\" ) Adjustment for ties When data are tied, we adjust the ranks using the mid-ranks for tied data as we did for the Wilcoxon test. The permutation method can also be applied to the \\(KW\\) statistic. However, in order to maintain the chi-square approximation, the \\(KW\\) statistic should be modified: \\[ KW_{ties} = \\frac{KW}{1-\\frac{\\sum^g_{i=1}{(t_i^3-t_i)}}{N^3-N}} \\] Code If no ties, use kruskal.test function in the base R installation. If ties exist, use kruskal_test function in the R add-on package coin . Multiple comparisons We want more details of the differences to know the location of the differences. Either we do 2-sample tests for each pair, but this will induce higher type I error. (If independent, then \\(1-0.95^3\\) ) Aim: Control the type-I error while performing pairwise tests Bonferroni Adjustment (union bound) Use \\(\\alpha_i = \\frac{\\alpha}{k(k-1)/2}\\) . Bonferroni inequality (union bound): \\(P(E_1 \\cup \\cdots \\cup E_m) \\leq P(E_1) + \\cdots + P(E_m)\\) Too conservative, high rate of type II error (false negative) Very wide confidence interval Fisher's Protected Least Significant Difference (LSD) (use MSE instead of two-sample error in t-test) Perform F-test for equality of means (ANOVA). If result of F-test is to accept H0 (all same), then stop. Perform all pairwise t-tests at significance level \\(\\alpha\\) . \\[ |\\bar{X_i} - \\bar{X_j}| \\geq t_{\\alpha/2, N-k} \\sqrt{MSE(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Can use \\(\\alpha/2\\) significance level But type I error may be greater than \\(\\alpha\\) Why use t? Difference follows normal distribution MSE is chi squared. \\(\\frac{Z}{\\sqrt{\\chi^2/n}} \\tilde t_n\\) Rank-based Fisher's LSD Procedure for Large Samples Benefit of using ranks: Less sensitive to outliers Perform KW-test for equality Stop if result is to accept H0 Perform all pariwise \\(z\\) -tests at significance level \\(\\alpha\\) . \\[ |\\bar{R_i} - \\bar{R_j}| \\geq z_{\\alpha/2} \\sqrt{S_R^2(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Can use normal critical value because \\(S^2_R = \\frac{N(N+1)}{12}\\) or \\(\\frac{N(N+1)}{12} - \\frac{\\sum^g_{i=1}(t_i^3-t_i)}{12(N-1)}\\) (if ties exist) \\(Var(\\bar{R_i} - \\bar{R_j}) = S_R^2(\\frac{1}{n_i} + \\frac{1}{n_j})\\) Why use normal instead of t? \\(S_R^2\\) is constant (variance of ranks is constant) The only random thing is \\(|\\bar{R_i} - \\bar{R_j}|\\) , which is normal (large sample approximation). Tukey's Honesty Significant Difference (HSD) (largest difference between sample means among all pairs) Suppose populations are normally distributed and sample sizes are equal. \\[ Q = \\max_{i=j}\\frac{\\sqrt{n}|\\bar{X_i} - \\bar{X_j}|}{\\sqrt{MSE}} \\] MSE is for combined sample. \\[ |\\bar{X_i} - \\bar{X_j}| \\geq q(\\alpha, k, df)\\sqrt{\\frac{MSE}{n}}, df = N-k \\] Look up \\(q\\) -distribution table Guarntee experiment-wise error rate to be exactly \\(\\alpha\\) . Unequal sample size: Tukey-Kramer procedure \\[ |\\bar{X_i} - \\bar{X_j}| \\geq q(\\alpha, k, df)\\sqrt{\\frac{MSE}{2}(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Conservative, error rate \\(\\leq \\alpha\\) . Rank-based Tukey's HSD Procedure for Large Samples \\[ |\\bar{R_i} - \\bar{R_j}| \\geq q(\\alpha, k, \\infty)\\sqrt{\\frac{S^2_R}{n}} \\] \\(df = \\infty\\) because of large samples. Unequal sample size \\[ |\\bar{R_i} - \\bar{R_j}| \\geq q(\\alpha, k, df)\\sqrt{\\frac{S^2_R}{2}(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Tukey's HSD vs Fisher's LSD HSD: \\(H_0\\) : All pairs are the same LSD: \\(H_0\\) : All pairs are the same, \\(\\mu_1 = \\mu_2 = \\cdots\\) . The \\(H_0\\) for LSD spans over a greater space. So the Type I error of LSD will be smaller(?) Two-step HSD: Evaluate the significance of the one-way ANOVA before proceeding to HSD. Replace \\(q(\\alpha, k, df)\\) with \\(q(\\alpha, k-1, df)\\) . Multiple comparison permutation tests Bonferroni Permutation Tests: Perform 2-sample permutation tests (e.g. permutation \\(t\\) -test or Wilcoxon rank sum test) on all pairs of treatments and compare the permutation \\(p\\) -value for each pair with the significance level \\(\\alpha' = \\frac{\\alpha}{k(k-1)/2}\\) Note that when comparing treatment \\(i\\) to treatment \\(j\\) , should re-rank from \\([1, n_i+n_j]\\) . If the \\(p\\) -value for one pair is less than \\(\\alpha'\\) , declare significant difference between this pair of treatments. Fisher's Protected LSD Permutation Tests: Perform a permutation test to test if there is any difference among the \\(k\\) treatments at significance level \\(\\alpha\\) . Obtain all (or a random sample) of permutations of the data from each pair of treatments. For each permutation, compute \\(T_{ij}\\) for each \\(ij\\) pair. \\(p\\) -value for comparing treatment \\(i, j\\) is the fraction of permutations for which \\(|T_{ij}| \\geq T_{ij, obs}\\) Tukey's HSD Permutation test Obtain all (or a random samplle) of permutation of the data as in the permutation F-test. For each permutation, compute Q. Declare {ij} to e significantly different if \\(Q_{obs} \\geq q*(\\alpha)\\) Ordered alternatives: JT test \\(H_1: F_1(x) \\geq F_2(x) \\geq \\cdots \\geq F_k(x)\\) JT statistic: \\(T=\\sum_{i \\le j} T_{ij}\\) , where \\(T_{ij}\\) is Mann-Whitney statistic. Larger \\(T\\) suppors \\(H1\\) . JT test: Permutation test based on JT statistic: Compute \\(JT_{obs}\\) based on the observed data Find all (or a random sample) of the permutations of the data and compute \\(JT\\) for each permutation Compute the \\(p\\) -value as the fraction of the permutations for which \\(JT \\geq JT_{obs}\\) Large sample approximation Use the expected value and variance of the Mann-Whitney statistic... TODO","title":"Chapter 3: k-Sample methods"},{"location":"stat/3620/ch3.html#chapter-3-k-sample-methods","text":"When there are more than two treatments: Do an overall comparison to determine whether or not there are differences among the \\(k\\) samples If the differences are found, we perform multiple comparisons to further detect which treatments differ from each other. Test between \\(H_0: F_1(x) = F_2(x) = \\cdots = F_k(x)\\) for all \\(x\\) , and \\(H_1: F_i(x) \\leq F_j(x)\\) or \\(F_i(x) \\geq F_j(x)\\) for at least one pair with strict inequality holding for at least one \\(x\\) . Alternatively, let \\(F_i(x) = F(x - \\mu_i)\\) . Then the hypothesis become \\(H_0: \\mu_1(x) = \\mu_2(x) = \\cdots = \\mu_k(x)\\) for all \\(x\\) , and \\(H_1: \\mu_i\\) are not all equal","title":"Chapter 3: k-Sample methods"},{"location":"stat/3620/ch3.html#parametric-method-based-on-normality-assumption","text":"\\(X_{ij} = \\mu_{i} + \\epsilon_{ij}\\) \\(SST = SSB + SSE\\) F-test: \\(F = \\frac{SSB/(k-1)}{SSE/(N-k)} \\sim F(k-1, N-k)\\) under \\(H_0\\) , large \\(F\\) values favor the rejection of \\(H_0\\) . However, if we are unwilling to assume that the population distributions are normal, we may carry out a permutation version of F-test instead.","title":"Parametric method based on Normality Assumption"},{"location":"stat/3620/ch3.html#permutation-f-test","text":"Compute the F-statistic for the observed data, denoted \\(F_{obs}\\) . For each of the \\(\\frac{N!}{n_1!n_2! \\cdots n_k!}\\) permutations (or a random sample of the permutations), compute the F-statistic Calculate the p-value as the fraction of the F's that are greater than or equal to \\(F_{obs}\\) . Can also perform permutation test based on \\(SSB = \\sum^k_{i=1}n_i(\\bar{X_i} - \\bar{X})^2\\) or \\(SSX = \\sum^k_{i=1} n_i \\bar{X_i}^2\\) as the test statistic. Note Note that for large samples, the permutation distribution may be approximated by the F-distribution. In other words, the ANOVA test may well approximate the permutation F-test. This is easy to conprehend since sample means will be normally distributed for large samples, according to the Central Limit Theorem. Warning In practice, the ANOVA test is relatively robust to departures from the normality assumption but it is sensitive to departures from the constant variance assumption . link Example 3.1 Three preservatives were compared to see their ability to inhibit the growth of bacteria. Samples were treated with one of the preservatives at the same time or left untreated as a control. After 48 hours, bacteria counts were made. The logarithms of the counts (Why?) were computed to meet ANOVA assumptions, recorded as the table: ctrl = c ( 4.302 , 4.017 , 4.049 , 4.176 ) pre1 = c ( 2.021 , 3.190 , 3.250 , 3.276 , 3.292 , 3.267 ) pre2 = c ( 3.397 , 3.552 , 3.630 , 3.578 , 3.612 ) pre3 = c ( 2.699 , 2.929 , 2.785 , 2.176 , 2.845 , 2.913 ) x = c ( ctrl , pre1 , pre2 , pre3 ) grps = rep ( c ( \"ctrl\" , \"pre1\" , \"pre2\" , \"pre3\" ), c ( 4 , 6 , 5 , 6 )) # grps = rep(1:4, c(4,6,5,6)) dat <- data.frame ( x , grps ) aggregate ( x ~ grps , dat , mean ) # compute group means # one-way ANOVA model onewayanova <- ( aov ( x ~ factor ( grps ))) summary ( onewayanova ) str ( summary ( onewayanova )) Fobs <- summary ( onewayanova )[[ 1 ]][ 1 , 4 ] Fobs # Permutation-F test R <- 9999 reps <- numeric ( R ) for ( i in 1 : R ) { permdat <- data.frame ( x = dat $ x , grps = sample ( dat $ grps , size = dim ( dat )[ 1 ], replace = FALSE )) permanova <- aov ( x ~ factor ( grps ), data = permdat ) reps [ i ] <- summary ( permanova )[[ 1 ]][ 1 , 4 ] # extract the ith permuted F statistic } pvalue <- mean ( c ( Fobs , reps ) >= Fobs ) # always an upper-tailed test pvalue hist ( c ( Fobs , reps ), main = \"Null distribution of F statistic\" ) abline ( v = Fobs , lty = 2 , col = \"red\" ) > # one-way ANOVA model > onewayanova <- (aov(x ~ factor(grps))) > summary(onewayanova) Df Sum Sq Mean Sq F value Pr(>F) factor(grps) 3 5.476 1.8254 17.66 1.82e-05 *** Residuals 17 1.757 0.1034 \u2013- Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1 > > str(summary(onewayanova)) List of 1 $ :Classes \u2018anova\u2019 and \u2019data.frame\u2019: 2 obs. of 5 variables: ..$ Df : num [1:2] 3 17 ..$ Sum Sq : num [1:2] 5.48 1.76 ..$ Mean Sq: num [1:2] 1.825 0.103 ..$ F value: num [1:2] 17.7 NA ..$ Pr(>F) : num [1:2] 1.82e-05 NA - attr(*, \"class\")= chr [1:2] \"summary.aov\" \"listof\" > Fobs <- summary(onewayanova)[[1]][1,4] > Fobs [1] 17.65729 One-way ANOVA has F-statistic = 17:66 with p-value = 0:000018 while the permutation F-test based on R = 10000 random permutations gives the exact p-value = 0:0001 (is this OK?). Close agreement is found between the two p-values. Note that for large samples, the permutation distribution may be approximated by the F-distribution. In other words, the ANOVA test may well approximate the permutation F-test. This is easy to conprehend since sample means will be normally distributed for large samples, according to the Central Limit Theorem.","title":"Permutation F-test"},{"location":"stat/3620/ch3.html#kruskal-wallis-test-anova-based-on-ranks","text":"Objective Obtaining a statistic based on ranks , similar to Wilcoxcon test. Approximating permutation test using chi-square, useful for large sample size. Rank the data combining all treatments. Let \\(\\bar{R_i}\\) denote the mean of the ranks of treatment \\(i\\) 's observations, assuming the data have no ties. The Kruskal-Wallis statistic is defined as: \\[ KW = \\frac{12}{N(N+1)}\\sum^k_{i=1}n_i\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2 = \\frac{12}{N(N+1)} \\sum^k_{i=1} n_i (\\bar{R_i})^2 - 3(N+1) \\] where \\(\\frac{N+1}{2}\\) is the overall mean of all ranks and \\(\\sum^k_{i=1}n_i\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2\\) can be viewed as the \\(SSB\\) on ranks. (recall \\(SSB = \\sum^k_{i=1}n_i(\\bar{X_i} - \\bar{X})^2\\) ) \\(\\frac{12}{N(N+1)}\\) is a constant scaling factor to make the \\(KW\\) statistic to follow approximately a chi-square distribution under \\(H_0\\) , i.e. \\(KW \\sim \\chi^2(k-1)\\) . (note that \\(E[\\sum^k_{i=1}n_i\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2] = \\frac{N(N+1)}{12}(k-1)\\) , we want \\(E[KW] = k-1\\) . \\(E[\\left(\\bar{R_i}-\\frac{N+1}{2}\\right)^2] = Var(\\bar{R_i}) + (E[\\bar{R_i}] - \\frac{N+1}{2})^2 = \\frac{1}{n_i^2}\\frac{n_i(N-n_i)\\sigma^2}{N-1}\\) Can get the result from p.20 Expectation and Variance of Wilcoxon statistic. But here we are using average rank instead of rank sum, so divide by \\({n_i^2}\\) \\(\\sigma = \\frac{(N-1)(N+1)}{12}\\) The \"exact\" p-value of the \\(KW\\) test can be obtained by using the permutation method. Reject when the upper \\(\\alpha\\) % critical value from the chi-square table is less than \\(KW\\) . Note The permutation \\(KW\\) test tends to have smaller critical values than the chi-square critical values. Hence, if we use the chi-square approximation, rejection of the null hypothesis will assure the same result with the exact \\(KW\\) test, and the exact p-value will be smaller. Example 3.2 Continue to use the data from example 3.1, but this time the sample size \\(N = 21\\) is moderately large. \\(KW = \\cdots = 17.14\\) , \\(KW \\sim \\chi^2(3)\\) under \\(H_0\\) , the upper 1% critical value from the chi-square table is \\(11.34\\) . Thus the test is significant at the 1% level and \\(H_0\\) should be rejected. ctrl = c ( 4.302 , 4.017 , 4.049 , 4.176 ) pre1 = c ( 2.021 , 3.190 , 3.250 , 3.276 , 3.292 , 3.267 ) pre2 = c ( 3.397 , 3.552 , 3.630 , 3.578 , 3.612 ) pre3 = c ( 2.699 , 2.929 , 2.785 , 2.176 , 2.845 , 2.913 ) x = c ( ctrl , pre1 , pre2 , pre3 ) grps = rep ( c ( \"ctrl\" , \"pre1\" , \"pre2\" , \"pre3\" ), c ( 4 , 6 , 5 , 6 )) # grps = rep(1:4, c(4,6,5,6)) dat <- data.frame ( x , grps ) aggregate ( x ~ grps , dat , mean ) # compute group means # K-W test kruskal.test ( x ~ grps , data = dat ) # Alternative way of computing the K-W test statistic rank.x = rank ( x ) summary ( aov ( rank.x ~ factor ( grps ))) SSB = summary ( aov ( rank.x ~ factor ( grps )))[[ 1 ]][ 1 , 2 ] SR2 = var ( rank.x ) KW = SSB / SR2 # Exact p-value using permutation test R <- 9999 reps <- numeric ( R ) for ( i in 1 : R ) { permdat <- data.frame ( x = rank.x , grps = sample ( dat $ grps , size = dim ( dat )[ 1 ], replace = FALSE )) permkw <- aov ( x ~ factor ( grps ), data = permdat ) reps [ i ] <- summary ( permkw )[[ 1 ]][ 1 , 2 ] / SR2 } pvalue <- mean ( c ( KW , reps ) >= KW ) # always an upper-tailed test pvalue hist ( c ( KW , reps ), main = \"Null distribution of KW statistic\" ) abline ( v = KW , lty = 2 , col = \"red\" ) Adjustment for ties When data are tied, we adjust the ranks using the mid-ranks for tied data as we did for the Wilcoxon test. The permutation method can also be applied to the \\(KW\\) statistic. However, in order to maintain the chi-square approximation, the \\(KW\\) statistic should be modified: \\[ KW_{ties} = \\frac{KW}{1-\\frac{\\sum^g_{i=1}{(t_i^3-t_i)}}{N^3-N}} \\] Code If no ties, use kruskal.test function in the base R installation. If ties exist, use kruskal_test function in the R add-on package coin .","title":"Kruskal-Wallis Test (ANOVA based on ranks)"},{"location":"stat/3620/ch3.html#multiple-comparisons","text":"We want more details of the differences to know the location of the differences. Either we do 2-sample tests for each pair, but this will induce higher type I error. (If independent, then \\(1-0.95^3\\) ) Aim: Control the type-I error while performing pairwise tests","title":"Multiple comparisons"},{"location":"stat/3620/ch3.html#bonferroni-adjustment-union-bound","text":"Use \\(\\alpha_i = \\frac{\\alpha}{k(k-1)/2}\\) . Bonferroni inequality (union bound): \\(P(E_1 \\cup \\cdots \\cup E_m) \\leq P(E_1) + \\cdots + P(E_m)\\) Too conservative, high rate of type II error (false negative) Very wide confidence interval","title":"Bonferroni Adjustment (union bound)"},{"location":"stat/3620/ch3.html#fishers-protected-least-significant-difference-lsd-use-mse-instead-of-two-sample-error-in-t-test","text":"Perform F-test for equality of means (ANOVA). If result of F-test is to accept H0 (all same), then stop. Perform all pairwise t-tests at significance level \\(\\alpha\\) . \\[ |\\bar{X_i} - \\bar{X_j}| \\geq t_{\\alpha/2, N-k} \\sqrt{MSE(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Can use \\(\\alpha/2\\) significance level But type I error may be greater than \\(\\alpha\\) Why use t? Difference follows normal distribution MSE is chi squared. \\(\\frac{Z}{\\sqrt{\\chi^2/n}} \\tilde t_n\\)","title":"Fisher's Protected Least Significant Difference (LSD) (use MSE instead of two-sample error in t-test)"},{"location":"stat/3620/ch3.html#rank-based-fishers-lsd-procedure-for-large-samples","text":"Benefit of using ranks: Less sensitive to outliers Perform KW-test for equality Stop if result is to accept H0 Perform all pariwise \\(z\\) -tests at significance level \\(\\alpha\\) . \\[ |\\bar{R_i} - \\bar{R_j}| \\geq z_{\\alpha/2} \\sqrt{S_R^2(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Can use normal critical value because \\(S^2_R = \\frac{N(N+1)}{12}\\) or \\(\\frac{N(N+1)}{12} - \\frac{\\sum^g_{i=1}(t_i^3-t_i)}{12(N-1)}\\) (if ties exist) \\(Var(\\bar{R_i} - \\bar{R_j}) = S_R^2(\\frac{1}{n_i} + \\frac{1}{n_j})\\) Why use normal instead of t? \\(S_R^2\\) is constant (variance of ranks is constant) The only random thing is \\(|\\bar{R_i} - \\bar{R_j}|\\) , which is normal (large sample approximation).","title":"Rank-based Fisher's LSD Procedure for Large Samples"},{"location":"stat/3620/ch3.html#tukeys-honesty-significant-difference-hsd-largest-difference-between-sample-means-among-all-pairs","text":"Suppose populations are normally distributed and sample sizes are equal. \\[ Q = \\max_{i=j}\\frac{\\sqrt{n}|\\bar{X_i} - \\bar{X_j}|}{\\sqrt{MSE}} \\] MSE is for combined sample. \\[ |\\bar{X_i} - \\bar{X_j}| \\geq q(\\alpha, k, df)\\sqrt{\\frac{MSE}{n}}, df = N-k \\] Look up \\(q\\) -distribution table Guarntee experiment-wise error rate to be exactly \\(\\alpha\\) . Unequal sample size: Tukey-Kramer procedure \\[ |\\bar{X_i} - \\bar{X_j}| \\geq q(\\alpha, k, df)\\sqrt{\\frac{MSE}{2}(\\frac{1}{n_i} + \\frac{1}{n_j})} \\] Conservative, error rate \\(\\leq \\alpha\\) .","title":"Tukey's Honesty Significant Difference (HSD) (largest difference between sample means among all pairs)"},{"location":"stat/3620/ch3.html#rank-based-tukeys-hsd-procedure-for-large-samples","text":"\\[ |\\bar{R_i} - \\bar{R_j}| \\geq q(\\alpha, k, \\infty)\\sqrt{\\frac{S^2_R}{n}} \\] \\(df = \\infty\\) because of large samples. Unequal sample size \\[ |\\bar{R_i} - \\bar{R_j}| \\geq q(\\alpha, k, df)\\sqrt{\\frac{S^2_R}{2}(\\frac{1}{n_i} + \\frac{1}{n_j})} \\]","title":"Rank-based Tukey's HSD Procedure for Large Samples"},{"location":"stat/3620/ch3.html#tukeys-hsd-vs-fishers-lsd","text":"HSD: \\(H_0\\) : All pairs are the same LSD: \\(H_0\\) : All pairs are the same, \\(\\mu_1 = \\mu_2 = \\cdots\\) . The \\(H_0\\) for LSD spans over a greater space. So the Type I error of LSD will be smaller(?) Two-step HSD: Evaluate the significance of the one-way ANOVA before proceeding to HSD. Replace \\(q(\\alpha, k, df)\\) with \\(q(\\alpha, k-1, df)\\) .","title":"Tukey's HSD vs Fisher's LSD"},{"location":"stat/3620/ch3.html#multiple-comparison-permutation-tests","text":"Bonferroni Permutation Tests: Perform 2-sample permutation tests (e.g. permutation \\(t\\) -test or Wilcoxon rank sum test) on all pairs of treatments and compare the permutation \\(p\\) -value for each pair with the significance level \\(\\alpha' = \\frac{\\alpha}{k(k-1)/2}\\) Note that when comparing treatment \\(i\\) to treatment \\(j\\) , should re-rank from \\([1, n_i+n_j]\\) . If the \\(p\\) -value for one pair is less than \\(\\alpha'\\) , declare significant difference between this pair of treatments. Fisher's Protected LSD Permutation Tests: Perform a permutation test to test if there is any difference among the \\(k\\) treatments at significance level \\(\\alpha\\) . Obtain all (or a random sample) of permutations of the data from each pair of treatments. For each permutation, compute \\(T_{ij}\\) for each \\(ij\\) pair. \\(p\\) -value for comparing treatment \\(i, j\\) is the fraction of permutations for which \\(|T_{ij}| \\geq T_{ij, obs}\\) Tukey's HSD Permutation test Obtain all (or a random samplle) of permutation of the data as in the permutation F-test. For each permutation, compute Q. Declare {ij} to e significantly different if \\(Q_{obs} \\geq q*(\\alpha)\\)","title":"Multiple comparison permutation tests"},{"location":"stat/3620/ch3.html#ordered-alternatives-jt-test","text":"\\(H_1: F_1(x) \\geq F_2(x) \\geq \\cdots \\geq F_k(x)\\) JT statistic: \\(T=\\sum_{i \\le j} T_{ij}\\) , where \\(T_{ij}\\) is Mann-Whitney statistic. Larger \\(T\\) suppors \\(H1\\) . JT test: Permutation test based on JT statistic: Compute \\(JT_{obs}\\) based on the observed data Find all (or a random sample) of the permutations of the data and compute \\(JT\\) for each permutation Compute the \\(p\\) -value as the fraction of the permutations for which \\(JT \\geq JT_{obs}\\)","title":"Ordered alternatives: JT test"},{"location":"stat/3620/ch3.html#large-sample-approximation","text":"Use the expected value and variance of the Mann-Whitney statistic... TODO","title":"Large sample approximation"},{"location":"stat/3620/ch4.html","text":"Chapter 4: Paired Comparisons and Block Designs Paired comparisons Paired data can be found in the following situations: Two measurements obtained from the same subject under two different conditions Two measurements obtained from matched pairs Parametric method: Paired t-test Perform \\(t\\) -test on the difference data \\(X_i - Y_i\\) . \\(t=\\frac{\\bar{D}}{S_D/\\sqrt{n}}\\) Paired comparison permutation test Compute the sample mean of differences \\(D_i = X_i - Y_i\\) for each pair of observed data, denoted by \\(\\bar{D_{obs}}\\) . Obtain the \\(2^n\\) possible assignments (or a random sample of \\(R\\) assignments) of plus and minus signs to the \\(|D_i|\\) 's. Compute \\(\\bar{D}\\) for each possible assignment Calculate the \\(p\\) -value as the fraction of \\(\\bar{D} \\geq \\bar{D_{obs}}\\) Binomial (sign) test for paired samples Apply binomial tet to the difference data \\(D_i\\) , test if population median of \\(D=0\\) . (refer to Chapter 1) Issue: Discard a lot of information of the data, only take into acount the direction of the difference, but not the magnitude. Wilcoxon signed-rank test for paired samples Rank the absolute values of the differences \\(|D_i|\\) Re-attach the rank of \\(|D_i|\\) by the sign ( \\(+, 0, -\\) ) of \\(D_i\\) Apply the permutation test on the sum of positive signed ranks, \\(SR_+\\) or the average of signed ranks \\(\\bar{SR}\\) Assumption: \\(D_i\\) are iid from a continuous cdf \\(F\\) with median \\(\\theta\\) , and its pdf \\(f\\) is symmetric about \\(\\theta\\) . Signed-rank test more powerful in asymmetric case. Can also use this tet to test for the median of a symmetric distribution based on a single sample. Example x <- c ( 20 , 18 , 24 , 14 , 5 , 26 , 15 , 29 , 15 , 9 , 25 , 31 , 35 , 12 ) y <- c ( 40 , 25 , 38 , 27 , 31 , 21 , 32 , 38 , 25 , 18 , 32 , 28 , 33 , 29 ) d = x - y # Compute the signed ranks SR <- { rank ( abs ( d )) * sign ( d )} SRpos <- sum ( SR [ SR > 0 ]) barSR.obs = mean ( SR ) # average signed-rank statistic # Permutation test based on average signed-rank n <- length ( SR ) junk <- matrix ( NA , 2 ^ n , n ) for ( i in 1 : ( 2 ^ n )) { remain <- i -1 for ( j in 1 : n ) { junk [ i , j ] <- remain %/% ( 2 ^ ( n - j )) #%/%: integer division remain <- remain %% ( 2 ^ ( n - j )) #%%: remainder } } signs <- junk - ! junk ### permuted signs (+/-) perm <- apply ( t ( abs ( SR ) * t ( signs )) , 1 , mean ) pval.twosided = mean ( abs ( perm ) >= abs ( barSR.obs )) pval.upper = mean ( perm >= barSR.obs ) pval.lower = mean ( perm <= barSR.obs ) # pval.twosided = 2*min(pval.upper, pval.lower) Large sample approximation of Wilcoxon signed-rank statistic SR+ Under H0, indicator of sign of \\(D\\) follows \\(Ber(0.5)\\) . Then \\(E(SR_+) = \\frac{1}{2} \\sum^n_{i=1} R_i\\) , \\(Var(SR_+) = \\frac{1}{4} \\sum^n_{i=1} R_i^2\\) , \\(\\frac{SR_+ - E(SR_+)}{\\sqrt{Var(SR_+)}} \\sim N(0, 1)\\) In case of ties, use average rank of tied observations. Example x <- c ( 20 , 18 , 24 , 14 , 5 , 26 , 15 , 29 , 15 , 9 , 25 , 31 , 35 , 12 ) y <- c ( 40 , 25 , 38 , 27 , 31 , 21 , 32 , 38 , 25 , 18 , 32 , 28 , 33 , 29 ) d = x - y # Wilcoxon signed-rank test using sum of positive signed ranks wilcox.test ( d , correct = FALSE ) wilcox.test ( x , y , paired = T , correct = FALSE ) wilcox.test ( x , y , paired = T , correct = TRUE ) Randomized Complete Block Design (RBCD) Apply different treatment to each person in each block. Each block contains similar people. \\(X_{ij} = \\mu + t_i + b_j + \\epsilon_{ij}\\) Test between \\(H_0: t_1 = \\cdots = t_k\\) vs \\(H_1\\) : Not all \\(t_i\\) are the same. When \\(\\epsilon \\sim N(\\mu, \\sigma^2)\\) , we can carry out the \\(F\\) -test using the \\(F\\) statistic: \\(F = \\frac{MSB}{MSE} = \\frac{SSB/(k-1)}{SSE/[(k-1)(b-1)]}\\) Permutation F-test for RCB designs Compute the F statistic \\(F_{obs}\\) using the observed data Permute the observations within each block. There are \\((k!)^b\\) permutations. Compute the F statistic for each possible permutation. Calculate the p-value as the fraction of \\(F\\) 's \\(geq\\) \\(F_{obs}\\) Alternatively, we can also use SSB or SSX for the test statistic. Friedman's test for RCB design (SSB on ranks) Test on SSB on ranks. rank within each block. \\[ FM = \\frac{12b}{k(k+1)} \\sum^k_{i=1}(\\bar{R_{i+} - \\frac{k+1}{2}})^2 \\] For large samples, \\(FM \\to \\chi^2(k-1)\\) under \\(H_0\\) . For \\(k=2\\) , Friedman test is equivalent to two-sided Wilcoxon signed rank test. (pair is also a block) Can obtain exact p-value from permutation method. Adjustment for ties \\[ FM_{ties} = \\frac{b}{\\frac{1}{b}\\sum^b_{j=1}S^2_{Bj}} \\sum^k_{i=1}\\left(\\bar{R_{i+}} - \\frac{k+1}{2}\\right)^2 \\] Multiple comparisons based on Friedman Statistic for Large Samples (Re: HSD) \\[ |\\bar{R_{i+}} - \\bar{R_{j+}}| \\geq q(\\alpha, k, \\infty)\\sqrt{\\frac{\\frac{1}{b}\\sum^b_{j=1}S^2_{Bj}}{b}} \\] Page\u2019s Test for Ordered Alternatives for RCB Designs Let \\(R_{i+} = b\\bar{R_{i+}}\\) be the sum ranks for the \\(i\\) 'th treatment. \\[ PG = \\sum^k_{i=1} iR_{i+} \\] Amplify the large rank sums. For \\(H_1: t_1 \\leq \\cdots t_k\\) , \\(PG\\) tends to be large. For \\(H_1: t_1 \\geq \\cdots t_k\\) , \\(PG\\) tends to be small. Large sample approximation: \\(E(PG) = \\frac{bk(k+1)^2}{4}, Var(PG) = \\frac{k(k^2-1)}{12}\\sum^b_{j=1}S^2_{Bj}\\) Page's test will have greater power to detect difference than Friedman's test if the ordering among treatments is correct. Example x = c ( 120 , 208 , 199 , 194 , 177 , 195 , 207 , 188 , 181 , 164 , 155 , 175 , 122 , 137 , 177 , 177 , 160 , 138 , 128 , 128 , 160 , 142 , 157 , 179 ) blocks = rep ( 1 : 6 , 4 ) grps = rep ( 1 : 4 , each = 6 ) k <- length ( table ( grps )) # obtain the within-block ranks mat <- cbind ( x , blocks , 1 : length ( x )) mat <- mat [ order ( mat [, 2 ]),] b <- length ( table ( blocks )) n <- length ( x ) for ( j in 1 : b ) mat [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] <- rank ( mat [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] ) mat <- mat [ order ( mat [, 3 ]),] Rij = mat [, 1 ] # compute within-block rank sums junk <- table ( grps ) ranksumvec <- rep ( NA , length ( junk )) for ( i in 1 : length ( junk )) ranksumvec [ i ] <- sum ( Rij [ grps == names ( junk )[ i ]]) ranksumvec # compute observed Page's statistic Pageobs <- sum (( 1 : k ) * ranksumvec ) # Permutation method based on Page's statistic set.seed ( 1234567 ) R = 10000 mat <- cbind ( x , grps , blocks ) mat <- mat [ order ( mat [, 3 ]),] # sort matrix rows by block k <- length ( table ( grps )) b <- length ( table ( blocks )) permpage <- rep ( NA , R ) for ( i in 1 : R ) { junk <- rep ( NA , b * k ) for ( j in 1 : b ) junk [ k * ( j -1 ) + ( 1 : k )] <- sample ( 1 : k , k ) mat1 <- cbind ( mat [, 1 ], blocks , 1 : length ( mat [, 1 ])) mat1 <- mat1 [ order ( mat1 [, 2 ]),] b <- length ( table ( blocks )) n <- length ( x ) for ( j in 1 : b ) mat1 [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] <- rank ( mat1 [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] ) mat1 <- mat1 [ order ( mat1 [, 3 ]),] Rij = mat1 [, 1 ] temp <- table ( junk ) sumvec <- rep ( NA , length ( temp )) for ( j in 1 : length ( temp )) sumvec [ j ] <- sum ( Rij [ junk == names ( temp )[ j ]]) permpage [ i ] <- sum (( 1 : k ) * sumvec ) } # tests if means are arranged t1 > t2 > t3 > t4, i.e. response decreases as changing trt from 1 to 4 pvalue = mean ( permpage <= Pageobs ) pvalue # correlation of treatment i with its rank sum R_i+ cor ( 1 : 4 , ranksumvec ) Special cases of Friedman's test Cochran's Q Test (Friedman's test with ties for 0/1 responses) Equivalent to Friedman's test. Example x = c ( 1 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 1 ) blocks = rep ( 1 : 10 , 4 ) grps = rep ( 1 : 4 , rep ( 10 , 4 )) # Friedman rank sum test based on chi-square approximation friedman.test ( x , grps , blocks ) Kendall's W Test (Test for agreements among judges) Agreement: Judges are the blocks , interns are the treatments . In other words, the interns can be distinguished well. Example x = c ( 5.5 , 7.0 , 8.5 , 5.0 , 6.0 , 6.0 , 6.0 , 8.0 , 9.5 , 4.5 , 7.5 , 8.0 , 9.0 , 9.5 , 9.0 ) grps = rep ( 1 : 5 , each = 3 ) blocks = rep ( 1 : 3 , 5 ) # Kendall's W test friedman.test ( x , grps , blocks )","title":"Chapter 4: Paired Comparisons and Block Designs"},{"location":"stat/3620/ch4.html#chapter-4-paired-comparisons-and-block-designs","text":"","title":"Chapter 4: Paired Comparisons and Block Designs"},{"location":"stat/3620/ch4.html#paired-comparisons","text":"Paired data can be found in the following situations: Two measurements obtained from the same subject under two different conditions Two measurements obtained from matched pairs","title":"Paired comparisons"},{"location":"stat/3620/ch4.html#parametric-method-paired-t-test","text":"Perform \\(t\\) -test on the difference data \\(X_i - Y_i\\) . \\(t=\\frac{\\bar{D}}{S_D/\\sqrt{n}}\\)","title":"Parametric method: Paired t-test"},{"location":"stat/3620/ch4.html#paired-comparison-permutation-test","text":"Compute the sample mean of differences \\(D_i = X_i - Y_i\\) for each pair of observed data, denoted by \\(\\bar{D_{obs}}\\) . Obtain the \\(2^n\\) possible assignments (or a random sample of \\(R\\) assignments) of plus and minus signs to the \\(|D_i|\\) 's. Compute \\(\\bar{D}\\) for each possible assignment Calculate the \\(p\\) -value as the fraction of \\(\\bar{D} \\geq \\bar{D_{obs}}\\)","title":"Paired comparison permutation test"},{"location":"stat/3620/ch4.html#binomial-sign-test-for-paired-samples","text":"Apply binomial tet to the difference data \\(D_i\\) , test if population median of \\(D=0\\) . (refer to Chapter 1) Issue: Discard a lot of information of the data, only take into acount the direction of the difference, but not the magnitude.","title":"Binomial (sign) test for paired samples"},{"location":"stat/3620/ch4.html#wilcoxon-signed-rank-test-for-paired-samples","text":"Rank the absolute values of the differences \\(|D_i|\\) Re-attach the rank of \\(|D_i|\\) by the sign ( \\(+, 0, -\\) ) of \\(D_i\\) Apply the permutation test on the sum of positive signed ranks, \\(SR_+\\) or the average of signed ranks \\(\\bar{SR}\\) Assumption: \\(D_i\\) are iid from a continuous cdf \\(F\\) with median \\(\\theta\\) , and its pdf \\(f\\) is symmetric about \\(\\theta\\) . Signed-rank test more powerful in asymmetric case. Can also use this tet to test for the median of a symmetric distribution based on a single sample. Example x <- c ( 20 , 18 , 24 , 14 , 5 , 26 , 15 , 29 , 15 , 9 , 25 , 31 , 35 , 12 ) y <- c ( 40 , 25 , 38 , 27 , 31 , 21 , 32 , 38 , 25 , 18 , 32 , 28 , 33 , 29 ) d = x - y # Compute the signed ranks SR <- { rank ( abs ( d )) * sign ( d )} SRpos <- sum ( SR [ SR > 0 ]) barSR.obs = mean ( SR ) # average signed-rank statistic # Permutation test based on average signed-rank n <- length ( SR ) junk <- matrix ( NA , 2 ^ n , n ) for ( i in 1 : ( 2 ^ n )) { remain <- i -1 for ( j in 1 : n ) { junk [ i , j ] <- remain %/% ( 2 ^ ( n - j )) #%/%: integer division remain <- remain %% ( 2 ^ ( n - j )) #%%: remainder } } signs <- junk - ! junk ### permuted signs (+/-) perm <- apply ( t ( abs ( SR ) * t ( signs )) , 1 , mean ) pval.twosided = mean ( abs ( perm ) >= abs ( barSR.obs )) pval.upper = mean ( perm >= barSR.obs ) pval.lower = mean ( perm <= barSR.obs ) # pval.twosided = 2*min(pval.upper, pval.lower)","title":"Wilcoxon signed-rank test for paired samples"},{"location":"stat/3620/ch4.html#large-sample-approximation-of-wilcoxon-signed-rank-statistic-sr","text":"Under H0, indicator of sign of \\(D\\) follows \\(Ber(0.5)\\) . Then \\(E(SR_+) = \\frac{1}{2} \\sum^n_{i=1} R_i\\) , \\(Var(SR_+) = \\frac{1}{4} \\sum^n_{i=1} R_i^2\\) , \\(\\frac{SR_+ - E(SR_+)}{\\sqrt{Var(SR_+)}} \\sim N(0, 1)\\) In case of ties, use average rank of tied observations. Example x <- c ( 20 , 18 , 24 , 14 , 5 , 26 , 15 , 29 , 15 , 9 , 25 , 31 , 35 , 12 ) y <- c ( 40 , 25 , 38 , 27 , 31 , 21 , 32 , 38 , 25 , 18 , 32 , 28 , 33 , 29 ) d = x - y # Wilcoxon signed-rank test using sum of positive signed ranks wilcox.test ( d , correct = FALSE ) wilcox.test ( x , y , paired = T , correct = FALSE ) wilcox.test ( x , y , paired = T , correct = TRUE )","title":"Large sample approximation of Wilcoxon signed-rank statistic SR+"},{"location":"stat/3620/ch4.html#randomized-complete-block-design-rbcd","text":"Apply different treatment to each person in each block. Each block contains similar people. \\(X_{ij} = \\mu + t_i + b_j + \\epsilon_{ij}\\) Test between \\(H_0: t_1 = \\cdots = t_k\\) vs \\(H_1\\) : Not all \\(t_i\\) are the same. When \\(\\epsilon \\sim N(\\mu, \\sigma^2)\\) , we can carry out the \\(F\\) -test using the \\(F\\) statistic: \\(F = \\frac{MSB}{MSE} = \\frac{SSB/(k-1)}{SSE/[(k-1)(b-1)]}\\)","title":"Randomized Complete Block Design (RBCD)"},{"location":"stat/3620/ch4.html#permutation-f-test-for-rcb-designs","text":"Compute the F statistic \\(F_{obs}\\) using the observed data Permute the observations within each block. There are \\((k!)^b\\) permutations. Compute the F statistic for each possible permutation. Calculate the p-value as the fraction of \\(F\\) 's \\(geq\\) \\(F_{obs}\\) Alternatively, we can also use SSB or SSX for the test statistic.","title":"Permutation F-test for RCB designs"},{"location":"stat/3620/ch4.html#friedmans-test-for-rcb-design-ssb-on-ranks","text":"Test on SSB on ranks. rank within each block. \\[ FM = \\frac{12b}{k(k+1)} \\sum^k_{i=1}(\\bar{R_{i+} - \\frac{k+1}{2}})^2 \\] For large samples, \\(FM \\to \\chi^2(k-1)\\) under \\(H_0\\) . For \\(k=2\\) , Friedman test is equivalent to two-sided Wilcoxon signed rank test. (pair is also a block) Can obtain exact p-value from permutation method. Adjustment for ties \\[ FM_{ties} = \\frac{b}{\\frac{1}{b}\\sum^b_{j=1}S^2_{Bj}} \\sum^k_{i=1}\\left(\\bar{R_{i+}} - \\frac{k+1}{2}\\right)^2 \\] Multiple comparisons based on Friedman Statistic for Large Samples (Re: HSD) \\[ |\\bar{R_{i+}} - \\bar{R_{j+}}| \\geq q(\\alpha, k, \\infty)\\sqrt{\\frac{\\frac{1}{b}\\sum^b_{j=1}S^2_{Bj}}{b}} \\]","title":"Friedman's test for RCB design (SSB on ranks)"},{"location":"stat/3620/ch4.html#pages-test-for-ordered-alternatives-for-rcb-designs","text":"Let \\(R_{i+} = b\\bar{R_{i+}}\\) be the sum ranks for the \\(i\\) 'th treatment. \\[ PG = \\sum^k_{i=1} iR_{i+} \\] Amplify the large rank sums. For \\(H_1: t_1 \\leq \\cdots t_k\\) , \\(PG\\) tends to be large. For \\(H_1: t_1 \\geq \\cdots t_k\\) , \\(PG\\) tends to be small. Large sample approximation: \\(E(PG) = \\frac{bk(k+1)^2}{4}, Var(PG) = \\frac{k(k^2-1)}{12}\\sum^b_{j=1}S^2_{Bj}\\) Page's test will have greater power to detect difference than Friedman's test if the ordering among treatments is correct. Example x = c ( 120 , 208 , 199 , 194 , 177 , 195 , 207 , 188 , 181 , 164 , 155 , 175 , 122 , 137 , 177 , 177 , 160 , 138 , 128 , 128 , 160 , 142 , 157 , 179 ) blocks = rep ( 1 : 6 , 4 ) grps = rep ( 1 : 4 , each = 6 ) k <- length ( table ( grps )) # obtain the within-block ranks mat <- cbind ( x , blocks , 1 : length ( x )) mat <- mat [ order ( mat [, 2 ]),] b <- length ( table ( blocks )) n <- length ( x ) for ( j in 1 : b ) mat [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] <- rank ( mat [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] ) mat <- mat [ order ( mat [, 3 ]),] Rij = mat [, 1 ] # compute within-block rank sums junk <- table ( grps ) ranksumvec <- rep ( NA , length ( junk )) for ( i in 1 : length ( junk )) ranksumvec [ i ] <- sum ( Rij [ grps == names ( junk )[ i ]]) ranksumvec # compute observed Page's statistic Pageobs <- sum (( 1 : k ) * ranksumvec ) # Permutation method based on Page's statistic set.seed ( 1234567 ) R = 10000 mat <- cbind ( x , grps , blocks ) mat <- mat [ order ( mat [, 3 ]),] # sort matrix rows by block k <- length ( table ( grps )) b <- length ( table ( blocks )) permpage <- rep ( NA , R ) for ( i in 1 : R ) { junk <- rep ( NA , b * k ) for ( j in 1 : b ) junk [ k * ( j -1 ) + ( 1 : k )] <- sample ( 1 : k , k ) mat1 <- cbind ( mat [, 1 ], blocks , 1 : length ( mat [, 1 ])) mat1 <- mat1 [ order ( mat1 [, 2 ]),] b <- length ( table ( blocks )) n <- length ( x ) for ( j in 1 : b ) mat1 [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] <- rank ( mat1 [ n / b * ( j -1 ) + ( 1 : ( n / b )), 1 ] ) mat1 <- mat1 [ order ( mat1 [, 3 ]),] Rij = mat1 [, 1 ] temp <- table ( junk ) sumvec <- rep ( NA , length ( temp )) for ( j in 1 : length ( temp )) sumvec [ j ] <- sum ( Rij [ junk == names ( temp )[ j ]]) permpage [ i ] <- sum (( 1 : k ) * sumvec ) } # tests if means are arranged t1 > t2 > t3 > t4, i.e. response decreases as changing trt from 1 to 4 pvalue = mean ( permpage <= Pageobs ) pvalue # correlation of treatment i with its rank sum R_i+ cor ( 1 : 4 , ranksumvec )","title":"Page\u2019s Test for Ordered Alternatives for RCB Designs"},{"location":"stat/3620/ch4.html#special-cases-of-friedmans-test","text":"","title":"Special cases of Friedman's test"},{"location":"stat/3620/ch4.html#cochrans-q-test-friedmans-test-with-ties-for-01-responses","text":"Equivalent to Friedman's test. Example x = c ( 1 , 0 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 1 ) blocks = rep ( 1 : 10 , 4 ) grps = rep ( 1 : 4 , rep ( 10 , 4 )) # Friedman rank sum test based on chi-square approximation friedman.test ( x , grps , blocks )","title":"Cochran's Q Test (Friedman's test with ties for 0/1 responses)"},{"location":"stat/3620/ch4.html#kendalls-w-test-test-for-agreements-among-judges","text":"Agreement: Judges are the blocks , interns are the treatments . In other words, the interns can be distinguished well. Example x = c ( 5.5 , 7.0 , 8.5 , 5.0 , 6.0 , 6.0 , 6.0 , 8.0 , 9.5 , 4.5 , 7.5 , 8.0 , 9.0 , 9.5 , 9.0 ) grps = rep ( 1 : 5 , each = 3 ) blocks = rep ( 1 : 3 , 5 ) # Kendall's W test friedman.test ( x , grps , blocks )","title":"Kendall's W Test (Test for agreements among judges)"},{"location":"stat/3620/ch5.html","text":"Chapter 5: Tests for Trends and Association Pearson relation coefficient Pearson product moment correlation \\(= \\pm 1\\) : Perfect linear relationship \\(Y = a+bX\\) Parametric test for zero correlation If \\((X_i, Y_i)\\) is a random sample froma bivariate normal distribution, the test statistic for the null hypothesis \\(H_0: \\rho=0\\) is \\[ t_{corr} = \\sqrt{\\frac{n-2}{1-r^2}}r \\sim t(n-2) \\] Proof comes from test for zero slope of \\(Y = \\alpha + \\beta X + \\epsilon\\) , the test statistic for correlation is the same as the test statistic for zero slope. \\(t_{corr} = t_{slope}, \\beta = 0\\) ( \\(\\rho\\) is monotonic function of \\(\\beta\\) ) (but the problem for testing zero slope is slightly different from the one for linear model, as linear model assume \\(X\\) is fixed, while here both \\(X\\) and \\(Y\\) are random) Permutation test for zero slope Calculate \\(\\hat{\\beta_{obs}}\\) using the observed data Keep the order of \\(X\\) unchanged, permute the order of \\(Y\\) observation. There are \\(n!\\) observations. Compute \\(\\hat{\\beta}\\) for each possible permutation Find p-value Large sample approximation for \\(r\\) \\[ Z = \\frac{r}{\\sqrt{\\frac{1}{n-1}}} = \\sqrt{n-1}r \\sim N(0, 1) \\] Spearman Rank Correlation Issue: Pearson's correlation coefficient can only capture the linear relationship. Solution: Consider the correlation of the ranking data instead. Spearman's rank correlation is obtained by applying the Pearson correlation to the pairs \\((R(X_i), R(Y_i))\\) . When there is no ties in ranking, \\(r_s\\) can be simplified in terms of the differences \\(D_i = R(X_i) - R(Y_i)\\) : \\(r_s = 1 - \\frac{6\\sum^n_{i=1}D_i^2}{n(n^2-1)}\\) Large sample approximation for \\(r_s\\) \\[ Z = \\sqrt{n-1}r_s \\sim N(0, 1) \\] Kendall's Tau Concordant: \\((X_i - X_j)(Y_i - Y_j) > 0\\) Discordant: \\((X_i - X_j)(Y_i - Y_j) < 0\\) Kendall's tau : \\(\\tau = p_c = p_d\\) \\[ r_\\tau = \\frac{\\sum\\sum_{i<j} sgn[(X_i - X_j)(Y_i - Y_j)]}{{n\\choose 2}} \\] Large sample approximation TODO Contingency table Chi-square test","title":"Chapter 5: Tests for Trends and Association"},{"location":"stat/3620/ch5.html#chapter-5-tests-for-trends-and-association","text":"","title":"Chapter 5: Tests for Trends and Association"},{"location":"stat/3620/ch5.html#pearson-relation-coefficient","text":"Pearson product moment correlation \\(= \\pm 1\\) : Perfect linear relationship \\(Y = a+bX\\)","title":"Pearson relation coefficient"},{"location":"stat/3620/ch5.html#parametric-test-for-zero-correlation","text":"If \\((X_i, Y_i)\\) is a random sample froma bivariate normal distribution, the test statistic for the null hypothesis \\(H_0: \\rho=0\\) is \\[ t_{corr} = \\sqrt{\\frac{n-2}{1-r^2}}r \\sim t(n-2) \\] Proof comes from test for zero slope of \\(Y = \\alpha + \\beta X + \\epsilon\\) , the test statistic for correlation is the same as the test statistic for zero slope. \\(t_{corr} = t_{slope}, \\beta = 0\\) ( \\(\\rho\\) is monotonic function of \\(\\beta\\) ) (but the problem for testing zero slope is slightly different from the one for linear model, as linear model assume \\(X\\) is fixed, while here both \\(X\\) and \\(Y\\) are random)","title":"Parametric test for zero correlation"},{"location":"stat/3620/ch5.html#permutation-test-for-zero-slope","text":"Calculate \\(\\hat{\\beta_{obs}}\\) using the observed data Keep the order of \\(X\\) unchanged, permute the order of \\(Y\\) observation. There are \\(n!\\) observations. Compute \\(\\hat{\\beta}\\) for each possible permutation Find p-value Large sample approximation for \\(r\\) \\[ Z = \\frac{r}{\\sqrt{\\frac{1}{n-1}}} = \\sqrt{n-1}r \\sim N(0, 1) \\]","title":"Permutation test for zero slope"},{"location":"stat/3620/ch5.html#spearman-rank-correlation","text":"Issue: Pearson's correlation coefficient can only capture the linear relationship. Solution: Consider the correlation of the ranking data instead. Spearman's rank correlation is obtained by applying the Pearson correlation to the pairs \\((R(X_i), R(Y_i))\\) . When there is no ties in ranking, \\(r_s\\) can be simplified in terms of the differences \\(D_i = R(X_i) - R(Y_i)\\) : \\(r_s = 1 - \\frac{6\\sum^n_{i=1}D_i^2}{n(n^2-1)}\\) Large sample approximation for \\(r_s\\) \\[ Z = \\sqrt{n-1}r_s \\sim N(0, 1) \\]","title":"Spearman Rank Correlation"},{"location":"stat/3620/ch5.html#kendalls-tau","text":"Concordant: \\((X_i - X_j)(Y_i - Y_j) > 0\\) Discordant: \\((X_i - X_j)(Y_i - Y_j) < 0\\) Kendall's tau : \\(\\tau = p_c = p_d\\) \\[ r_\\tau = \\frac{\\sum\\sum_{i<j} sgn[(X_i - X_j)(Y_i - Y_j)]}{{n\\choose 2}} \\] Large sample approximation TODO","title":"Kendall's Tau"},{"location":"stat/3620/ch5.html#contingency-table","text":"Chi-square test","title":"Contingency table"}]}